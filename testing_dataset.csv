Title,Author,Date,Summary,Comments,Reference
"Silences, Spikes and Bursts: Three-Part Knot of the Neural Code","[arxiv.Result.Author('Richard Naud'), arxiv.Result.Author('Zachary Friedenberger'), arxiv.Result.Author('Katalin Toth')]",2023-02-14 17:30:05+00:00,"When a neuron breaks silence, it can emit action potentials in a number of
patterns. Some responses are so sudden and intense that electrophysiologists
felt the need to single them out, labeling action potentials emitted at a
particularly high frequency with a metonym -- bursts. Is there more to bursts
than a figure of speech? After all, sudden bouts of high-frequency firing are
expected to occur whenever inputs surge. The burst coding hypothesis advances
that the neural code has three syllables: silences, spikes and bursts. We
review evidence supporting this ternary code in terms of devoted mechanisms for
burst generation, synaptic transmission and synaptic plasticity. We also review
the learning and attention theories for which such a triad is beneficial.","15 pages, 4 figures",
Synthesizing audio from tongue motion during speech using tagged MRI via transformer,"[arxiv.Result.Author('Xiaofeng Liu'), arxiv.Result.Author('Fangxu Xing'), arxiv.Result.Author('Jerry L. Prince'), arxiv.Result.Author('Maureen Stone'), arxiv.Result.Author('Georges El Fakhri'), arxiv.Result.Author('Jonghye Woo')]",2023-02-14 17:27:55+00:00,"Investigating the relationship between internal tissue point motion of the
tongue and oropharyngeal muscle deformation measured from tagged MRI and
intelligible speech can aid in advancing speech motor control theories and
developing novel treatment methods for speech related-disorders. However,
elucidating the relationship between these two sources of information is
challenging, due in part to the disparity in data structure between
spatiotemporal motion fields (i.e., 4D motion fields) and one-dimensional audio
waveforms. In this work, we present an efficient encoder-decoder translation
network for exploring the predictive information inherent in 4D motion fields
via 2D spectrograms as a surrogate of the audio data. Specifically, our encoder
is based on 3D convolutional spatial modeling and transformer-based temporal
modeling. The extracted features are processed by an asymmetric 2D convolution
decoder to generate spectrograms that correspond to 4D motion fields.
Furthermore, we incorporate a generative adversarial training approach into our
framework to further improve synthesis quality on our generated spectrograms.
We experiment on 63 paired motion field sequences and speech waveforms,
demonstrating that our framework enables the generation of clear audio
waveforms from a sequence of motion fields. Thus, our framework has the
potential to improve our understanding of the relationship between these two
modalities and inform the development of treatments for speech disorders.",SPIE Medical Imaging: Deep Dive Oral,
Multi-Source Contrastive Learning from Musical Audio,"[arxiv.Result.Author('Christos Garoufis'), arxiv.Result.Author('Athanasia Zlatintsi'), arxiv.Result.Author('Petros Maragos')]",2023-02-14 14:36:59+00:00,"Contrastive learning constitutes an emerging branch of self-supervised
learning that leverages large amounts of unlabeled data, by learning a latent
space, where pairs of different views of the same sample are associated. In
this paper, we propose musical source association as a pair generation strategy
in the context of contrastive music representation learning. To this end, we
modify COLA, a widely used contrastive learning audio framework, to learn to
associate a song excerpt with a stochastically selected and automatically
extracted vocal or instrumental source. We further introduce a novel
modification to the contrastive loss to incorporate information about the
existence or absence of specific sources. Our experimental evaluation in three
different downstream tasks (music auto-tagging, instrument classification and
music genre classification) using the publicly available Magna-Tag-A-Tune
(MTAT) as a source dataset yields competitive results to existing literature
methods, as well as faster network convergence. The results also show that this
pre-training method can be steered towards specific features, according to the
selected musical source, while also being dependent on the quality of the
separated sources.","8 pages, 5 figures, 3 tables. (Slightly edited) submission at SMC23",
Speaker-Independent Acoustic-to-Articulatory Speech Inversion,"[arxiv.Result.Author('Peter Wu'), arxiv.Result.Author('Li-Wei Chen'), arxiv.Result.Author('Cheol Jun Cho'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Louis Goldstein'), arxiv.Result.Author('Alan W Black'), arxiv.Result.Author('Gopala K. Anumanchipalli')]",2023-02-14 01:20:31+00:00,"To build speech processing methods that can handle speech as naturally as
humans, researchers have explored multiple ways of building an invertible
mapping from speech to an interpretable space. The articulatory space is a
promising inversion target, since this space captures the mechanics of speech
production. To this end, we build an acoustic-to-articulatory inversion (AAI)
model that leverages autoregression, adversarial training, and self supervision
to generalize to unseen speakers. Our approach obtains 0.784 correlation on an
electromagnetic articulography (EMA) dataset, improving the state-of-the-art by
12.5%. Additionally, we show the interpretability of these representations
through directly comparing the behavior of estimated representations with
speech production behavior. Finally, we propose a resynthesis-based AAI
evaluation metric that does not rely on articulatory labels, demonstrating its
efficacy with an 18-speaker dataset.",,
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,"[arxiv.Result.Author('Gorka Abad'), arxiv.Result.Author('Oguzhan Ersoy'), arxiv.Result.Author('Stjepan Picek'), arxiv.Result.Author('Aitor Urbieta')]",2023-02-13 11:34:17+00:00,"Deep neural networks (DNNs) have achieved excellent results in various tasks,
including image and speech recognition. However, optimizing the performance of
DNNs requires careful tuning of multiple hyperparameters and network parameters
via training. High-performance DNNs utilize a large number of parameters,
corresponding to high energy consumption during training. To address these
limitations, researchers have developed spiking neural networks (SNNs), which
are more energy-efficient and can process data in a biologically plausible
manner, making them well-suited for tasks involving sensory data processing,
i.e., neuromorphic data. Like DNNs, SNNs are vulnerable to various threats,
such as adversarial examples and backdoor attacks. Yet, the attacks and
countermeasures for SNNs have been almost fully unexplored.
  This paper investigates the application of backdoor attacks in SNNs using
neuromorphic datasets and different triggers. More precisely, backdoor triggers
in neuromorphic data can change their position and color, allowing a larger
range of possibilities than common triggers in, e.g., the image domain. We
propose different attacks achieving up to 100\% attack success rate without
noticeable clean accuracy degradation. We also evaluate the stealthiness of the
attacks via the structural similarity metric, showing our most powerful attacks
being also stealthy. Finally, we adapt the state-of-the-art defenses from the
image domain, demonstrating they are not necessarily effective for neuromorphic
data resulting in inaccurate performance.",,
Fast and small footprint Hybrid HMM-HiFiGAN based system for speech synthesis in Indian languages,"[arxiv.Result.Author('Sudhanshu Srivastava'), arxiv.Result.Author('Ishika Gupta'), arxiv.Result.Author('Anusha Prakash'), arxiv.Result.Author('Jom Kuriakose'), arxiv.Result.Author('Hema A. Murthy')]",2023-02-13 10:01:43+00:00,"Hidden-Markov-model (HMM) based text-to-speech (HTS) offers flexibility in
speaking styles along with fast training and synthesis while being
computationally less intense. HTS performs well even in low-resource scenarios.
The primary drawback is that the voice quality is poor compared to that of E2E
systems. A hybrid approach combining HMM-based feature generation and
neural-network-based HiFi-GAN vocoder to improve HTS synthesis quality is
proposed. HTS is trained on high-resolution mel-spectrograms instead of
conventional mel generalized coefficients (MGC), and the output mel-spectrogram
corresponding to the input text is used in a HiFi-GAN vocoder trained on Indic
languages, to produce naturalness that is equivalent to that of E2E systems, as
evidenced from the DMOS and PC tests.","5 pages, 5 figures",
"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis","[arxiv.Result.Author('Yu-Chien Tang'), arxiv.Result.Author('Kuang-Da Wang'), arxiv.Result.Author('Ting-Yun Ou'), arxiv.Result.Author('Wen-Chih Peng')]",2023-02-13 03:25:37+00:00,"This paper presents a robust solution to the Memotion 3.0 Shared Task. The
goal of this task is to classify the emotion and the corresponding intensity
expressed by memes, which are usually in the form of images with short captions
on social media. Understanding the multi-modal features of the given memes will
be the key to solving the task. In this work, we use CLIP to extract aligned
image-text features and propose a novel meme sentiment analysis framework,
consisting of a Cooperative Teaching Model (CTM) for Task A and a Cascaded
Emotion Classifier (CEC) for Tasks B&C. CTM is based on the idea of knowledge
distillation, and can better predict the sentiment of a given meme in Task A;
CEC can leverage the emotion intensity suggestion from the prediction of Task C
to classify the emotion more precisely in Task B. Experiments show that we
achieved the 2nd place ranking for both Task A and Task B and the 4th place
ranking for Task C, with weighted F1-scores of 0.342, 0.784, and 0.535
respectively. The results show the robustness and effectiveness of our
framework. Our code is released at github.","De-Factify 2: Second Workshop on Multimodal Fact Checking and Hate
  Speech Detection, co-located with AAAI 2023",
ASR Bundestag: A Large-Scale political debate dataset in German,"[arxiv.Result.Author('Johannes Wirth'), arxiv.Result.Author('René Peinl')]",2023-02-12 21:45:18+00:00,"We present ASR Bundestag, a dataset for automatic speech recognition in
German, consisting of 610 hours of aligned audio-transcript pairs for
supervised training as well as 1,038 hours of unlabeled audio snippets for
self-supervised learning, based on raw audio data and transcriptions from
plenary sessions and committee meetings of the German parliament. In addition,
we discuss utilized approaches for the automated creation of speech datasets
and assess the quality of the resulting dataset based on evaluations and
finetuning of a pre-trained state of the art model. We make the dataset
publicly available, including all subsets.","13 pages, 2 tables, 4 figures",
SemanticAC: Semantics-Assisted Framework for Audio Classification,"[arxiv.Result.Author('Yicheng Xiao'), arxiv.Result.Author('Yue Ma'), arxiv.Result.Author('Shuyan Li'), arxiv.Result.Author('Hantao Zhou'), arxiv.Result.Author('Ran Liao'), arxiv.Result.Author('Xiu Li')]",2023-02-12 15:30:28+00:00,"In this paper, we propose SemanticAC, a semantics-assisted framework for
Audio Classification to better leverage the semantic information. Unlike
conventional audio classification methods that treat class labels as discrete
vectors, we employ a language model to extract abundant semantics from labels
and optimize the semantic consistency between audio signals and their labels.
We verify that simple textual information from labels and advanced pretraining
models enable more abundant semantic supervision for better performance.
Specifically, we design a text encoder to capture the semantic information from
the text extension of labels. Then we map the audio signals to align with the
semantics of corresponding class labels via an audio encoder and a similarity
calculation module so as to enforce the semantic consistency. Extensive
experiments on two audio datasets, ESC-50 and US8K demonstrate that our
proposed method consistently outperforms the compared audio classification
methods.",,
LipLearner: Customizable Silent Speech Interactions on Mobile Devices,"[arxiv.Result.Author('Zixiong Su'), arxiv.Result.Author('Shitao Fang'), arxiv.Result.Author('Jun Rekimoto')]",2023-02-12 13:10:57+00:00,"Silent speech interface is a promising technology that enables private
communications in natural language. However, previous approaches only support a
small and inflexible vocabulary, which leads to limited expressiveness. We
leverage contrastive learning to learn efficient lipreading representations,
enabling few-shot command customization with minimal user effort. Our model
exhibits high robustness to different lighting, posture, and gesture conditions
on an in-the-wild dataset. For 25-command classification, an F1-score of 0.8947
is achievable only using one shot, and its performance can be further boosted
by adaptively learning from more data. This generalizability allowed us to
develop a mobile silent speech interface empowered with on-device fine-tuning
and visual keyword spotting. A user study demonstrated that with LipLearner,
users could define their own commands with high reliability guaranteed by an
online incremental learning scheme. Subjective feedback indicated that our
system provides essential functionalities for customizable silent speech
interactions with high usability and learnability.","Conditionally accepted to the ACM CHI Conference on Human Factors in
  Computing Systems 2023 (CHI '23)",
Neural Architecture Search with Multimodal Fusion Methods for Diagnosing Dementia,"[arxiv.Result.Author('Michail Chatzianastasis'), arxiv.Result.Author('Loukas Ilias'), arxiv.Result.Author('Dimitris Askounis'), arxiv.Result.Author('Michalis Vazirgiannis')]",2023-02-12 11:25:29+00:00,"Alzheimer's dementia (AD) affects memory, thinking, and language,
deteriorating person's life. An early diagnosis is very important as it enables
the person to receive medical help and ensure quality of life. Therefore,
leveraging spontaneous speech in conjunction with machine learning methods for
recognizing AD patients has emerged into a hot topic. Most of the previous
works employ Convolutional Neural Networks (CNNs), to process the input signal.
However, finding a CNN architecture is a time-consuming process and requires
domain expertise. Moreover, the researchers introduce early and late fusion
approaches for fusing different modalities or concatenate the representations
of the different modalities during training, thus the inter-modal interactions
are not captured. To tackle these limitations, first we exploit a Neural
Architecture Search (NAS) method to automatically find a high performing CNN
architecture. Next, we exploit several fusion methods, including Multimodal
Factorized Bilinear Pooling and Tucker Decomposition, to combine both speech
and text modalities. To the best of our knowledge, there is no prior work
exploiting a NAS approach and these fusion methods in the task of dementia
detection from spontaneous speech. We perform extensive experiments on the
ADReSS Challenge dataset and show the effectiveness of our approach over
state-of-the-art methods.",,
Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation,"[arxiv.Result.Author('Cong Han'), arxiv.Result.Author('Vishal Choudhari'), arxiv.Result.Author('Yinghao Aaron Li'), arxiv.Result.Author('Nima Mesgarani')]",2023-02-11 18:33:42+00:00,"Auditory attention decoding (AAD) is a technique used to identify and amplify
the talker that a listener is focused on in a noisy environment. This is done
by comparing the listener's brainwaves to a representation of all the sound
sources to find the closest match. The representation is typically the waveform
or spectrogram of the sounds. The effectiveness of these representations for
AAD is uncertain. In this study, we examined the use of self-supervised learned
speech representation in improving the accuracy and speed of AAD. We recorded
the brain activity of three subjects using invasive electrocorticography (ECoG)
as they listened to two conversations and focused on one. We used WavLM to
extract a latent representation of each talker and trained a spatiotemporal
filter to map brain activity to intermediate representations of speech. During
the evaluation, the reconstructed representation is compared to each speaker's
representation to determine the target speaker. Our results indicate that
speech representation from WavLM provides better decoding accuracy and speed
than the speech envelope and spectrogram. Our findings demonstrate the
advantages of self-supervised learned speech representation for auditory
attention decoding and pave the way for developing brain-controlled hearable
technologies.",,
Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,"[arxiv.Result.Author('Daniel Kang'), arxiv.Result.Author('Xuechen Li'), arxiv.Result.Author('Ion Stoica'), arxiv.Result.Author('Carlos Guestrin'), arxiv.Result.Author('Matei Zaharia'), arxiv.Result.Author('Tatsunori Hashimoto')]",2023-02-11 15:57:44+00:00,"Recent advances in instruction-following large language models (LLMs) have
led to dramatic improvements in a range of NLP tasks. Unfortunately, we find
that the same improved capabilities amplify the dual-use risks for malicious
purposes of these models. Dual-use is difficult to prevent as
instruction-following capabilities now enable standard attacks from computer
security. The capabilities of these instruction-following LLMs provide strong
economic incentives for dual-use by malicious actors. In particular, we show
that instruction-following LLMs can produce targeted malicious content,
including hate speech and scams, bypassing in-the-wild defenses implemented by
LLM API vendors. Our analysis shows that this content can be generated
economically and at cost likely lower than with human effort alone. Together,
our findings suggest that LLMs will increasingly attract more sophisticated
adversaries and attacks, and addressing these attacks may require new
approaches to mitigations.",,
Parameterizable Acoustical Modeling and Auralization of Cultural Heritage Sites based on Photogrammetry,[arxiv.Result.Author('Dominik Ukolov')],2023-02-11 15:44:54+00:00,"The photogrammetric and reconstructive modeling of cultural heritage sites is
mostly focused on visually perceivable aspects, but if their intended purpose
is the performance of cultural acts with a sonic emphasis, it is important to
consider the preservation of their acoustical behaviour to make them audible in
an authentic way. This applies in particular to sacral and concert environments
as popular objects for photogrammetric models, which contain geometrical and
textural information that can be used to locate and classify acoustically
relevant surface properties. With the advancing conversion or destruction of
historical acoustical spaces, it becomes even more important to preserve their
unique sonic characters, while three-dimensional auralizations become widely
applicable. The proposed study presents the current state of a new
methodological approach to acoustical modeling using photogrammetric data and
introduces a parameterizable pipeline that will be accessible as an open-source
software with a graphical user interface.","6 pages, 3 figures, 27th Conference on Cultural Heritage and New
  Technologies (Vienna, 2022)",
MTTM: Metamorphic Testing for Textual Content Moderation Software,"[arxiv.Result.Author('Wenxuan Wang'), arxiv.Result.Author('Jen-tse Huang'), arxiv.Result.Author('Weibin Wu'), arxiv.Result.Author('Jianping Zhang'), arxiv.Result.Author('Yizhan Huang'), arxiv.Result.Author('Shuqing Li'), arxiv.Result.Author('Pinjia He'), arxiv.Result.Author('Michael Lyu')]",2023-02-11 14:44:39+00:00,"The exponential growth of social media platforms such as Twitter and Facebook
has revolutionized textual communication and textual content publication in
human society. However, they have been increasingly exploited to propagate
toxic content, such as hate speech, malicious advertisement, and pornography,
which can lead to highly negative impacts (e.g., harmful effects on teen mental
health). Researchers and practitioners have been enthusiastically developing
and extensively deploying textual content moderation software to address this
problem. However, we find that malicious users can evade moderation by changing
only a few words in the toxic content. Moreover, modern content moderation
software performance against malicious inputs remains underexplored. To this
end, we propose MTTM, a Metamorphic Testing framework for Textual content
Moderation software. Specifically, we conduct a pilot study on 2,000 text
messages collected from real users and summarize eleven metamorphic relations
across three perturbation levels: character, word, and sentence. MTTM employs
these metamorphic relations on toxic textual contents to generate test cases,
which are still toxic yet likely to evade moderation. In our evaluation, we
employ MTTM to test three commercial textual content moderation software and
two state-of-the-art moderation algorithms against three kinds of toxic
content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error
finding rates (EFR) when testing commercial moderation software provided by
Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when
testing the state-of-the-art algorithms from the academy. In addition, we
leverage the test cases generated by MTTM to retrain the model we explored,
which largely improves model robustness (0% to 5.9% EFR) while maintaining the
accuracy on the original test set.",Accepted by ICSE 2023,
Local spectral attention for full-band speech enhancement,"[arxiv.Result.Author('Zhongshu Hou'), arxiv.Result.Author('Qinwen Hu'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Jing Lu')]",2023-02-11 13:25:19+00:00,"Attention mechanism has been widely utilized in speech enhancement (SE)
because theoretically it can effectively model the inherent connection of
signal both in time domain and spectrum domain. Usually, the span of attention
is limited in time domain while the attention in frequency domain spans the
whole frequency range. In this paper, we notice that the attention over the
whole frequency range hampers the inference for full-band SE and possibly leads
to excessive residual noise. To alleviate this problem, we introduce local
spectral attention (LSA) into full-band SE model by limiting the span of
attention. The ablation test on the state-of-the-art (SOTA) full-band SE model
reveals that the local frequency attention can effectively improve overall
performance. The improved model achieves the best objective score on the
full-band VoiceBank+DEMAND set.",,
Attention does not guarantee best performance in speech enhancement,"[arxiv.Result.Author('Zhongshu Hou'), arxiv.Result.Author('Qinwen Hu'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Jing Lu')]",2023-02-11 13:17:59+00:00,"Attention mechanism has been widely utilized in speech enhancement (SE)
because theoretically it can effectively model the long-term inherent
connection of signal both in time domain and spectrum domain. However, the
generally used global attention mechanism might not be the best choice since
the adjacent information naturally imposes more influence than the far-apart
information in speech enhancement. In this paper, we validate this conjecture
by replacing attention with RNN in two typical state-of-the-art (SOTA) models,
multi-scale temporal frequency convolutional network (MTFAA) with axial
attention and conformer-based metric-GAN network (CMGAN).",,
ASDF: A Differential Testing Framework for Automatic Speech Recognition Systems,"[arxiv.Result.Author('Daniel Hao Xian Yuen'), arxiv.Result.Author('Andrew Yong Chen Pang'), arxiv.Result.Author('Zhou Yang'), arxiv.Result.Author('Chun Yong Chong'), arxiv.Result.Author('Mei Kuan Lim'), arxiv.Result.Author('David Lo')]",2023-02-11 02:53:12+00:00,"Recent years have witnessed wider adoption of Automated Speech Recognition
(ASR) techniques in various domains. Consequently, evaluating and enhancing the
quality of ASR systems is of great importance. This paper proposes ASDF, an
Automated Speech Recognition Differential Testing Framework for testing ASR
systems. ASDF extends an existing ASR testing tool, the CrossASR++, which
synthesizes test cases from a text corpus. However, CrossASR++ fails to make
use of the text corpus efficiently and provides limited information on how the
failed test cases can improve ASR systems. To address these limitations, our
tool incorporates two novel features: (1) a text transformation module to boost
the number of generated test cases and uncover more errors in ASR systems and
(2) a phonetic analysis module to identify on which phonemes the ASR system
tend to produce errors. ASDF generates more high-quality test cases by applying
various text transformation methods (e.g., change tense) to the texts in failed
test cases. By doing so, ASDF can utilize a small text corpus to generate a
large number of audio test cases, something which CrossASR++ is not capable of.
In addition, ASDF implements more metrics to evaluate the performance of ASR
systems from multiple perspectives. ASDF performs phonetic analysis on the
identified failed test cases to identify the phonemes that ASR systems tend to
transcribe incorrectly, providing useful information for developers to improve
ASR systems. The demonstration video of our tool is made online at
https://www.youtube.com/watch?v=DzVwfc3h9As. The implementation is available at
https://github.com/danielyuenhx/asdf-differential-testing.",Accpeted by ICST 2023 Tool Demo Track,
GTR-CTRL: Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers,"[arxiv.Result.Author('Pedro Sarmento'), arxiv.Result.Author('Adarsh Kumar'), arxiv.Result.Author('Yu-Hua Chen'), arxiv.Result.Author('CJ Carr'), arxiv.Result.Author('Zack Zukowski'), arxiv.Result.Author('Mathieu Barthet')]",2023-02-10 17:43:03+00:00,"Recently, symbolic music generation with deep learning techniques has
witnessed steady improvements. Most works on this topic focus on MIDI
representations, but less attention has been paid to symbolic music generation
using guitar tablatures (tabs) which can be used to encode multiple
instruments. Tabs include information on expressive techniques and fingerings
for fretted string instruments in addition to rhythm and pitch. In this work,
we use the DadaGP dataset for guitar tab music generation, a corpus of over 26k
songs in GuitarPro and token formats. We introduce methods to condition a
Transformer-XL deep learning model to generate guitar tabs (GTR-CTRL) based on
desired instrumentation (inst-CTRL) and genre (genre-CTRL). Special control
tokens are appended at the beginning of each song in the training corpus. We
assess the performance of the model with and without conditioning. We propose
instrument presence metrics to assess the inst-CTRL model's response to a given
instrumentation prompt. We trained a BERT model for downstream genre
classification and used it to assess the results obtained with the genre-CTRL
model. Statistical analyses evidence significant differences between the
conditioned and unconditioned models. Overall, results indicate that the
GTR-CTRL methods provide more flexibility and control for guitar-focused
symbolic music generation than an unconditioned model.","This preprint is licensed under a Creative Commons Attribution 4.0
  International License (CC BY 4.0). The Version of Record of this contribution
  is published in Proceedings of EvoMUSART: International Conference on
  Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)
  2023","EvoMUSART: International Conference on Computational Intelligence
  in Music, Sound, Art and Design (Part of EvoStar) 2023"
The LuViRA Dataset: Measurement Description,"[arxiv.Result.Author('Ilayda Yaman'), arxiv.Result.Author('Guoda Tian'), arxiv.Result.Author('Martin Larsson'), arxiv.Result.Author('Patrik Persson'), arxiv.Result.Author('Michiel Sandra'), arxiv.Result.Author('Alexander Dürr'), arxiv.Result.Author('Erik Tegler'), arxiv.Result.Author('Nikhil Challa'), arxiv.Result.Author('Henrik Garde'), arxiv.Result.Author('Fredrik Tufvesson'), arxiv.Result.Author('Kalle Åström'), arxiv.Result.Author('Ove Edfors'), arxiv.Result.Author('Steffen Malkowsky'), arxiv.Result.Author('Liang Liu')]",2023-02-10 15:12:40+00:00,"We present a dataset to evaluate localization algorithms, which utilizes
vision, audio, and radio sensors: the Lund University Vision, Radio, and Audio
(LuViRA) Dataset. The dataset includes RGB images, corresponding depth maps,
IMU readings, channel response between a massive MIMO channel sounder and a
user equipment, audio recorded by 12 microphones, and 0.5 mm accurate 6DoF pose
ground truth. We synchronize these sensors to make sure that all data are
recorded simultaneously. A camera, speaker, and transmit antenna are placed on
top of a slowly moving service robot and 88 trajectories are recorded. Each
trajectory includes 20 to 50 seconds of recorded sensor data and ground truth
labels. The data from different sensors can be used separately or jointly to
conduct localization tasks and a motion capture system is used to verify the
results obtained by the localization algorithms. The main aim of this dataset
is to enable research on fusing the most commonly used sensors for localization
tasks. However, the full dataset or some parts of it can also be used for other
research areas such as channel estimation, image classification, etc. Fusing
sensor data can lead to increased localization accuracy and reliability, as
well as decreased latency and power consumption. The created dataset will be
made public at a later date.","7 pages, 7 figures",
Spoken language change detection inspired by speaker change detection,"[arxiv.Result.Author('Jagabandhu Mishra'), arxiv.Result.Author('S. R. Mahadeva Prasanna')]",2023-02-10 14:25:49+00:00,"Spoken language change detection (LCD) refers to identifying the language
transitions in a code-switched utterance. Similarly, identifying the speaker
transitions in a multispeaker utterance is known as speaker change detection
(SCD). Since tasks-wise both are similar, the architecture/framework developed
for the SCD task may be suitable for the LCD task. Hence, the aim of the
present work is to develop LCD systems inspired by SCD. Initially, both LCD and
SCD are performed by humans. The study suggests humans require (a) a larger
duration around the change point and (b) language-specific prior exposure, for
performing LCD as compared to SCD. The larger duration requirement is
incorporated by increasing the analysis window length of the unsupervised
distance-based approach. This leads to a relative performance improvement of
29.1% and 2.4%, and a priori language knowledge provides a relative improvement
of 31.63% and 14.27% on the synthetic and practical codeswitched datasets,
respectively. The performance difference between the practical and synthetic
datasets is mostly due to differences in the distribution of the monolingual
segment duration.",,
Cross-Corpora Spoken Language Identification with Domain Diversification and Generalization,"[arxiv.Result.Author('Spandan Dey'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Goutam Saha')]",2023-02-10 08:21:42+00:00,"This work addresses the cross-corpora generalization issue for the
low-resourced spoken language identification (LID) problem. We have conducted
the experiments in the context of Indian LID and identified strikingly poor
cross-corpora generalization due to corpora-dependent non-lingual biases. Our
contribution to this work is twofold. First, we propose domain diversification,
which diversifies the limited training data using different audio data
augmentation methods. We then propose the concept of maximally diversity-aware
cascaded augmentations and optimize the augmentation fold-factor for effective
diversification of the training data. Second, we introduce the idea of domain
generalization considering the augmentation methods as pseudo-domains. Towards
this, we investigate both domain-invariant and domain-aware approaches. Our LID
system is based on the state-of-the-art emphasized channel attention,
propagation, and aggregation based time delay neural network (ECAPA-TDNN)
architecture. We have conducted extensive experiments with three widely used
corpora for Indian LID research. In addition, we conduct a final blind
evaluation of our proposed methods on the Indian subset of VoxLingua107 corpus
collected in the wild. Our experiments demonstrate that the proposed domain
diversification is more promising over commonly used simple augmentation
methods. The study also reveals that domain generalization is a more effective
solution than domain diversification. We also notice that domain-aware learning
performs better for same-corpora LID, whereas domain-invariant learning is more
suitable for cross-corpora generalization. Compared to basic ECAPA-TDNN, its
proposed domain-invariant extensions improve the cross-corpora EER up to 5.23%.
In contrast, the proposed domain-aware extensions also improve performance for
same-corpora test scenarios.",Accepted for publication in Elsevier Computer Speech & Language,
ControversialQA: Exploring Controversy in Question Answering,"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Peide Zhu'), arxiv.Result.Author('Jie Yang')]",2023-02-10 05:39:29+00:00,"Controversy is widespread online. Previous studies mainly define controversy
based on vague assumptions of its relation to sentiment such as hate speech and
offensive words. This paper introduces the first question-answering dataset
that defines content controversy by user perception, i.e., votes from plenty of
users. It contains nearly 10K questions, and each question has a best answer
and a most controversial answer. Experimental results reveal that controversy
detection in question answering is essential and challenging, and there is no
strong correlation between controversy and sentiment tasks.",,
PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction,"[arxiv.Result.Author('Ziji Zhang'), arxiv.Result.Author('Zhehui Wang'), arxiv.Result.Author('Rajesh Kamma'), arxiv.Result.Author('Sharanya Eswaran'), arxiv.Result.Author('Narayanan Sadagopan')]",2023-02-10 04:05:24+00:00,"Speech-to-text errors made by automatic speech recognition (ASR) system
negatively impact downstream models relying on ASR transcriptions. Language
error correction models as a post-processing text editing approach have been
recently developed for refining the source sentences. However, efficient models
for correcting errors in ASR transcriptions that meet the low latency
requirements of industrial grade production systems have not been well studied.
In this work, we propose a novel non-autoregressive (NAR) error correction
approach to improve the transcription quality by reducing word error rate (WER)
and achieve robust performance across different upstream ASR systems. Our
approach augments the text encoding of the Transformer model with a phoneme
encoder that embeds pronunciation information. The representations from phoneme
encoder and text encoder are combined via multi-modal fusion before feeding
into the length tagging predictor for predicting target sequence lengths. The
joint encoders also provide inputs to the attention mechanism in the NAR
decoder. We experiment on 3 open-source ASR systems with varying speech-to-text
transcription quality and their erroneous transcriptions on 2 public English
corpus datasets. Results show that our PATCorrect (Phoneme Augmented
Transformer for ASR error Correction) consistently outperforms state-of-the-art
NAR error correction method on English corpus across different upstream ASR
systems. For example, PATCorrect achieves 11.62% WER reduction (WERR) averaged
on 3 ASR systems compared to 9.46% WERR achieved by other method using text
only modality and also achieves an inference latency comparable to other NAR
models at tens of millisecond scale, especially on GPU hardware, while still
being 4.2 - 6.7x times faster than autoregressive models on Common Voice and
LibriSpeech datasets.","12 pages, 1 figure",
AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations,"[arxiv.Result.Author('Jiachen Lian'), arxiv.Result.Author('Alexei Baevski'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Michael Auli')]",2023-02-10 02:55:52+00:00,"Self-supervision has shown great potential for audio-visual speech
recognition by vastly reducing the amount of labeled data required to build
good systems. However, existing methods are either not entirely end-to-end or
do not train joint representations of both modalities. In this paper, we
introduce AV-data2vec which addresses these challenges and builds audio-visual
representations based on predicting contextualized representations which has
been successful in the uni-modal case. The model uses a shared transformer
encoder for both audio and video and can combine both modalities to improve
speech recognition. Results on LRS3 show that AV-data2vec consistently
outperforms existing methods under most settings.",,
Leveraging supplementary text data to kick-start automatic speech recognition system development with limited transcriptions,"[arxiv.Result.Author('Nay San'), arxiv.Result.Author('Martijn Bartelds'), arxiv.Result.Author('Blaine Billings'), arxiv.Result.Author('Ella de Falco'), arxiv.Result.Author('Hendi Feriza'), arxiv.Result.Author('Johan Safri'), arxiv.Result.Author('Wawan Sahrozi'), arxiv.Result.Author('Ben Foley'), arxiv.Result.Author('Bradley McDonnell'), arxiv.Result.Author('Dan Jurafsky')]",2023-02-09 23:30:49+00:00,"Recent research using pre-trained transformer models suggests that just 10
minutes of transcribed speech may be enough to fine-tune such a model for
automatic speech recognition (ASR) -- at least if we can also leverage vast
amounts of text data (803 million tokens). But is that much text data
necessary? We study the use of different amounts of text data, both for
creating a lexicon that constrains ASR decoding to possible words (e.g. *dogz
vs. dogs), and for training larger language models that bias the system toward
probable word sequences (e.g. too dogs vs. two dogs). We perform experiments
using 10 minutes of transcribed speech from English (for replicating prior
work) and two additional pairs of languages differing in the availability of
supplemental text data: Gronings and Frisian (~7.5M token corpora available),
and Besemah and Nasal (only small lexica available). For all languages, we
found that using only a lexicon did not appreciably improve ASR performance.
For Gronings and Frisian, we found that lexica and language models derived from
'novel-length' 80k token subcorpora reduced the word error rate (WER) to 39% on
average. Our findings suggest that where a text corpus in the upper tens of
thousands of tokens or more is available, fine-tuning a transformer model with
just tens of minutes of transcribed speech holds some promise towards obtaining
human-correctable transcriptions near the 30% WER rule-of-thumb.",Accepted for ComputEL-6,
Hypernetworks build Implicit Neural Representations of Sounds,"[arxiv.Result.Author('Filip Szatkowski'), arxiv.Result.Author('Karol J. Piczak'), arxiv.Result.Author('Przemtsław Spurek'), arxiv.Result.Author('Jacek Tabor'), arxiv.Result.Author('Tomasz Trzciński')]",2023-02-09 22:24:26+00:00,"Implicit Neural Representations (INRs) are nowadays used to represent
multimedia signals across various real-life applications, including image
super-resolution, image compression, or 3D rendering. Existing methods that
leverage INRs are predominantly focused on visual data, as their application to
other modalities, such as audio, is nontrivial due to the inductive biases
present in architectural attributes of image-based INR models. To address this
limitation, we introduce HyperSound, the first meta-learning approach to
produce INRs for audio samples that leverages hypernetworks to generalize
beyond samples observed in training. Our approach reconstructs audio samples
with quality comparable to other state-of-the-art models and provides a viable
alternative to contemporary sound representations used in deep neural networks
for audio processing, such as spectrograms.",,
A Composite T60 Regression and Classification Approach for Speech Dereverberation,"[arxiv.Result.Author('Yuying Li'), arxiv.Result.Author('Yuchen Liu'), arxiv.Result.Author('Donald S. Williamson')]",2023-02-09 20:56:09+00:00,"Dereverberation is often performed directly on the reverberant audio signal,
without knowledge of the acoustic environment. Reverberation time, T60,
however, is an essential acoustic factor that reflects how reverberation may
impact a signal. In this work, we propose to perform dereverberation while
leveraging key acoustic information from the environment. More specifically, we
develop a joint learning approach that uses a composite T60 module and a
separate dereverberation module to simultaneously perform reverberation time
estimation and dereverberation. The reverberation time module provides key
features to the dereverberation module during fine tuning. We evaluate our
approach in simulated and real environments, and compare against several
approaches. The results show that this composite framework improves performance
in environments.",,
Robot Synesthesia: A Sound and Emotion Guided AI Painter,"[arxiv.Result.Author('Vihaan Misra'), arxiv.Result.Author('Peter Schaldenbrand'), arxiv.Result.Author('Jean Oh')]",2023-02-09 18:53:44+00:00,"If a picture paints a thousand words, sound may voice a million. While recent
robotic painting and image synthesis methods have achieved progress in
generating visuals from text inputs, the translation of sound into images is
vastly unexplored. Generally, sound-based interfaces and sonic interactions
have the potential to expand accessibility and control for the user and provide
a means to convey complex emotions and the dynamic aspects of the real world.
In this paper, we propose an approach for using sound and speech to guide a
robotic painting process, known here as robot synesthesia. For general sound,
we encode the simulated paintings and input sounds into the same latent space.
For speech, we decouple speech into its transcribed text and the tone of the
speech. Whereas we use the text to control the content, we estimate the
emotions from the tone to guide the mood of the painting. Our approach has been
fully integrated with FRIDA, a robotic painting framework, adding sound and
speech to FRIDA's existing input modalities, such as text and style. In two
surveys, participants were able to correctly guess the emotion or natural sound
used to generate a given painting more than twice as likely as random chance.
On our sound-guided image manipulation and music-guided paintings, we discuss
the results qualitatively.","9 pages, 10 figures",
Incorporating Total Variation Regularization in the design of an intelligent Query by Humming system,"[arxiv.Result.Author('Shivangi Ranjan'), arxiv.Result.Author('Vishal Srivastava')]",2023-02-09 11:34:23+00:00,"A Query-By-Humming (QBH) system constitutes a particular case of music
information retrieval where the input is a user-hummed melody and the output is
the original song which contains that melody. A typical QBH system consists of
melody extraction and candidate melody retrieval.
  For melody extraction, accurate note transcription is the key enabling
technology. However, current transcription methods are unable to definitively
capture the melody and address inaccuracies in user-hummed queries. In this
paper, we incorporate Total Variation Regularization (TVR) to denoise queries.
This approach accounts for user error in humming without loss of meaningful
data and reliably captures the underlying melody.
  For candidate melody retrieval, we employ a deep learning approach to time
series classification using a Fully Convolutional Neural Network. The trained
network classifies the incoming query as belonging to one of the target songs.
  For our experiments, we use Roger Jang's MIR-QBSH dataset which is the
standard MIREX dataset. We demonstrate that inclusion of TVR denoised queries
in the training set enhances the overall accuracy of the system to 93% which is
higher than other state-of-the-art QBH systems.",,
Joint Acoustic Echo Cancellation and Speech Dereverberation Using Kalman filters,"[arxiv.Result.Author('Ziteng Wang'), arxiv.Result.Author('Yueyue Na'), arxiv.Result.Author('Biao Tian'), arxiv.Result.Author('Qiang Fu')]",2023-02-09 07:18:10+00:00,"This paper proposes a joint acoustic echo cancellation (AEC) and speech
dereverberation (DR) algorithm in the short-time Fourier transform domain. The
reverberant microphone signals are described using an auto-regressive (AR)
model. The AR coefficients and the loudspeaker-to-microphone acoustic transfer
functions (ATFs) are considered time-varying and are modeled simultaneously
using a first-order Markov process. This leads to a solution where these
parameters can be optimally estimated using Kalman filters. It is shown that
the proposed algorithm outperforms vanilla solutions that solve AEC and DR
sequentially and one state-of-the-art joint DRAEC algorithm based on semi-blind
source separation, in terms of both speech quality and echo reduction
performance.",,
ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models,"[arxiv.Result.Author('Pengfei Zhu'), arxiv.Result.Author('Chao Pang'), arxiv.Result.Author('Shuohuan Wang'), arxiv.Result.Author('Yekun Chai'), arxiv.Result.Author('Yu Sun'), arxiv.Result.Author('Hao Tian'), arxiv.Result.Author('Hua Wu')]",2023-02-09 06:27:09+00:00,"In recent years, there has been an increased popularity in image and speech
generation using diffusion models. However, directly generating music waveforms
from free-form text prompts is still under-explored. In this paper, we propose
the first text-to-waveform music generation model that can receive arbitrary
texts using diffusion models. We incorporate the free-form textual prompt as
the condition to guide the waveform generation process of diffusion models. To
solve the problem of lacking such text-music parallel data, we collect a
dataset of text-music pairs from the Internet with weak supervision. Besides,
we compare the effect of two prompt formats of conditioning texts (music tags
and free-form texts) and prove the superior performance of our method in terms
of text-music relevance. We further demonstrate that our generated music in the
waveform domain outperforms previous works by a large margin in terms of
diversity, quality, and text-music relevance.",,
Short-Term Memory Convolutions,"[arxiv.Result.Author('Grzegorz Stefański'), arxiv.Result.Author('Krzysztof Arendt'), arxiv.Result.Author('Paweł Daniluk'), arxiv.Result.Author('Bartłomiej Jasik'), arxiv.Result.Author('Artur Szumaczuk')]",2023-02-08 20:52:24+00:00,"The real-time processing of time series signals is a critical issue for many
real-life applications. The idea of real-time processing is especially
important in audio domain as the human perception of sound is sensitive to any
kind of disturbance in perceived signals, especially the lag between auditory
and visual modalities. The rise of deep learning (DL) models complicated the
landscape of signal processing. Although they often have superior quality
compared to standard DSP methods, this advantage is diminished by higher
latency. In this work we propose novel method for minimization of inference
time latency and memory consumption, called Short-Term Memory Convolution
(STMC) and its transposed counterpart. The main advantage of STMC is the low
latency comparable to long short-term memory (LSTM) networks. Furthermore, the
training of STMC-based models is faster and more stable as the method is based
solely on convolutional neural networks (CNNs). In this study we demonstrate an
application of this solution to a U-Net model for a speech separation task and
GhostNet model in acoustic scene classification (ASC) task. In case of speech
separation we achieved a 5-fold reduction in inference time and a 2-fold
reduction in latency without affecting the output quality. The inference time
for ASC task was up to 4 times faster while preserving the original accuracy.",ICLR 2023,
A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,"[arxiv.Result.Author('Li-Wei Chen'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Alexander Rudnicky')]",2023-02-08 17:34:32+00:00,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have
achieved near human-level naturalness. The diversity of human speech, however,
often goes beyond the coverage of these corpora. We believe the ability to
handle such diversity is crucial for AI systems to achieve human-level
communication. Our work explores the use of more abundant real-world data for
building speech synthesizers. We train TTS systems using real-world speech from
YouTube and podcasts. We observe the mismatch between training and inference
alignments in mel-spectrogram based autoregressive models, leading to
unintelligible synthesis, and demonstrate that learned discrete codes within
multiple code groups effectively resolves this issue. We introduce our MQTTS
system whose architecture is designed for multiple code generation and
monotonic alignment, along with the use of a clean silence prompt to improve
synthesis quality. We conduct ablation analyses to identify the efficacy of our
methods. We show that MQTTS outperforms existing TTS systems in several
objective and subjective measures.",Accepted to AAAI 2023,
Masking Kernel for Learning Energy-Efficient Speech Representation,"[arxiv.Result.Author('Apiwat Ditthapron'), arxiv.Result.Author('Emmanuel O. Agu'), arxiv.Result.Author('Adam C. Lammert')]",2023-02-08 16:13:28+00:00,"Modern smartphones are equipped with powerful audio hardware and processors,
allowing them to acquire and perform on-device speech processing at high
sampling rates. However, energy consumption remains a concern, especially for
resource-intensive DNNs. Prior mobile speech processing reduced computational
complexity by compacting the model or reducing input dimensions via
hyperparameter tuning, which reduced accuracy or required more training
iterations. This paper proposes gradient descent for optimizing
energy-efficient speech recording format (length and sampling rate). The goal
is to reduce the input size, which reduces data collection and inference
energy. For a backward pass, a masking function with non-zero derivatives
(Gaussian, Hann, and Hamming) is used as a windowing function and a lowpass
filter. An energy-efficient penalty is introduced to incentivize the reduction
of the input size. The proposed masking outperformed baselines by 8.7% in
speaker recognition and traumatic brain injury detection using 49% shorter
duration, sampled at a lower frequency.",,
Prompting for Multimodal Hateful Meme Classification,"[arxiv.Result.Author('Rui Cao'), arxiv.Result.Author('Roy Ka-Wei Lee'), arxiv.Result.Author('Wen-Haw Chong'), arxiv.Result.Author('Jing Jiang')]",2023-02-08 16:04:08+00:00,"Hateful meme classification is a challenging multimodal task that requires
complex reasoning and contextual background knowledge. Ideally, we could
leverage an explicit external knowledge base to supplement contextual and
cultural information in hateful memes. However, there is no known explicit
external knowledge base that could provide such hate speech contextual
information. To address this gap, we propose PromptHate, a simple yet effective
prompt-based model that prompts pre-trained language models (PLMs) for hateful
meme classification. Specifically, we construct simple prompts and provide a
few in-context examples to exploit the implicit knowledge in the pre-trained
RoBERTa language model for hateful meme classification. We conduct extensive
experiments on two publicly available hateful and offensive meme datasets. Our
experimental results show that PromptHate is able to achieve a high AUC of
90.96, outperforming state-of-the-art baselines on the hateful meme
classification task. We also perform fine-grained analyses and case studies on
various prompt settings and demonstrate the effectiveness of the prompts on
hateful meme classification.","Accepted in EMNLP, 2022",
Machine Learning for Synthetic Data Generation: a Review,"[arxiv.Result.Author('Yingzhou Lu'), arxiv.Result.Author('Huazheng Wang'), arxiv.Result.Author('Wenqi Wei')]",2023-02-08 13:59:31+00:00,"Data plays a crucial role in machine learning. However, in real-world
applications, there are several problems with data, e.g., data are of low
quality; a limited number of data points lead to under-fitting of the machine
learning model; it is hard to access the data due to privacy, safety and
regulatory concerns. \textit{Synthetic data generation} offers a promising new
avenue, as it can be shared and used in ways that real-world data cannot. This
paper systematically reviews the existing works that leverage machine learning
models for synthetic data generation. Specifically, we discuss the synthetic
data generation works from several perspectives: (i) applications, including
computer vision, speech, natural language, healthcare, and business; (ii)
machine learning methods, particularly neural network architectures and deep
generative models; (iii) privacy and fairness issue. In addition, we identify
the challenges and opportunities in this emerging field and suggest future
research directions.",,
Noise2Music: Text-conditioned Music Generation with Diffusion Models,"[arxiv.Result.Author('Qingqing Huang'), arxiv.Result.Author('Daniel S. Park'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Timo I. Denk'), arxiv.Result.Author('Andy Ly'), arxiv.Result.Author('Nanxin Chen'), arxiv.Result.Author('Zhengdong Zhang'), arxiv.Result.Author('Zhishuai Zhang'), arxiv.Result.Author('Jiahui Yu'), arxiv.Result.Author('Christian Frank'), arxiv.Result.Author('Jesse Engel'), arxiv.Result.Author('Quoc V. Le'), arxiv.Result.Author('William Chan'), arxiv.Result.Author('Wei Han')]",2023-02-08 07:27:27+00:00,"We introduce Noise2Music, where a series of diffusion models is trained to
generate high-quality 30-second music clips from text prompts. Two types of
diffusion models, a generator model, which generates an intermediate
representation conditioned on text, and a cascader model, which generates
high-fidelity audio conditioned on the intermediate representation and possibly
the text, are trained and utilized in succession to generate high-fidelity
music. We explore two options for the intermediate representation, one using a
spectrogram and the other using audio with lower fidelity. We find that the
generated audio is not only able to faithfully reflect key elements of the text
prompt such as genre, tempo, instruments, mood, and era, but goes beyond to
ground fine-grained semantics of the prompt. Pretrained large language models
play a key role in this story -- they are used to generate paired text for the
audio of the training set and to extract embeddings of the text prompts
ingested by the diffusion models.
  Generated examples: https://google-research.github.io/noise2music",15 pages,
Characterizing Financial Market Coverage using Artificial Intelligence,"[arxiv.Result.Author('Jean Marie Tshimula'), arxiv.Result.Author(""D'Jeff K. Nkashama""), arxiv.Result.Author('Patrick Owusu'), arxiv.Result.Author('Marc Frappier'), arxiv.Result.Author('Pierre-Martin Tardif'), arxiv.Result.Author('Froduald Kabanza'), arxiv.Result.Author('Armelle Brun'), arxiv.Result.Author('Jean-Marc Patenaude'), arxiv.Result.Author('Shengrui Wang'), arxiv.Result.Author('Belkacem Chikhaoui')]",2023-02-07 16:03:33+00:00,"This paper scrutinizes a database of over 4900 YouTube videos to characterize
financial market coverage. Financial market coverage generates a large number
of videos. Therefore, watching these videos to derive actionable insights could
be challenging and complex. In this paper, we leverage Whisper, a
speech-to-text model from OpenAI, to generate a text corpus of market coverage
videos from Bloomberg and Yahoo Finance. We employ natural language processing
to extract insights regarding language use from the market coverage. Moreover,
we examine the prominent presence of trending topics and their evolution over
time, and the impacts that some individuals and organizations have on the
financial market. Our characterization highlights the dynamics of the financial
market coverage and provides valuable insights reflecting broad discussions
regarding recent financial events and the world economy.",,
"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","[arxiv.Result.Author('Eugene Kharitonov'), arxiv.Result.Author('Damien Vincent'), arxiv.Result.Author('Zalán Borsos'), arxiv.Result.Author('Raphaël Marinier'), arxiv.Result.Author('Sertan Girgin'), arxiv.Result.Author('Olivier Pietquin'), arxiv.Result.Author('Matt Sharifi'), arxiv.Result.Author('Marco Tagliasacchi'), arxiv.Result.Author('Neil Zeghidour')]",2023-02-07 15:48:31+00:00,"We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can
be trained with minimal supervision. By combining two types of discrete speech
representations, we cast TTS as a composition of two sequence-to-sequence
tasks: from text to high-level semantic tokens (akin to ""reading"") and from
semantic tokens to low-level acoustic tokens (""speaking""). Decoupling these two
tasks enables training of the ""speaking"" module using abundant audio-only data,
and unlocks the highly efficient combination of pretraining and backtranslation
to reduce the need for parallel data when training the ""reading"" component. To
control the speaker identity, we adopt example prompting, which allows
SPEAR-TTS to generalize to unseen speakers using only a short sample of 3
seconds, without any explicit speaker representation or speaker-id labels. Our
experiments demonstrate that SPEAR-TTS achieves a character error rate that is
competitive with state-of-the-art methods using only 15 minutes of parallel
data, while matching ground-truth speech in terms of naturalness and acoustic
quality, as measured in subjective tests.",,
Revisiting Pre-training in Audio-Visual Learning,"[arxiv.Result.Author('Ruoxuan Feng'), arxiv.Result.Author('Wenke Xia'), arxiv.Result.Author('Di Hu')]",2023-02-07 15:34:14+00:00,"Pre-training technique has gained tremendous success in enhancing model
performance on various tasks, but found to perform worse than training from
scratch in some uni-modal situations. This inspires us to think: are the
pre-trained models always effective in the more complex multi-modal scenario,
especially for the heterogeneous modalities such as audio and visual ones? We
find that the answer is No. Specifically, we explore the effects of pre-trained
models on two audio-visual learning scenarios: cross-modal initialization and
multi-modal joint learning. When cross-modal initialization is applied, the
phenomena of ""dead channel"" caused by abnormal Batchnorm parameters hinders the
utilization of model capacity. Thus, we propose Adaptive Batchnorm
Re-initialization (ABRi) to better exploit the capacity of pre-trained models
for target tasks. In multi-modal joint learning, we find a strong pre-trained
uni-modal encoder would bring negative effects on the encoder of another
modality. To alleviate such problem, we introduce a two-stage Fusion Tuning
strategy, taking better advantage of the pre-trained knowledge while making the
uni-modal encoders cooperate with an adaptive masking method. The experiment
results show that our methods could further exploit pre-trained models'
potential and boost performance in audio-visual learning.",,
Network-based Statistics Distinguish Anomic and Broca Aphasia,"[arxiv.Result.Author('Xingpei Zhao'), arxiv.Result.Author('Nicholas Riccardi'), arxiv.Result.Author('Rutvik Desai'), arxiv.Result.Author('Dirk-Bart den Ouden'), arxiv.Result.Author('Julius Fridriksson'), arxiv.Result.Author('Yuan Wang')]",2023-02-07 04:33:18+00:00,"Aphasia is a speech-language impairment commonly caused by damage to the left
hemisphere. Due to the complexity of speech-language processing, the neural
mechanisms that underpin various symptoms between different types of aphasia
are still not fully understood. We used the network-based statistic method to
identify distinct subnetwork(s) of connections differentiating the
resting-state functional networks of the anomic and Broca groups. We identified
one such subnetwork that mainly involved the brain regions in the premotor,
primary motor, primary auditory, and primary sensory cortices in both
hemispheres. The majority of connections in the subnetwork were weaker in the
Broca group than the anomic group. The network properties of the subnetwork
were examined through complex network measures, which indicated that the
regions in the superior temporal gyrus and auditory cortex bilaterally exhibit
intensive interaction, and primary motor, premotor and primary sensory cortices
in the left hemisphere play an important role in information flow and overall
communication efficiency. These findings underlied articulatory difficulties
and reduced repetition performance in Broca aphasia, which are rarely observed
in anomic aphasia. This research provides novel findings into the resting-state
brain network differences between groups of individuals with anomic and Broca
aphasia. We identified a subnetwork of, rather than isolated, connections that
statistically differentiate the resting-state brain networks of the two groups,
in comparison with standard lesion symptom mapping results that yield isolated
connections.",,
Automatic Sleep Stage Classification with Cross-modal Self-supervised Features from Deep Brain Signals,"[arxiv.Result.Author('Chen Gong'), arxiv.Result.Author('Yue Chen'), arxiv.Result.Author('Yanan Sui'), arxiv.Result.Author('Luming Li')]",2023-02-07 03:21:33+00:00,"The detection of human sleep stages is widely used in the diagnosis and
intervention of neurological and psychiatric diseases. Some patients with deep
brain stimulator implanted could have their neural activities recorded from the
deep brain. Sleep stage classification based on deep brain recording has great
potential to provide more precise treatment for patients. The accuracy and
generalizability of existing sleep stage classifiers based on local field
potentials are still limited. We proposed an applicable cross-modal transfer
learning method for sleep stage classification with implanted devices. This
end-to-end deep learning model contained cross-modal self-supervised feature
representation, self-attention, and classification framework. We tested the
model with deep brain recording data from 12 patients with Parkinson's disease.
The best total accuracy reached 83.2% for sleep stage classification. Results
showed speech self-supervised features catch the conversion pattern of sleep
stages effectively. We provide a new method on transfer learning from acoustic
signals to local field potentials. This method supports an effective solution
for the insufficient scale of clinical data. This sleep stage classification
model could be adapted to chronic and continuous monitor sleep for Parkinson's
patients in daily life, and potentially utilized for more precise treatment in
deep brain-machine interfaces, such as closed-loop deep brain stimulation.","4 pages, 5 figures, 11th International IEEE EMBS Conference on Neural
  Engineering (NER)",
Context-Gloss Augmentation for Improving Arabic Target Sense Verification,"[arxiv.Result.Author('Sanad Malaysha'), arxiv.Result.Author('Mustafa Jarrar'), arxiv.Result.Author('Mohammed Khalilia')]",2023-02-06 21:24:02+00:00,"Arabic language lacks semantic datasets and sense inventories. The most
common semantically-labeled dataset for Arabic is the ArabGlossBERT, a
relatively small dataset that consists of 167K context-gloss pairs (about 60K
positive and 107K negative pairs), collected from Arabic dictionaries. This
paper presents an enrichment to the ArabGlossBERT dataset, by augmenting it
using (Arabic-English-Arabic) machine back-translation. Augmentation increased
the dataset size to 352K pairs (149K positive and 203K negative pairs). We
measure the impact of augmentation using different data configurations to
fine-tune BERT on target sense verification (TSV) task. Overall, the accuracy
ranges between 78% to 84% for different data configurations. Although our
approach performed at par with the baseline, we did observe some improvements
for some POS tags in some experiments. Furthermore, our fine-tuned models are
trained on a larger dataset covering larger vocabulary and contexts. We provide
an in-depth analysis of the accuracy for each part-of-speech (POS).",,"The 12th International Global Wordnet Conference (GWC2023), Global
  Wordnet Association. (pp. ). San Sebastian, Spain, 2023"
Autodecompose: A generative self-supervised model for semantic decomposition,[arxiv.Result.Author('Mohammad Reza Bonyadi')],2023-02-06 21:18:09+00:00,"We introduce Autodecompose, a novel self-supervised generative model that
decomposes data into two semantically independent properties: the desired
property, which captures a specific aspect of the data (e.g. the voice in an
audio signal), and the context property, which aggregates all other information
(e.g. the content of the audio signal), without any labels given. Autodecompose
uses two complementary augmentations, one that manipulates the context while
preserving the desired property and the other that manipulates the desired
property while preserving the context. The augmented variants of the data are
encoded by two encoders and reconstructed by a decoder. We prove that one of
the encoders embeds the desired property while the other embeds the context
property. We apply Autodecompose to audio signals to encode sound source (human
voice) and content. We pre-trained the model on YouTube and LibriSpeech
datasets and fine-tuned in a self-supervised manner without exposing the
labels. Our results showed that, using the sound source encoder of pre-trained
Autodecompose, a linear classifier achieves F1 score of 97.6\% in recognizing
the voice of 30 speakers using only 10 seconds of labeled samples, compared to
95.7\% for supervised models. Additionally, our experiments showed that
Autodecompose is robust against overfitting even when a large model is
pre-trained on a small dataset. A large Autodecompose model was pre-trained
from scratch on 60 seconds of audio from 3 speakers achieved over 98.5\% F1
score in recognizing those three speakers in other unseen utterances. We
finally show that the context encoder embeds information about the content of
the speech and ignores the sound source information.
  Our sample code for training the model, as well as examples for using the
pre-trained models are available here:
\url{https://github.com/rezabonyadi/autodecompose}",,
Improved Vehicle Sub-type Classification for Acoustic Traffic Monitoring,"[arxiv.Result.Author('Mohd Ashhad'), arxiv.Result.Author('Umang Goenka'), arxiv.Result.Author('Aaryan Jagetia'), arxiv.Result.Author('Parwin Akhtari'), arxiv.Result.Author('Sooraj K. Ambat'), arxiv.Result.Author('Mary Samuel')]",2023-02-06 17:26:51+00:00,"The detection and classification of vehicles on the road is a crucial task
for traffic monitoring. Usually, Computer Vision (CV) algorithms dominate the
task of vehicle classification on the road, but CV methodologies might suffer
in poor lighting conditions and require greater amounts of computational power.
Additionally, there is a privacy concern with installing cameras in sensitive
and secure areas. In contrast, acoustic traffic monitoring is cost-effective,
and can provide greater accuracy, particularly in low lighting conditions and
in places where cameras cannot be installed. In this paper, we consider the
task of acoustic vehicle sub-type classification, where we classify acoustic
signals into 4 classes: car, truck, bike, and no vehicle. We experimented with
Mel spectrograms, MFCC and GFCC as features and performed data pre-processing
to train a simple, well optimized CNN that performs well at the task. When used
with MFCC as features and careful data pre-processing, our proposed methodology
improves upon the established state-of-the-art baseline on the IDMT Traffic
dataset with an accuracy of 98.95%.","Accepted at Twenty-Ninth National Conference on Communications(NCC)
  23 - 26 February, Indian Institute of Technology Guwahati",
Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design,"[arxiv.Result.Author('Lyle Regenwetter'), arxiv.Result.Author('Akash Srivastava'), arxiv.Result.Author('Dan Gutfreund'), arxiv.Result.Author('Faez Ahmed')]",2023-02-06 16:34:16+00:00,"Deep generative models, such as Variational Autoencoders (VAEs), Generative
Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown
great promise in a variety of applications, including image and speech
synthesis, natural language processing, and drug discovery. However, when
applied to engineering design problems, evaluating the performance of these
models can be challenging, as traditional statistical metrics based on
likelihood may not fully capture the requirements of engineering applications.
This paper doubles as a review and a practical guide to evaluation metrics for
deep generative models (DGMs) in engineering design. We first summarize
well-accepted `classic' evaluation metrics for deep generative models grounded
in machine learning theory and typical computer science applications. Using
case studies, we then highlight why these metrics seldom translate well to
design problems but see frequent use due to the lack of established
alternatives. Next, we curate a set of design-specific metrics which have been
proposed across different research communities and can be used for evaluating
deep generative models. These metrics focus on unique requirements in design
and engineering, such as constraint satisfaction, functional performance,
novelty, and conditioning. We structure our review and discussion as a set of
practical selection criteria and usage guidelines. Throughout our discussion,
we apply the metrics to models trained on simple 2-dimensional example
problems. Finally, to illustrate the selection process and classic usage of the
presented metrics, we evaluate three deep generative models on a multifaceted
bicycle frame design problem considering performance target achievement, design
novelty, and geometric constraints. We publicly release the code for the
datasets, models, and metrics used throughout the paper at
decode.mit.edu/projects/metrics/.",,
Audio Representation Learning by Distilling Video as Privileged Information,"[arxiv.Result.Author('Amirhossein Hajavi'), arxiv.Result.Author('Ali Etemad')]",2023-02-06 15:09:34+00:00,"Deep audio representation learning using multi-modal audio-visual data often
leads to a better performance compared to uni-modal approaches. However, in
real-world scenarios both modalities are not always available at the time of
inference, leading to performance degradation by models trained for multi-modal
inference. In this work, we propose a novel approach for deep audio
representation learning using audio-visual data when the video modality is
absent at inference. For this purpose, we adopt teacher-student knowledge
distillation under the framework of learning using privileged information
(LUPI). While the previous methods proposed for LUPI use soft-labels generated
by the teacher, in our proposed method we use embeddings learned by the teacher
to train the student network. We integrate our method in two different
settings: sequential data where the features are divided into multiple segments
throughout time, and non-sequential data where the entire features are treated
as one whole segment. In the non-sequential setting both the teacher and
student networks are comprised of an encoder component and a task header. We
use the embeddings produced by the encoder component of the teacher to train
the encoder of the student, while the task header of the student is trained
using ground-truth labels. In the sequential setting, the networks have an
additional aggregation component that is placed between the encoder and task
header. We use two sets of embeddings produced by the encoder and aggregation
component of the teacher to train the student. Similar to the non-sequential
setting, the task header of the student network is trained using ground-truth
labels. We test our framework on two different audio-visual tasks, namely
speaker recognition and speech emotion recognition and show considerable
improvements over sole audio-based recognition as well as prior works that use
LUPI.",,
Residual Information in Deep Speaker Embedding Architectures,[arxiv.Result.Author('Adriana Stan')],2023-02-06 12:37:57+00:00,"Speaker embeddings represent a means to extract representative vectorial
representations from a speech signal such that the representation pertains to
the speaker identity alone. The embeddings are commonly used to classify and
discriminate between different speakers. However, there is no objective measure
to evaluate the ability of a speaker embedding to disentangle the speaker
identity from the other speech characteristics. This means that the embeddings
are far from ideal, highly dependent on the training corpus and still include a
degree of residual information pertaining to factors such as linguistic
content, recording conditions or speaking style of the utterance. This paper
introduces an analysis over six sets of speaker embeddings extracted with some
of the most recent and high-performing DNN architectures, and in particular,
the degree to which they are able to truly disentangle the speaker identity
from the speech signal. To correctly evaluate the architectures, a large
multi-speaker parallel speech dataset is used. The dataset includes 46 speakers
uttering the same set of prompts, recorded in either a professional studio or
their home environments. The analysis looks into the intra- and inter-speaker
similarity measures computed over the different embedding sets, as well as if
simple classification and regression methods are able to extract several
residual information factors from the speaker embeddings. The results show that
the discriminative power of the analyzed embeddings is very high, yet across
all the analyzed architectures, residual information is still present in the
representations in the form of a high correlation to the recording conditions,
linguistic contents and utterance duration.",,"Mathematics 2022, 10(21), 3927"
Hatemongers ride on echo chambers to escalate hate speech diffusion,"[arxiv.Result.Author('Vasu Goel'), arxiv.Result.Author('Dhruv Sahnan'), arxiv.Result.Author('Subhabrata Dutta'), arxiv.Result.Author('Anil Bandhakavi'), arxiv.Result.Author('Tanmoy Chakraborty')]",2023-02-05 20:30:48+00:00,"Recent years have witnessed a swelling rise of hateful and abusive content
over online social networks. While detection and moderation of hate speech have
been the early go-to countermeasures, the solution requires a deeper
exploration of the dynamics of hate generation and propagation. We analyze more
than 32 million posts from over 6.8 million users across three popular online
social networks to investigate the interrelations between hateful behavior,
information dissemination, and polarised organization mediated by echo
chambers. We find that hatemongers play a more crucial role in governing the
spread of information compared to singled-out hateful content. This observation
holds for both the growth of information cascades as well as the conglomeration
of hateful actors. Dissection of the core-wise distribution of these networks
points towards the fact that hateful users acquire a more well-connected
position in the social network and often flock together to build up information
cascades. We observe that this cohesion is far from mere organized behavior;
instead, in these networks, hatemongers dominate the echo chambers -- groups of
users actively align themselves to specific ideological positions. The observed
dominance of hateful users to inflate information cascades is primarily via
user interactions amplified within these echo chambers. We conclude our study
with a cautionary note that popularity-based recommendation of content is
susceptible to be exploited by hatemongers given their potential to escalate
content popularity via echo-chambered interactions.",Accepted in PNAS Nexus,
cross-modal fusion techniques for utterance-level emotion recognition from text and speech,"[arxiv.Result.Author('Jiachen Luo'), arxiv.Result.Author('Huy Phan'), arxiv.Result.Author('Joshua Reiss')]",2023-02-05 18:16:12+00:00,"Multimodal emotion recognition (MER) is a fundamental complex research
problem due to the uncertainty of human emotional expression and the
heterogeneity gap between different modalities. Audio and text modalities are
particularly important for a human participant in understanding emotions.
Although many successful attempts have been designed multimodal representations
for MER, there still exist multiple challenges to be addressed: 1) bridging the
heterogeneity gap between multimodal features and model inter- and intra-modal
interactions of multiple modalities; 2) effectively and efficiently modelling
the contextual dynamics in the conversation sequence. In this paper, we propose
Cross-Modal RoBERTa (CM-RoBERTa) model for emotion detection from spoken audio
and corresponding transcripts. As the core unit of the CM-RoBERTa, parallel
self- and cross- attention is designed to dynamically capture inter- and
intra-modal interactions of audio and text. Specially, the mid-level fusion and
residual module are employed to model long-term contextual dependencies and
learn modality-specific patterns. We evaluate the approach on the MELD dataset
and the experimental results show the proposed approach achieves the
state-of-art performance on the dataset.","6 pages, 2 figures",
deep learning of segment-level feature representation for speech emotion recognition in conversations,"[arxiv.Result.Author('Jiachen Luo'), arxiv.Result.Author('Huy Phan'), arxiv.Result.Author('Joshua Reiss')]",2023-02-05 16:15:46+00:00,"Accurately detecting emotions in conversation is a necessary yet challenging
task due to the complexity of emotions and dynamics in dialogues. The emotional
state of a speaker can be influenced by many different factors, such as
interlocutor stimulus, dialogue scene, and topic. In this work, we propose a
conversational speech emotion recognition method to deal with capturing
attentive contextual dependency and speaker-sensitive interactions. First, we
use a pretrained VGGish model to extract segment-based audio representation in
individual utterances. Second, an attentive bi-directional gated recurrent unit
(GRU) models contextual-sensitive information and explores intra- and
inter-speaker dependencies jointly in a dynamic manner. The experiments
conducted on the standard conversational dataset MELD demonstrate the
effectiveness of the proposed method when compared against state-of the-art
methods.","6 pages, 4 figures",
PAMP: A unified framework boosting low resource automatic speech recognition,"[arxiv.Result.Author('Zeping Min'), arxiv.Result.Author('Qian Ge'), arxiv.Result.Author('Zhong Li'), arxiv.Result.Author('Weinan E')]",2023-02-05 09:49:18+00:00,"We propose a novel text-to-speech (TTS) data augmentation framework for low
resource automatic speech recognition (ASR) tasks, named phoneme audio mix up
(PAMP). The PAMP method is highly interpretable and can incorporate prior
knowledge of pronunciation rules. Furthermore, PAMP can be easily deployed in
almost any language, extremely for low resource ASR tasks. Extensive
experiments have demonstrated the great effectiveness of PAMP on low resource
ASR tasks: we achieve a \textbf{10.84\%} character error rate (CER) on the
common voice Cantonese ASR task, bringing a great relative improvement of about
\textbf{30\%} compared to the previous state-of-the-art which was achieved by
fine-tuning the wav2vec2 pretrained model.",,
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,"[arxiv.Result.Author('Han He'), arxiv.Result.Author('Jinho D. Choi')]",2023-02-05 01:37:26+00:00,"Sequence-to-Sequence (S2S) models have achieved remarkable success on various
text generation tasks. However, learning complex structures with S2S models
remains challenging as external neural modules and additional lexicons are
often supplemented to predict non-textual outputs. We present a systematic
study of S2S modeling using contained decoding on four core tasks:
part-of-speech tagging, named entity recognition, constituency and dependency
parsing, to develop efficient exploitation methods costing zero extra
parameters. In particular, 3 lexically diverse linearization schemas and
corresponding constrained decoding methods are designed and evaluated.
Experiments show that although more lexicalized schemas yield longer output
sequences that require heavier training, their sequences being closer to
natural language makes them easier to learn. Moreover, S2S models using our
constrained decoding outperform other S2S approaches using external resources.
Our best models perform better than or comparably to the state-of-the-art for
all 4 tasks, lighting a promise for S2S models to generate non-sequential
structures.","Accepted to TACL 2023: Transactions of the Association for
  Computational Linguistics, post-acceptance final version",
Multi-Source Diffusion Models for Simultaneous Music Generation and Separation,"[arxiv.Result.Author('Giorgio Mariani'), arxiv.Result.Author('Irene Tallini'), arxiv.Result.Author('Emilian Postolache'), arxiv.Result.Author('Michele Mancusi'), arxiv.Result.Author('Luca Cosmo'), arxiv.Result.Author('Emanuele Rodolà')]",2023-02-04 23:18:36+00:00,"In this work, we define a diffusion-based generative model capable of both
music synthesis and source separation by learning the score of the joint
probability density of sources sharing a context. Alongside the classic total
inference tasks (i.e. generating a mixture, separating the sources), we also
introduce and experiment on the partial inference task of source imputation,
where we generate a subset of the sources given the others (e.g., play a piano
track that goes well with the drums). Additionally, we introduce a novel
inference method for the separation task. We train our model on Slakh2100, a
standard dataset for musical source separation, provide qualitative results in
the generation settings, and showcase competitive quantitative results in the
separation setting. Our method is the first example of a single model that can
handle both generation and separation tasks, thus representing a step toward
general audio models.","Demo page:
  https://gladia-research-group.github.io/multi-source-diffusion-models/",
LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers,"[arxiv.Result.Author('Feng Xue'), arxiv.Result.Author('Yu Li'), arxiv.Result.Author('Deyin Liu'), arxiv.Result.Author('Yincen Xie'), arxiv.Result.Author('Lin Wu'), arxiv.Result.Author('Richang Hong')]",2023-02-04 10:22:18+00:00,"Lipreading refers to understanding and further translating the speech of a
speaker in the video into natural language. State-of-the-art lipreading methods
excel in interpreting overlap speakers, i.e., speakers appear in both training
and inference sets. However, generalizing these methods to unseen speakers
incurs catastrophic performance degradation due to the limited number of
speakers in training bank and the evident visual variations caused by the
shape/color of lips for different speakers. Therefore, merely depending on the
visible changes of lips tends to cause model overfitting. To address this
problem, we propose to use multi-modal features across visual and landmarks,
which can describe the lip motion irrespective to the speaker identities. Then,
we develop a sentence-level lipreading framework based on visual-landmark
transformers, namely LipFormer. Specifically, LipFormer consists of a lip
motion stream, a facial landmark stream, and a cross-modal fusion. The
embeddings from the two streams are produced by self-attention, which are fed
to the cross-attention module to achieve the alignment between visuals and
landmarks. Finally, the resulting fused features can be decoded to output texts
by a cascade seq2seq model. Experiments demonstrate that our method can
effectively enhance the model generalization to unseen speakers.",Under review,
AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,"[arxiv.Result.Author('Susan Liang'), arxiv.Result.Author('Chao Huang'), arxiv.Result.Author('Yapeng Tian'), arxiv.Result.Author('Anurag Kumar'), arxiv.Result.Author('Chenliang Xu')]",2023-02-04 04:17:19+00:00,"Human perception of the complex world relies on a comprehensive analysis of
multi-modal signals, and the co-occurrences of audio and video signals provide
humans with rich cues. This paper focuses on novel audio-visual scene synthesis
in the real world. Given a video recording of an audio-visual scene, the task
is to synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that audio-visual scene. Directly using a NeRF-based model for
audio synthesis is insufficient due to its lack of prior knowledge and acoustic
supervision. To tackle the challenges, we first propose an acoustic-aware audio
generation module that integrates our prior knowledge of audio propagation into
NeRF, in which we associate audio generation with the 3D geometry of the visual
environment. In addition, we propose a coordinate transformation module that
expresses a viewing direction relative to the sound source. Such a direction
transformation helps the model learn sound source-centric acoustic fields.
Moreover, we utilize a head-related impulse response function to synthesize
pseudo binaural audio for data augmentation that strengthens training. We
qualitatively and quantitatively demonstrate the advantage of our model on
real-world audio-visual scenes. We refer interested readers to view our video
results for convincing comparisons.",,
Lived Experience Matters: Automatic Detection of Stigma toward People Who Use Substances on Social Media,"[arxiv.Result.Author('Salvatore Giorgi'), arxiv.Result.Author('Douglas Bellew'), arxiv.Result.Author('Daniel Roy Sadek Habib'), arxiv.Result.Author('Joao Sedoc'), arxiv.Result.Author('Chase Smitterberg'), arxiv.Result.Author('Amanda Devoto'), arxiv.Result.Author('McKenzie Himelein-Wachowiak'), arxiv.Result.Author('Brenda Curtis')]",2023-02-04 02:26:08+00:00,"Stigma toward people who use substances (PWUS) is a leading barrier to
seeking treatment. Further, those in treatment are more likely to drop out if
they experience higher levels of stigmatization. While related concepts of hate
speech and toxicity, including those targeted toward vulnerable populations,
have been the focus of automatic content moderation research, stigma and, in
particular, people who use substances have not. This paper explores stigma
toward PWUS using a data set of roughly 5,000 public Reddit posts. We performed
a crowd-sourced annotation task where workers are asked to annotate each post
for the presence of stigma toward PWUS and answer a series of questions related
to their experiences with substance use. Results show that workers who use
substances or know someone with a substance use disorder are more likely to
rate a post as stigmatizing. Building on this, we use a supervised machine
learning framework that centers workers with lived substance use experience to
label each Reddit post as stigmatizing. Modeling person-level demographics in
addition to comment-level language results in a classification accuracy (as
measured by AUC) of 0.69 -- a 17% increase over modeling language alone.
Finally, we explore the linguist cues which distinguish stigmatizing content:
PWUS substances and those who don't agree that language around othering
(""people"", ""they"") and terms like ""addict"" are stigmatizing, while PWUS (as
opposed to those who do not) find discussions around specific substances more
stigmatizing. Our findings offer insights into the nature of perceived stigma
in substance use. Additionally, these results further establish the subjective
nature of such machine learning tasks, highlighting the need for understanding
their social contexts.",,
PSST! Prosodic Speech Segmentation with Transformers,"[arxiv.Result.Author('Nathan Roll'), arxiv.Result.Author('Calbert Graham'), arxiv.Result.Author('Simon Todd')]",2023-02-03 20:09:17+00:00,"Self-attention mechanisms have enabled transformers to achieve
superhuman-level performance on many speech-to-text (STT) tasks, yet the
challenge of automatic prosodic segmentation has remained unsolved. In this
paper we finetune Whisper, a pretrained STT model, to annotate intonation unit
(IU) boundaries by repurposing low-frequency tokens. Our approach achieves an
accuracy of 95.8%, outperforming previous methods without the need for
large-scale labeled data or enterprise grade compute resources. We also
diminish input signals by applying a series of filters, finding that low pass
filters at a 3.2 kHz level improve segmentation performance in out of sample
and out of distribution contexts. We release our model as both a transcription
tool and a baseline for further improvements in prosodic segmentation.","5 pages, 3 figures. For associated repository, see
  https://github.com/Nathan-Roll1/psst",
Lexical Simplification using multi level and modular approach,"[arxiv.Result.Author('Nikita Katyal'), arxiv.Result.Author('Pawan Kumar Rajpoot')]",2023-02-03 15:57:54+00:00,"Text Simplification is an ongoing problem in Natural Language Processing,
solution to which has varied implications. In conjunction with the TSAR-2022
Workshop @EMNLP2022 Lexical Simplification is the process of reducing the
lexical complexity of a text by replacing difficult words with easier to read
(or understand) expressions while preserving the original information and
meaning. This paper explains the work done by our team ""teamPN"" for English sub
task. We created a modular pipeline which combines modern day transformers
based models with traditional NLP methods like paraphrasing and verb sense
disambiguation. We created a multi level and modular pipeline where the target
text is treated according to its semantics(Part of Speech Tag). Pipeline is
multi level as we utilize multiple source models to find potential candidates
for replacement, It is modular as we can switch the source models and their
weight-age in the final re-ranking.",,
Relating EEG to continuous speech using deep neural networks: a review,"[arxiv.Result.Author('Corentin Puffay'), arxiv.Result.Author('Bernd Accou'), arxiv.Result.Author('Lies Bollens'), arxiv.Result.Author('Mohammad Jalilpour Monesi'), arxiv.Result.Author('Jonas Vanthornhout'), arxiv.Result.Author('Hugo Van hamme'), arxiv.Result.Author('Tom Francart')]",2023-02-03 13:51:01+00:00,"Objective. When a person listens to continuous speech, a corresponding
response is elicited in the brain and can be recorded using
electroencephalography (EEG). Linear models are presently used to relate the
EEG recording to the corresponding speech signal. The ability of linear models
to find a mapping between these two signals is used as a measure of neural
tracking of speech. Such models are limited as they assume linearity in the
EEG-speech relationship, which omits the nonlinear dynamics of the brain. As an
alternative, deep learning models have recently been used to relate EEG to
continuous speech, especially in auditory attention decoding (AAD) and
single-speech-source paradigms. Approach. This paper reviews and comments on
deep-learning-based studies that relate EEG to continuous speech in AAD and
single-speech-source paradigms. We point out recurrent methodological pitfalls
and the need for a standard benchmark of model analysis. Main results. We
gathered 28 studies. The main methodological issues we found are biased
cross-validations, data leakage leading to over-fitted models, or
disproportionate data size compared to the model's complexity. In addition, we
address requirements for a standard benchmark model analysis, such as public
datasets, common evaluation metrics, and good practices for the match-mismatch
task. Significance. We are the first to present a review paper summarizing the
main deep-learning-based studies that relate EEG to speech while addressing
methodological pitfalls and important considerations for this newly expanding
field. Our study is particularly relevant given the growing application of deep
learning in EEG-speech decoding.",,
Efficient Domain Adaptation for Speech Foundation Models,"[arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Dongseong Hwang'), arxiv.Result.Author('Zhouyuan Huo'), arxiv.Result.Author('Junwen Bai'), arxiv.Result.Author('Guru Prakash'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Khe Chai Sim'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Wei Han'), arxiv.Result.Author('Trevor Strohman'), arxiv.Result.Author('Francoise Beaufays')]",2023-02-03 02:10:35+00:00,"Foundation models (FMs), that are trained on broad data at scale and are
adaptable to a wide range of downstream tasks, have brought large interest in
the research community. Benefiting from the diverse data sources such as
different modalities, languages and application domains, foundation models have
demonstrated strong generalization and knowledge transfer capabilities. In this
paper, we present a pioneering study towards building an efficient solution for
FM-based speech recognition systems. We adopt the recently developed
self-supervised BEST-RQ for pretraining, and propose the joint finetuning with
both source and unsupervised target domain data using JUST Hydra. The FM
encoder adapter and decoder are then finetuned to the target domain with a
small amount of supervised in-domain data. On a large-scale YouTube and Voice
Search task, our method is shown to be both data and model parameter efficient.
It achieves the same quality with only 21.6M supervised in-domain data and
130.8M finetuned parameters, compared to the 731.1M model trained from scratch
on additional 300M supervised in-domain data.",,
SPADE: Self-supervised Pretraining for Acoustic DisEntanglement,"[arxiv.Result.Author('John Harvill'), arxiv.Result.Author('Jarred Barber'), arxiv.Result.Author('Arun Nair'), arxiv.Result.Author('Ramin Pishehvar')]",2023-02-03 01:36:38+00:00,"Self-supervised representation learning approaches have grown in popularity
due to the ability to train models on large amounts of unlabeled data and have
demonstrated success in diverse fields such as natural language processing,
computer vision, and speech. Previous self-supervised work in the speech domain
has disentangled multiple attributes of speech such as linguistic content,
speaker identity, and rhythm. In this work, we introduce a self-supervised
approach to disentangle room acoustics from speech and use the acoustic
representation on the downstream task of device arbitration. Our results
demonstrate that our proposed approach significantly improves performance over
a baseline when labeled training data is scarce, indicating that our
pretraining scheme learns to encode room acoustic information while remaining
invariant to other attributes of the speech signal.",,
Machine Learning Extreme Acoustic Non-reciprocity in a Linear Waveguide with Multiple Nonlinear Asymmetric Gates,"[arxiv.Result.Author('Anargyros Michaloliakos'), arxiv.Result.Author('Chongan Wang'), arxiv.Result.Author('Alexander F. Vakakis')]",2023-02-02 17:28:04+00:00,"This work is a study of acoustic non-reciprocity exhibited by a passive
one-dimensional linear waveguide incorporating two local strongly nonlinear,
asymmetric gates. Two local nonlinear gates break the symmetry and linearity of
the waveguide, yielding strong global non-reciprocal acoustics, in the way that
extremely different acoustical responses occur depending on the side of
application of harmonic excitation. To the authors' best knowledge that the
present two-gated waveguide is capable of extremely high acoustic
non-reciprocity, at a much higher level to what is reported by active or
passive devices in the current literature; moreover, this extreme performance
combines with acceptable levels of transmissibility in the desired direction of
wave propagation. Machine learning is utilized for predictive design of this
gated waveguide in terms of the measures of transmissibility and
non-reciprocity, with the aim of reducing the required computational time for
high-dimensional parameter space analysis. The study sheds new light into the
physics of these media and considers the advantages and limitations of using
neural networks to analyze this type of physical problems. In the predicted
desirable parameter space for intense non-reciprocity, the maximum
transmissibility reaches as much as 40%, and the transmitted energy from
upstream to downstream varies up to nine orders of magnitude, depending on the
direction of wave transmission. The machine learning tools along with the
numerical methods of this work can inform predictive designs of practical
non-reciprocal waveguides and acoustic metamaterials that incorporate local
nonlinear gates. The current paper shows that combinations of nonlinear gates
can lead to extremely high non-reciprocity while maintaining desired levels of
transmissibility.",,
New threats to society from free-speech social media platforms,"[arxiv.Result.Author('Dominik Bär'), arxiv.Result.Author('Nicolas Pröllochs'), arxiv.Result.Author('Stefan Feuerriegel')]",2023-02-02 17:08:12+00:00,"In recent years, several free-speech social media platforms (so-called
""alt-techs"") have emerged, such as Parler, Gab, and Telegram. These platforms
market themselves as alternatives to mainstream social media and proclaim
""free-speech"" due to the absence of content moderation, which has been
attracting a large base of partisan users, extremists, and supporters of
conspiracy theories. In this comment, we discuss some of the threats that
emerge from such social media platforms and call for more policy efforts
directed at understanding and countering the risks for society.",,
Complex Dynamic Neurons Improved Spiking Transformer Network for Efficient Automatic Speech Recognition,"[arxiv.Result.Author('Minglun Han'), arxiv.Result.Author('Qingyu Wang'), arxiv.Result.Author('Tielin Zhang'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Duzhen Zhang'), arxiv.Result.Author('Bo Xu')]",2023-02-02 16:20:27+00:00,"The spiking neural network (SNN) using leaky-integrated-and-fire (LIF)
neurons has been commonly used in automatic speech recognition (ASR) tasks.
However, the LIF neuron is still relatively simple compared to that in the
biological brain. Further research on more types of neurons with different
scales of neuronal dynamics is necessary. Here we introduce four types of
neuronal dynamics to post-process the sequential patterns generated from the
spiking transformer to get the complex dynamic neuron improved spiking
transformer neural network (DyTr-SNN). We found that the DyTr-SNN could handle
the non-toy automatic speech recognition task well, representing a lower
phoneme error rate, lower computational cost, and higher robustness. These
results indicate that the further cooperation of SNNs and neural dynamics at
the neuron and network scales might have much in store for the future,
especially on the ASR tasks.","8 pages. Spiking Neural Networks, ASR, Speech and Language
  Processing. The first three authors contributed equally",
Goniometers are a Powerful Acoustic Feature for Music Information Retrieval Tasks,[arxiv.Result.Author('Tim Ziemer')],2023-02-02 13:23:54+00:00,"Goniometers, also known as Phase Scopes or Vector Scopes, are audio metering
tools that help music producers and mixing engineers monitor spatial aspects of
a music mix, such as the stereo panorama, the width of single sources, the
amount and diffuseness of reverberation as well as phase cancellations that may
occur on the sweet-spot and in a mono-mixdown. In addition, they implicitly
inform about the dynamics of the sound. Self-organizing maps trained with a
goniometer, are consulted to explore the usefulness of this acoustic feature
for music information retrieval tasks. One can see that goniometers are able to
classify different genres and cluster a single album. The advantage of
goniometers is the causality: Music producers and mixing engineers consciously
consult goniometers to reach their desired sound, which is not the case for
other acoustic features, from Zero-Crossing Rate to Mel-Frequency Cepstral
Coefficients.",,
Generalized Uncertainty Principles for Quantum Cryptography,[arxiv.Result.Author('Randy Kuang')],2023-02-02 11:40:32+00:00,"We know the classical public cryptographic algorithms are based on certain
NP-hard problems such as the integer factoring in RSA and the discrete
logarithm in Diffie-Hellman. They are going to be vulnerable with
fault-tolerant quantum computers. We also know that the uncertainty principle
for quantum bits or qubits such as quantum key distribution or QKD based on the
quantum uncertainty principle offers the information theoretical security. The
interesting implication with the paradigm shifts from classical computing to
quantum computing is that the NP-hardness used for classical cryptography may
shift to the uncertainty principles for quantum cryptography including quantum
symmetric encryption, post-quantum cryptography, as well as quantum encryption
in phase space for coherent optical communications. This paper would like to
explore those so-called generalized uncertainty principles and explain what
their implications are for quantum security. We identified three generalized
uncertainty principles offering quantum security: non-commutability between
permutation gates, non-commutability between the displacement and phase shift
operators for coherent states, and the modular Diophantine Equation Problem in
general linear algebra for post-quantum cryptography.","7 pages, 5 figures, conference invited speech at ICCCAS, then to be
  published by JCM",
Content Adaptive Wavelet Lifting for Scalable Lossless Video Coding,"[arxiv.Result.Author('Daniela Lanz'), arxiv.Result.Author('Christian Herbert'), arxiv.Result.Author('André Kaup')]",2023-02-02 10:51:23+00:00,"Scalable lossless video coding is an important aspect for many professional
applications. Wavelet-based video coding decomposes an input sequence into a
lowpass and a highpass subband by filtering along the temporal axis. The
lowpass subband can be used for previewing purposes, while the highpass subband
provides the residual content for lossless reconstruction of the original
sequence. The recursive application of the wavelet transform to the lowpass
subband of the previous stage yields coarser temporal resolutions of the input
sequence. This allows for lower bit rates, but also affects the visual quality
of the lowpass subband. So far, the number of total decomposition levels is
determined for the entire input sequence in advance. However, if the motion in
the video sequence is strong or if abrupt scene changes occur, a further
decomposition leads to a low-quality lowpass subband. Therefore, we propose a
content adaptive wavelet transform, which locally adapts the depth of the
decomposition to the content of the input sequence. Thereby, the visual quality
of the low-pass subband is increased by up to 10.28 dB compared to a uniform
wavelet transform with the same number of total decomposition levels, while the
required rate is reduced by 1.06% additionally.",,"IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brighton, UK, 2019, pp. 1782-1786"
Speech Enhancement for Virtual Meetings on Cellular Networks,"[arxiv.Result.Author('Hojeong Lee'), arxiv.Result.Author('Minseon Gwak'), arxiv.Result.Author('Kawon Lee'), arxiv.Result.Author('Minjeong Kim'), arxiv.Result.Author('Joseph Konan'), arxiv.Result.Author('Ojas Bhargave')]",2023-02-02 04:35:48+00:00,"We study speech enhancement using deep learning (DL) for virtual meetings on
cellular devices, where transmitted speech has background noise and
transmission loss that affects speech quality. Since the Deep Noise Suppression
(DNS) Challenge dataset does not contain practical disturbance, we collect a
transmitted DNS (t-DNS) dataset using Zoom Meetings over T-Mobile network. We
select two baseline models: Demucs and FullSubNet. The Demucs is an end-to-end
model that takes time-domain inputs and outputs time-domain denoised speech,
and the FullSubNet takes time-frequency-domain inputs and outputs the energy
ratio of the target speech in the inputs. The goal of this project is to
enhance the speech transmitted over the cellular networks using deep learning
models.",,
Scene2BIR: Material-aware learning-based binaural impulse response generator for reconstructed real-world 3D scenes,"[arxiv.Result.Author('Anton Jeran Ratnarajah'), arxiv.Result.Author('Dinesh Manocha')]",2023-02-02 04:09:23+00:00,"We present an end-to-end binaural impulse response generator (BIR) to
generate plausible sounds in real-time for real-world models. Our approach uses
a novel neural-network-based BIR generator (Scene2BIR) for the reconstructed 3D
model. We propose a graph neural network that uses both the material and the
topology information of the 3D scenes and generates a scene latent vector.
Moreover, we use a conditional generative adversarial network (CGAN) to
generate BIRs from the scene latent vector. Our network is able to handle holes
or other artifacts in the reconstructed 3D mesh model. We present an efficient
cost function to the generator network to incorporate spatial audio effects.
Given the source and the listener position, our approach can generate a BIR in
0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU and can easily handle
multiple sources. We have evaluated the accuracy of our approach with
real-world captured BIRs and an interactive geometric sound propagation
algorithm.",Project page: https://anton-jeran.github.io/S2BIR/,
Improving Rare Words Recognition through Homophone Extension and Unified Writing for Low-resource Cantonese Speech Recognition,"[arxiv.Result.Author('HoLam Chung'), arxiv.Result.Author('Junan Li'), arxiv.Result.Author('Pengfei Liu1'), arxiv.Result.Author('Wai-Kim Leung'), arxiv.Result.Author('Xixin Wu'), arxiv.Result.Author('Helen Meng')]",2023-02-02 02:46:32+00:00,"Homophone characters are common in tonal syllable-based languages, such as
Mandarin and Cantonese. The data-intensive end-to-end Automatic Speech
Recognition (ASR) systems are more likely to mis-recognize homophone characters
and rare words under low-resource settings. For the problem of lowresource
Cantonese speech recognition, this paper presents a novel homophone extension
method to integrate human knowledge of the homophone lexicon into the beam
search decoding process with language model re-scoring. Besides, we propose an
automatic unified writing method to merge the variants of Cantonese characters
and standardize speech annotation guidelines, which enables more efficient
utilization of labeled utterances by providing more samples for the merged
characters. We empirically show that both homophone extension and unified
writing improve the recognition performance significantly on both in-domain and
out-of-domain test sets, with an absolute Character Error Rate (CER) decrease
of around 5% and 18%.","The 13th International Symposium on Chinese Spoken Language
  Processing (ISCSLP 2022)",Published in ISCSLP 2022
FAVOR#: Sharp Attention Kernel Approximations via New Classes of Positive Random Features,"[arxiv.Result.Author('Valerii Likhosherstov'), arxiv.Result.Author('Krzysztof Choromanski'), arxiv.Result.Author('Avinava Dubey'), arxiv.Result.Author('Frederick Liu'), arxiv.Result.Author('Tamas Sarlos'), arxiv.Result.Author('Adrian Weller')]",2023-02-01 22:43:29+00:00,"The problem of efficient approximation of a linear operator induced by the
Gaussian or softmax kernel is often addressed using random features (RFs) which
yield an unbiased approximation of the operator's result. Such operators emerge
in important applications ranging from kernel methods to efficient
Transformers. We propose parameterized, positive, non-trigonometric RFs which
approximate Gaussian and softmax-kernels. In contrast to traditional RF
approximations, parameters of these new methods can be optimized to reduce the
variance of the approximation, and the optimum can be expressed in closed form.
We show that our methods lead to variance reduction in practice ($e^{10}$-times
smaller variance and beyond) and outperform previous methods in a kernel
regression task. Using our proposed mechanism, we also present FAVOR#, a method
for self-attention approximation in Transformers. We show that FAVOR#
outperforms other random feature methods in speech modelling and natural
language processing.",,
Visually Grounded Keyword Detection and Localisation for Low-Resource Languages,[arxiv.Result.Author('Kayode Kolawole Olaleye')],2023-02-01 21:32:15+00:00,"This study investigates the use of Visually Grounded Speech (VGS) models for
keyword localisation in speech. The study focusses on two main research
questions: (1) Is keyword localisation possible with VGS models and (2) Can
keyword localisation be done cross-lingually in a real low-resource setting?
Four methods for localisation are proposed and evaluated on an English dataset,
with the best-performing method achieving an accuracy of 57%. A new dataset
containing spoken captions in Yoruba language is also collected and released
for cross-lingual keyword localisation. The cross-lingual model obtains a
precision of 16% in actual keyword localisation and this performance can be
improved by initialising from a model pretrained on English data. The study
presents a detailed analysis of the model's success and failure modes and
highlights the challenges of using VGS models for keyword localisation in
low-resource settings.","PhD dissertation, University of Stellenbosch, 108 pages, submitted
  and accepted 2023",
Epic-Sounds: A Large-scale Dataset of Actions That Sound,"[arxiv.Result.Author('Jaesung Huh'), arxiv.Result.Author('Jacob Chalk'), arxiv.Result.Author('Evangelos Kazakos'), arxiv.Result.Author('Dima Damen'), arxiv.Result.Author('Andrew Zisserman')]",2023-02-01 18:19:37+00:00,"We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations
capturing temporal extents and class labels within the audio stream of the
egocentric videos. We propose an annotation pipeline where annotators
temporally label distinguishable audio segments and describe the action that
could have caused this sound. We identify actions that can be discriminated
purely from audio, through grouping these free-form descriptions of audio into
classes. For actions that involve objects colliding, we collect human
annotations of the materials of these objects (e.g. a glass object being placed
on a wooden surface), which we verify from visual labels, discarding
ambiguities. Overall, EPIC-SOUNDS includes 78.4k categorised segments of
audible events and actions, distributed across 44 classes as well as 39.2k
non-categorised segments. We train and evaluate two state-of-the-art audio
recognition models on our dataset, highlighting the importance of audio-only
labels and the limitations of current models to recognise actions that sound.","6 pages, 4 figures",
Prioritizing Speech Test Cases,"[arxiv.Result.Author('Zhou Yang'), arxiv.Result.Author('Jieke Shi'), arxiv.Result.Author('Muhammad Hilmi Asyrofi'), arxiv.Result.Author('Bowen Xu'), arxiv.Result.Author('Xin Zhou'), arxiv.Result.Author('DongGyun Han'), arxiv.Result.Author('David Lo')]",2023-02-01 09:20:08+00:00,"With the wide adoption of automated speech recognition (ASR) systems, it is
increasingly important to test and improve ASR systems. However, collecting and
executing speech test cases is usually expensive and time-consuming, motivating
us to strategically prioritize speech test cases. A key question is: how to
determine the ideal order of collecting and executing speech test cases to
uncover more errors as early as possible? Each speech test case consists of a
piece of audio and the corresponding reference text. In this work, we propose
PROPHET (PRiOritizing sPeecH tEsT), a tool that predicts potential
error-uncovering speech test cases only based on their reference texts. Thus,
PROPHET analyzes test cases and prioritizes them without running the ASR
system, which can analyze speech test cases at a large scale. We evaluate 6
different prioritization methods on 3 ASR systems and 12 datasets. Given the
same testing budget, we find that our approach uncovers 12.63% more wrongly
recognized words than the state-of-the-art method. We select test cases from
the prioritized list to fine-tune ASR systems and analyze how our approach can
improve the ASR system performance. Statistical tests show that our proposed
method can bring significantly larger performance improvement to ASR systems
than the existing baseline methods. Furthermore, we perform correlation
analysis and confirm that fine-tuning an ASR system using a dataset, on which
the model performs worse, tends to improve the performance more.",,
Jointist: Simultaneous Improvement of Multi-instrument Transcription and Music Source Separation via Joint Training,"[arxiv.Result.Author('Kin Wai Cheuk'), arxiv.Result.Author('Keunwoo Choi'), arxiv.Result.Author('Qiuqiang Kong'), arxiv.Result.Author('Bochen Li'), arxiv.Result.Author('Minz Won'), arxiv.Result.Author('Ju-Chiang Wang'), arxiv.Result.Author('Yun-Ning Hung'), arxiv.Result.Author('Dorien Herremans')]",2023-02-01 07:35:02+00:00,"In this paper, we introduce Jointist, an instrument-aware multi-instrument
framework that is capable of transcribing, recognizing, and separating multiple
musical instruments from an audio clip. Jointist consists of an instrument
recognition module that conditions the other two modules: a transcription
module that outputs instrument-specific piano rolls, and a source separation
module that utilizes instrument information and transcription results. The
joint training of the transcription and source separation modules serves to
improve the performance of both tasks. The instrument module is optional and
can be directly controlled by human users. This makes Jointist a flexible
user-controllable framework. Our challenging problem formulation makes the
model highly useful in the real world given that modern popular music typically
consists of multiple instruments. Its novelty, however, necessitates a new
perspective on how to evaluate such a model. In our experiments, we assess the
proposed model from various aspects, providing a new evaluation perspective for
multi-instrument transcription. Our subjective listening study shows that
Jointist achieves state-of-the-art performance on popular music, outperforming
existing multi-instrument transcription models such as MT3. We conducted
experiments on several downstream tasks and found that the proposed method
improved transcription by more than 1 percentage points (ppt.), source
separation by 5 SDR, downbeat detection by 1.8 ppt., chord recognition by 1.4
ppt., and key estimation by 1.4 ppt., when utilizing transcription results
obtained from Jointist.
  Demo available at \url{https://jointist.github.io/Demo}.",arXiv admin note: text overlap with arXiv:2206.10805,
Automated Sentiment and Hate Speech Analysis of Facebook Data by Employing Multilingual Transformer Models,"[arxiv.Result.Author('Ritumbra Manuvie'), arxiv.Result.Author('Saikat Chatterjee')]",2023-01-31 14:37:04+00:00,"In recent years, there has been a heightened consensus within academia and in
the public discourse that Social Media Platforms (SMPs), amplify the spread of
hateful and negative sentiment content. Researchers have identified how hateful
content, political propaganda, and targeted messaging contributed to real-world
harms including insurrections against democratically elected governments,
genocide, and breakdown of social cohesion due to heightened negative discourse
towards certain communities in parts of the world. To counter these issues,
SMPs have created semi-automated systems that can help identify toxic speech.
In this paper we analyse the statistical distribution of hateful and negative
sentiment contents within a representative Facebook dataset (n= 604,703)
scrapped through 648 public Facebook pages which identify themselves as
proponents (and followers) of far-right Hindutva actors. These pages were
identified manually using keyword searches on Facebook and on CrowdTangleand
classified as far-right Hindutva pages based on page names, page descriptions,
and discourses shared on these pages. We employ state-of-the-art, open-source
XLM-T multilingual transformer-based language models to perform sentiment and
hate speech analysis of the textual contents shared on these pages over a
period of 5.5 years. The result shows the statistical distributions of the
predicted sentiment and the hate speech labels; top actors, and top page
categories. We further discuss the benchmark performances and limitations of
these pre-trained language models.",,
InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt,"[arxiv.Result.Author('Dongchao Yang'), arxiv.Result.Author('Songxiang Liu'), arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Guangzhi Lei'), arxiv.Result.Author('Chao Weng'), arxiv.Result.Author('Helen Meng'), arxiv.Result.Author('Dong Yu')]",2023-01-31 14:26:52+00:00,"Expressive text-to-speech (TTS) aims to synthesize different speaking style
speech according to human's demands. Nowadays, there are two common ways to
control speaking styles: (1) Pre-defining a group of speaking style and using
categorical index to denote different speaking style. However, there are
limitations in the diversity of expressiveness, as these models can only
generate the pre-defined styles. (2) Using reference speech as style input,
which results in a problem that the extracted style information is not
intuitive or interpretable. In this study, we attempt to use natural language
as style prompt to control the styles in the synthetic speech, \textit{e.g.},
``Sigh tone in full of sad mood with some helpless feeling"". Considering that
there is no existing TTS corpus which is proper to benchmark this novel task,
we first construct a speech corpus, whose speech samples are annotated with not
only content transcriptions but also style descriptions in natural language.
Then we propose an expressive TTS model, named as InstructTTS, which is novel
in the sense of following aspects: (1) We fully take the advantage of
self-supervised learning and cross-modal metric learning, and propose a novel
three-stage training procedure to obtain a robust sentence embedding model,
which can effectively capture semantic information from the style prompts and
control the speaking style in the generated speech. (2) We propose to model
acoustic features in discrete latent space and train a novel discrete diffusion
probabilistic model to generate vector-quantized (VQ) acoustic tokens rather
than the commonly-used mel spectrogram. (3) We jointly apply mutual information
(MI) estimation and minimization during acoustic model training to minimize
style-speaker and style-content MI, avoiding possible content and speaker
information leakage from the style prompt.",,
An Analysis of Classification Approaches for Hit Song Prediction using Engineered Metadata Features with Lyrics and Audio Features,"[arxiv.Result.Author('Mengyisong Zhao'), arxiv.Result.Author('Morgan Harvey'), arxiv.Result.Author('David Cameron'), arxiv.Result.Author('Frank Hopfgartner'), arxiv.Result.Author('Valerie J. Gillet')]",2023-01-31 09:48:53+00:00,"Hit song prediction, one of the emerging fields in music information
retrieval (MIR), remains a considerable challenge. Being able to understand
what makes a given song a hit is clearly beneficial to the whole music
industry. Previous approaches to hit song prediction have focused on using
audio features of a record. This study aims to improve the prediction result of
the top 10 hits among Billboard Hot 100 songs using more alternative metadata,
including song audio features provided by Spotify, song lyrics, and novel
metadata-based features (title topic, popularity continuity and genre class).
Five machine learning approaches are applied, including: k-nearest neighbours,
Naive Bayes, Random Forest, Logistic Regression and Multilayer Perceptron. Our
results show that Random Forest (RF) and Logistic Regression (LR) with all
features (including novel features, song audio features and lyrics features)
outperforms other models, achieving 89.1% and 87.2% accuracy, and 0.91 and 0.93
AUC, respectively. Our findings also demonstrate the utility of our novel music
metadata features, which contributed most to the models' discriminative
performance.",,
GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis,"[arxiv.Result.Author('Zhenhui Ye'), arxiv.Result.Author('Ziyue Jiang'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('JinZheng He'), arxiv.Result.Author('Zhou Zhao')]",2023-01-31 05:56:06+00:00,"Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.",Accepted by ICLR2023. Project page: https://geneface.github.io/,
An Comparative Analysis of Different Pitch and Metrical Grid Encoding Methods in the Task of Sequential Music Generation,"[arxiv.Result.Author('Yuqiang Li'), arxiv.Result.Author('Shengchen Li'), arxiv.Result.Author('George Fazekas')]",2023-01-31 03:19:50+00:00,"Pitch and meter are two fundamental music features for symbolic music
generation tasks, where researchers usually choose different encoding methods
depending on specific goals. However, the advantages and drawbacks of different
encoding methods have not been frequently discussed. This paper presents a
integrated analysis of the influence of two low-level feature, pitch and meter,
on the performance of a token-based sequential music generation model. First,
the commonly used MIDI number encoding and a less used class-octave encoding
are compared. Second, an dense intra-bar metric grid is imposed to the encoded
sequence as auxiliary features. Different complexity and resolutions of the
metric grid are compared. For complexity, the single token approach and the
multiple token approach are compared; for grid resolution, 0 (ablation), 1
(bar-level), 4 (downbeat-level) 12, (8th-triplet-level) up to 64
(64th-note-grid-level) are compared; for duration resolution, 4, 8, 12 and 16
subdivisions per beat are compared. All different encodings are tested on
separately trained Transformer-XL models for a melody generation task.
Regarding distribution similarity of several objective evaluation metrics to
the test dataset, results suggest that the class-octave encoding significantly
outperforms the taken-for-granted MIDI encoding on pitch-related metrics; finer
grids and multiple-token grids improve the rhythmic quality, but also suffer
from over-fitting at early training stage. Results display a general phenomenon
of over-fitting from two aspects, the pitch embedding space and the test loss
of the single-token grid encoding. From a practical perspective, we both
demonstrate the feasibility and raise the concern of easy over-fitting problem
of using smaller networks and lower embedding dimensions on the generation
task. The findings can also contribute to futural models in terms of feature
engineering.",This is a draft before submitted to TISMIR as a journal paper,
Automated Time-frequency Domain Audio Crossfades using Graph Cuts,"[arxiv.Result.Author('Kyle Robinson'), arxiv.Result.Author('Dan Brown')]",2023-01-31 03:05:48+00:00,"The problem of transitioning smoothly from one audio clip to another arises
in many music consumption scenarios, especially as music consumption has moved
from professionally curated and live-streamed radios to personal playback
devices and services. we present the first steps toward a new method of
automatically transitioning from one audio clip to another by discretizing the
frequency spectrum into bins and then finding transition times for each bin. We
phrase the problem as one of graph flow optimization; specifically
min-cut/max-flow.",,"Late Breaking/Demo at the 20th International Society for Music
  Information Retrieval, Delft, The Netherlands, 2019"
Neural Target Speech Extraction: An Overview,"[arxiv.Result.Author('Katerina Zmolikova'), arxiv.Result.Author('Marc Delcroix'), arxiv.Result.Author('Tsubasa Ochiai'), arxiv.Result.Author('Keisuke Kinoshita'), arxiv.Result.Author('Jan Černocký'), arxiv.Result.Author('Dong Yu')]",2023-01-31 00:26:52+00:00,"Humans can listen to a target speaker even in challenging acoustic conditions
that have noise, reverberation, and interfering speakers. This phenomenon is
known as the cocktail-party effect. For decades, researchers have focused on
approaching the listening ability of humans. One critical issue is handling
interfering speakers because the target and non-target speech signals share
similar characteristics, complicating their discrimination. Target
speech/speaker extraction (TSE) isolates the speech signal of a target speaker
from a mixture of several speakers with or without noises and reverberations
using clues that identify the speaker in the mixture. Such clues might be a
spatial clue indicating the direction of the target speaker, a video of the
speaker's lips, or a pre-recorded enrollment utterance from which their voice
characteristics can be derived. TSE is an emerging field of research that has
received increased attention in recent years because it offers a practical
approach to the cocktail-party problem and involves such aspects of signal
processing as audio, visual, array processing, and deep learning. This paper
focuses on recent neural-based approaches and presents an in-depth overview of
TSE. We guide readers through the different major approaches, emphasizing the
similarities among frameworks and discussing potential future directions.","Submitted to IEEE Signal Processing Magazine on Apr. 25, 2022, and
  accepted on Jan. 12, 2023",
ArchiSound: Audio Generation with Diffusion,[arxiv.Result.Author('Flavio Schneider')],2023-01-30 20:23:26+00:00,"The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
generation. One area that has yet to be fully explored is the application of
diffusion models to audio generation. Audio generation requires an
understanding of multiple aspects, such as the temporal dimension, long term
structure, multiple layers of overlapping sounds, and the nuances that only
trained listeners can detect. In this work, we investigate the potential of
diffusion models for audio generation. We propose a set of models to tackle
multiple aspects, including a new method for text-conditional latent audio
diffusion with stacked 1D U-Nets, that can generate multiple minutes of music
from a textual description. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open source libraries
with the hope of simplifying future work in the field. Samples can be found at
https://bit.ly/audio-diffusion. Codes are at
https://github.com/archinetai/audio-diffusion-pytorch.",Master Thesis at ETH Zurich,
MYRiAD: A Multi-Array Room Acoustic Database,"[arxiv.Result.Author('Thomas Dietzen'), arxiv.Result.Author('Randall Ali'), arxiv.Result.Author('Maja Taseska'), arxiv.Result.Author('Toon van Waterschoot')]",2023-01-30 16:54:03+00:00,"In the development of acoustic signal processing algorithms, their evaluation
in various acoustic environments is of utmost importance. In order to advance
evaluation in realistic and reproducible scenarios, several high-quality
acoustic databases have been developed over the years. In this paper, we
present another complementary database of acoustic recordings, referred to as
the Multi-arraY Room Acoustic Database (MYRiAD). The MYRiAD database is unique
in its diversity of microphone configurations suiting a wide range of
enhancement and reproduction applications (such as assistive hearing,
teleconferencing, or sound zoning), the acoustics of the two recording spaces,
and the variety of contained signals including 1214 room impulse responses
(RIRs), reproduced speech, music, and stationary noise, as well as recordings
of live cocktail parties held in both rooms. The microphone configurations
comprise a dummy head (DH) with in-ear omnidirectional microphones, two
behind-the-ear (BTE) pieces equipped with 2 omnidirectional microphones each, 5
external omnidirectional microphones (XMs), and two concentric circular
microphone arrays (CMAs) consisting of 12 omnidirectional microphones in total.
The two recording spaces, namely the SONORA Audio Laboratory (SAL) and the
Alamire Interactive Laboratory (AIL), have reverberation times of 2.1s and
0.5s, respectively. Audio signals were reproduced using 10 movable loudspeakers
in the SAL and a built-in array of 24 loudspeakers in the AIL. MATLAB and
Python scripts are included for accessing the signals as well as microphone and
loudspeaker coordinates. The database is publicly available at [1].",submitted for publication,
Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation,"[arxiv.Result.Author('Minglun Han'), arxiv.Result.Author('Feilong Chen'), arxiv.Result.Author('Jing Shi'), arxiv.Result.Author('Shuang Xu'), arxiv.Result.Author('Bo Xu')]",2023-01-30 15:44:55+00:00,"Large-scale pre-trained language models (PLMs) with powerful language
modeling capabilities have been widely used in natural language processing. For
automatic speech recognition (ASR), leveraging PLMs to improve performance has
also become a promising research trend. However, most previous works may suffer
from the inflexible sizes and structures of PLMs, along with the insufficient
utilization of the knowledge in PLMs. To alleviate these problems, we propose
the hierarchical knowledge distillation on the continuous integrate-and-fire
(CIF) based ASR models. Specifically, we distill the knowledge from PLMs to the
ASR model by applying cross-modal distillation with contrastive loss at the
acoustic level and applying distillation with regression loss at the linguistic
level. On the AISHELL-1 dataset, our method achieves 15% relative error rate
reduction over the original CIF-based model and achieves comparable performance
(3.8%/4.1% on dev/test) to the state-of-the-art model.","5 pages; Keywords: speech recognition, continuous integrate-and-fire,
  knowledge distillation, contrastive learning, pre-trained language models",
Real-Time Acoustic Perception for Automotive Applications,"[arxiv.Result.Author('Jun Yin'), arxiv.Result.Author('Stefano Damiano'), arxiv.Result.Author('Marian Verhelst'), arxiv.Result.Author('Toon van Waterschoot'), arxiv.Result.Author('Andre Guntoro')]",2023-01-30 12:00:20+00:00,"In recent years the automotive industry has been strongly promoting the
development of smart cars, equipped with multi-modal sensors to gather
information about the surroundings, in order to aid human drivers or make
autonomous decisions. While the focus has mostly been on visual sensors, also
acoustic events are crucial to detect situations that require a change in the
driving behavior, such as a car honking, or the sirens of approaching emergency
vehicles. In this paper, we summarize the results achieved so far in the Marie
Sklodowska-Curie Actions (MSCA) European Industrial Doctorates (EID) project
Intelligent Ultra Low-Power Signal Processing for Automotive (I-SPOT). On the
algorithmic side, the I-SPOT Project aims to enable detecting, localizing and
tracking environmental audio signals by jointly developing microphone array
processing and deep learning techniques that specifically target automotive
applications. Data generation software has been developed to cover the I-SPOT
target scenarios and research challenges. This tool is currently being used to
develop low-complexity deep learning techniques for emergency sound detection.
On the hardware side, the goal impels workflows for hardware-algorithm
co-design to ease the generation of architectures that are sufficiently
flexible towards algorithmic evolutions without giving up on efficiency, as
well as enable rapid feedback of hardware implications of algorithmic decision.
This is pursued though a hierarchical workflow that breaks the
hardware-algorithm design space into reasonable subsets, which has been tested
for operator-level optimizations on state-of-the-art robust sound source
localization for edge devices. Further, several open challenges towards an
end-to-end system are clarified for the next stage of I-SPOT.",,
UzbekTagger: The rule-based POS tagger for Uzbek language,"[arxiv.Result.Author('Maksud Sharipov'), arxiv.Result.Author('Elmurod Kuriyozov'), arxiv.Result.Author('Ollabergan Yuldashev'), arxiv.Result.Author('Ogabek Sobirov')]",2023-01-30 07:40:45+00:00,"This research paper presents a part-of-speech (POS) annotated dataset and
tagger tool for the low-resource Uzbek language. The dataset includes 12 tags,
which were used to develop a rule-based POS-tagger tool. The corpus text used
in the annotation process was made sure to be balanced over 20 different fields
in order to ensure its representativeness. Uzbek being an agglutinative
language so the most of the words in an Uzbek sentence are formed by adding
suffixes. This nature of it makes the POS-tagging task difficult to find the
stems of words and the right part-of-speech they belong to. The methodology
proposed in this research is the stemming of the words with an affix/suffix
stripping approach including database of the stem forms of the words in the
Uzbek language. The tagger tool was tested on the annotated dataset and showed
high accuracy in identifying and tagging parts of speech in Uzbek text. This
newly presented dataset and tagger tool can be used for a variety of natural
language processing tasks such as language modeling, machine translation, and
text-to-speech synthesis. The presented dataset is the first of its kind to be
made publicly available for Uzbek, and the POS-tagger tool created can also be
used as a pivot to use as a base for other closely-related Turkic languages.","Preprint of the accepted paper to The 10th Language & Technology
  Conference: Human Language Technologies as a Challenge for Computer Science
  and Linguistics, April 21-23, 2023, Pozna\'n, Poland",
GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration,"[arxiv.Result.Author('Naoki Murata'), arxiv.Result.Author('Koichi Saito'), arxiv.Result.Author('Chieh-Hsin Lai'), arxiv.Result.Author('Yuhta Takida'), arxiv.Result.Author('Toshimitsu Uesaka'), arxiv.Result.Author('Yuki Mitsufuji'), arxiv.Result.Author('Stefano Ermon')]",2023-01-30 06:27:48+00:00,"Pre-trained diffusion models have been successfully used as priors in a
variety of linear inverse problems, where the goal is to reconstruct a signal
from noisy linear measurements. However, existing approaches require knowledge
of the linear operator. In this paper, we propose GibbsDDRM, an extension of
Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the
linear measurement operator is unknown. GibbsDDRM constructs a joint
distribution of the data, measurements, and linear operator by using a
pre-trained diffusion model for the data prior, and it solves the problem by
posterior sampling with an efficient variant of a Gibbs sampler. The proposed
method is problem-agnostic, meaning that a pre-trained diffusion model can be
applied to various inverse problems without fine tuning. In experiments, it
achieved high performance on both blind image deblurring and vocal
dereverberation tasks, despite the use of simple generic priors for the
underlying linear operators.",,
SingSong: Generating musical accompaniments from singing,"[arxiv.Result.Author('Chris Donahue'), arxiv.Result.Author('Antoine Caillon'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Ethan Manilow'), arxiv.Result.Author('Philippe Esling'), arxiv.Result.Author('Andrea Agostinelli'), arxiv.Result.Author('Mauro Verzetti'), arxiv.Result.Author('Ian Simon'), arxiv.Result.Author('Olivier Pietquin'), arxiv.Result.Author('Neil Zeghidour'), arxiv.Result.Author('Jesse Engel')]",2023-01-30 04:53:23+00:00,"We present SingSong, a system that generates instrumental music to accompany
input vocals, potentially offering musicians and non-musicians alike an
intuitive new way to create music featuring their own voice. To accomplish
this, we build on recent developments in musical source separation and audio
generation. Specifically, we apply a state-of-the-art source separation
algorithm to a large corpus of music audio to produce aligned pairs of vocals
and instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a
state-of-the-art approach for unconditional audio generation -- to be suitable
for conditional ""audio-to-audio"" generation tasks, and train it on the
source-separated (vocal, instrumental) pairs. In a pairwise comparison with the
same vocal inputs, listeners expressed a significant preference for
instrumentals generated by SingSong compared to those from a strong retrieval
baseline.
  Sound examples at https://g.co/magenta/singsong",,
Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models,"[arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Jiawei Huang'), arxiv.Result.Author('Dongchao Yang'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Luping Liu'), arxiv.Result.Author('Mingze Li'), arxiv.Result.Author('Zhenhui Ye'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Xiang Yin'), arxiv.Result.Author('Zhou Zhao')]",2023-01-30 04:44:34+00:00,"Large-scale multimodal generative modeling has created milestones in
text-to-image and text-to-video generation. Its application to audio still lags
behind for two main reasons: the lack of large-scale datasets with high-quality
text-audio pairs, and the complexity of modeling long continuous audio data. In
this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that
addresses these gaps by 1) introducing pseudo prompt enhancement with a
distill-then-reprogram approach, it alleviates data scarcity with orders of
magnitude concept compositions by using language-free audios; 2) leveraging
spectrogram autoencoder to predict the self-supervised audio representation
instead of waveforms. Together with robust contrastive language-audio
pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art
results in both objective and subjective benchmark evaluation. Moreover, we
present its controllability and generalization for X-to-Audio with ""No Modality
Left Behind"", for the first time unlocking the ability to generate
high-definition, high-fidelity audios given a user-defined modality input.
Audio samples are available at https://Text-to-Audio.github.io",Audio samples are available at https://Text-to-Audio.github.io,
Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining,"[arxiv.Result.Author('Takaaki Saeki'), arxiv.Result.Author('Soumi Maiti'), arxiv.Result.Author('Xinjian Li'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Hiroshi Saruwatari')]",2023-01-30 00:53:50+00:00,"While neural text-to-speech (TTS) has achieved human-like natural synthetic
speech, multilingual TTS systems are limited to resource-rich languages due to
the need for paired text and studio-quality audio data. This paper proposes a
method for zero-shot multilingual TTS using text-only data for the target
language. The use of text-only data allows the development of TTS systems for
low-resource languages for which only textual resources are available, making
TTS accessible to thousands of languages. Inspired by the strong cross-lingual
transferability of multilingual language models, our framework first performs
masked language model pretraining with multilingual text-only data. Then we
train this model with a paired data in a supervised manner, while freezing a
language-aware embedding layer. This allows inference even for languages not
included in the paired data but present in the text-only data. Evaluation
results demonstrate highly intelligible zero-shot TTS with a character error
rate of less than 12% for an unseen language. All experiments were conducted
using public datasets and the implementation will be made available for
reproducibility.",,
Vicarious Offense and Noise Audit of Offensive Speech Classifiers,"[arxiv.Result.Author('Tharindu Cyril Weerasooriya'), arxiv.Result.Author('Sujan Dutta'), arxiv.Result.Author('Tharindu Ranasinghe'), arxiv.Result.Author('Marcos Zampieri'), arxiv.Result.Author('Christopher M. Homan'), arxiv.Result.Author('Ashiqur R. KhudaBukhsh')]",2023-01-29 20:39:21+00:00,"This paper examines social web content moderation from two key perspectives:
automated methods (machine moderators) and human evaluators (human moderators).
We conduct a noise audit at an unprecedented scale using nine machine
moderators trained on well-known offensive speech data sets evaluated on a
corpus sampled from 92 million YouTube comments discussing a multitude of
issues relevant to US politics. We introduce a first-of-its-kind data set of
vicarious offense. We ask annotators: (1) if they find a given social media
post offensive; and (2) how offensive annotators sharing different political
beliefs would find the same content. Our experiments with machine moderators
reveal that moderation outcomes wildly vary across different machine
moderators. Our experiments with human moderators suggest that (1) political
leanings considerably affect first-person offense perspective; (2) Republicans
are the worst predictors of vicarious offense; (3) predicting vicarious offense
for the Republicans is most challenging than predicting vicarious offense for
the Independents and the Democrats; and (4) disagreement across political
identity groups considerably increases when sensitive issues such as
reproductive rights or gun control/rights are discussed. Both experiments
suggest that offense, is indeed, highly subjective and raise important
questions concerning content moderation practices.",,
Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling,[arxiv.Result.Author('Martin E. Malandro')],2023-01-29 19:45:10+00:00,"We consider the task of multi-track MIDI infilling when arbitrary (track,
measure) pairs of information have been deleted from a contiguous slice of
measures from a MIDI file. We train two T5-like models to solve this task, one
using a basic MIDI-like event vocabulary and one using a joined word-like
version of this vocabulary. We introduce a new test set, created from the Lakh
MIDI dataset, consisting of 9 multi-track MIDI infilling tasks. We evaluate our
models on these tasks and find that one model works better on some tasks while
the other works better on others. Our results have implications for the
training of neural networks in other small-vocabulary domains, such as byte
sequence modeling and protein sequence modeling. We release our source code,
and we demonstrate that our models are capable of enabling real-time
human-computer interactive composition in the REAPER digital audio workstation.","16 pages, 7 figures, 3 tables",
AudioLDM: Text-to-Audio Generation with Latent Diffusion Models,"[arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Zehua Chen'), arxiv.Result.Author('Yi Yuan'), arxiv.Result.Author('Xinhao Mei'), arxiv.Result.Author('Xubo Liu'), arxiv.Result.Author('Danilo Mandic'), arxiv.Result.Author('Wenwu Wang'), arxiv.Result.Author('Mark D. Plumbley')]",2023-01-29 17:48:17+00:00,"Text-to-audio (TTA) system has recently gained attention for its ability to
synthesize general audio based on text descriptions. However, previous studies
in TTA have limited generation quality with high computational costs. In this
study, we propose AudioLDM, a TTA system that is built on a latent space to
learn the continuous audio representations from contrastive language-audio
pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs
with audio embedding while providing text embedding as a condition during
sampling. By learning the latent representations of audio signals and their
compositions without modeling the cross-modal relationship, AudioLDM is
advantageous in both generation quality and computational efficiency. Trained
on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA
performance measured by both objective and subjective metrics (e.g., frechet
distance). Moreover, AudioLDM is the first TTA system that enables various
text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.
Our implementation and demos are available at https://audioldm.github.io.","Demo and implementation at https://audioldm.github.io. Evaluation
  toolbox at https://github.com/haoheliu/audioldm_eval",
Exploring Attention Map Reuse for Efficient Transformer Neural Networks,"[arxiv.Result.Author('Kyuhong Shim'), arxiv.Result.Author('Jungwook Choi'), arxiv.Result.Author('Wonyong Sung')]",2023-01-29 13:38:45+00:00,"Transformer-based deep neural networks have achieved great success in various
sequence applications due to their powerful ability to model long-range
dependency. The key module of Transformer is self-attention (SA) which extracts
features from the entire sequence regardless of the distance between positions.
Although SA helps Transformer performs particularly well on long-range tasks,
SA requires quadratic computation and memory complexity with the input sequence
length. Recently, attention map reuse, which groups multiple SA layers to share
one attention map, has been proposed and achieved significant speedup for
speech recognition models. In this paper, we provide a comprehensive study on
attention map reuse focusing on its ability to accelerate inference. We compare
the method with other SA compression techniques and conduct a breakdown
analysis of its advantages for a long sequence. We demonstrate the
effectiveness of attention map reuse by measuring the latency on both CPU and
GPU platforms.",,
NeuralKalman: A Learnable Kalman Filter for Acoustic Echo Cancellation,"[arxiv.Result.Author('Yixuan Zhang'), arxiv.Result.Author('Meng Yu'), arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Dong Yu'), arxiv.Result.Author('DeLiang Wang')]",2023-01-29 05:41:30+00:00,"The Kalman filter is widely used for addressing acoustic echo cancellation
(AEC) problems due to their robustness to double-talk and fast convergence.
However, the inability to model nonlinearity and the need to tune control
parameters cast limitations on such adaptive filtering algorithms. In this
paper, we integrate the frequency domain Kalman filter (FDKF) and deep neural
networks (DNNs) into a hybrid method, called NeuralKalman, to leverage the
advantages of deep learning and adaptive filtering algorithms. Specifically, we
employ a DNN to estimate nonlinearly distorted far-end signals, a transition
factor, and the nonlinear transition function in the state equation of the FDKF
algorithm. Experimental results show that the proposed NeuralKalman improves
the performance of FDKF significantly and outperforms strong baseline methods.","The term of the algorithm is renamed because it conflicts with an
  existing KalmanNet algorithm proposed by Revach et. al. (arXiv:2107.10043)",
Artistic Curve Steganography Carried by Musical Audio,[arxiv.Result.Author('Christopher J. Tralie')],2023-01-29 04:15:57+00:00,"In this work, we create artistic closed loop curves that trace out images and
3D shapes, which we then hide in musical audio as a form of steganography. We
use traveling salesperson art to create artistic plane loops to trace out image
contours, and we use Hamiltonian cycles on triangle meshes to create artistic
space loops that fill out 3D surfaces. Our embedding scheme is designed to
faithfully preserve the geometry of these loops after lossy compression, while
keeping their presence undetectable to the audio listener. To accomplish this,
we hide each dimension of the curve in a different frequency, and we perturb a
sliding window sum of the magnitude of that frequency to best match the target
curve at that dimension, while hiding scale information in that frequency's
phase. In the process, we exploit geometric properties of the curves to help to
more effectively hide and recover them. Our scheme is simple and encoding
happens efficiently with a nonnegative least squares framework, while decoding
is trivial. We validate our technique quantitatively on large datasets of
images and audio, and we show results of a crowd sourced listening test that
validate that the hidden information is indeed unobtrusive.","18 pages, 14 figures, in Proceedings of EvoMUSART 2023",
Achieving Timestamp Prediction While Recognizing with Non-Autoregressive End-to-End ASR Model,"[arxiv.Result.Author('Xian Shi'), arxiv.Result.Author('Yanni Chen'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Zhijie Yan')]",2023-01-29 03:47:59+00:00,"Conventional ASR systems use frame-level phoneme posterior to conduct
force-alignment~(FA) and provide timestamps, while end-to-end ASR systems
especially AED based ones are short of such ability. This paper proposes to
perform timestamp prediction~(TP) while recognizing by utilizing continuous
integrate-and-fire~(CIF) mechanism in non-autoregressive ASR model -
Paraformer. Foucing on the fire place bias issue of CIF, we conduct
post-processing strategies including fire-delay and silence insertion. Besides,
we propose to use scaled-CIF to smooth the weights of CIF output, which is
proved beneficial for both ASR and TP task. Accumulated averaging shift~(AAS)
and diarization error rate~(DER) are adopted to measure the quality of
timestamps and we compare these metrics of proposed system and conventional
hybrid force-alignment system. The experiment results over manually-marked
timestamps testset show that the proposed optimization methods significantly
improve the accuracy of CIF timestamps, reducing 66.7\% and 82.1\% of AAS and
DER respectively. Comparing to Kaldi force-alignment trained with the same
data, optimized CIF timestamps achieved 12.3\% relative AAS reduction.",,
Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker,"[arxiv.Result.Author('Navjot Kaur'), arxiv.Result.Author('Paige Tuttosi')]",2023-01-29 02:58:01+00:00,"Voice synthesis has seen significant improvements in the past decade
resulting in highly intelligible voices. Further investigations have resulted
in models that can produce variable speech, including conditional emotional
expression. The problem lies, however, in a focus on phrase-level modifications
and prosodic vocal features. Using the CREMA-D dataset we have trained a GAN
conditioned on emotion to generate worth lengths for a given input text. These
word lengths are relative to neutral speech and can be provided, through speech
synthesis markup language (SSML) to a text-to-speech (TTS) system to generate
more expressive speech. Additionally, a generative model is also trained using
implicit maximum likelihood estimation (IMLE) and a comparative analysis with
GANs is included. We were able to achieve better performances on objective
measures for neutral speech, and better time alignment for happy speech when
compared to an out-of-box model. However, further investigation of subjective
evaluation is required.",,
Cross-domain Neural Pitch and Periodicity Estimation,"[arxiv.Result.Author('Max Morrison'), arxiv.Result.Author('Caedon Hsieh'), arxiv.Result.Author('Nathan Pruyne'), arxiv.Result.Author('Bryan Pardo')]",2023-01-28 17:30:47+00:00,"Pitch is a foundational aspect of our perception of audio signals. Pitch
contours are commonly used to analyze speech and music signals and as input
features for many audio tasks, including music transcription, singing voice
synthesis, and prosody editing. In this paper, we describe a set of techniques
for improving the accuracy of state-of-the-art neural pitch and periodicity
estimators. We also introduce a novel entropy-based method for extracting
periodicity and per-frame voiced-unvoiced classifications from statistical
inference-based pitch estimators (e.g., neural networks), and show how to train
a neural pitch estimator to simultaneously handle speech and music without
performance degradation. While neural pitch trackers have historically been
significantly slower than signal processing based pitch trackers, our estimator
implementations approach the speed of state-of-the-art DSP-based pitch
estimators on a standard CPU, but with significantly more accurate pitch and
periodicity estimation. Our experiments show that an accurate, cross-domain
pitch and periodicity estimator written in PyTorch with a hopsize of ten
milliseconds can run 11.2x faster than real-time on a Intel i9-9820X 10-core
3.30 GHz CPU or 408x faster than real-time on a NVIDIA GeForce RTX 3090 GPU
without hardware optimization. We release all of our code and models as
Pitch-Estimating Neural Networks (penn), an open-source, pip-installable Python
module for training, evaluating, and performing inference with pitch- and
periodicity-estimating neural networks. The code for penn is available at
https://github.com/interactiveaudiolab/penn.",,
who is snoring? snore based user recognition,"[arxiv.Result.Author('Shenghao Li'), arxiv.Result.Author('Jagmohan Chauhan')]",2023-01-28 14:28:57+00:00,"Snoring is one of the most prominent symptoms of Obstructive Sleep
Apnea-Hypopnea Syndrome (OSAH), a highly prevalent disease that causes
repetitive collapse and cessation of the upper airway. Thus, accurate snore
sound monitoring and analysis is crucial. However, the traditional monitoring
method polysomnography (PSG) requires the patients to stay at a sleep clinic
for the whole night and be connected to many pieces of equipment. An
alternative and less invasive way is passive monitoring using a smartphone at
home or in the clinical settings. But, there is a challenge: the environment
may be shared by people such that the raw audio may contain the snore
activities of the bed partner or other person. False capturing of the snoring
activity could lead to critical false alarms and misdiagnosis of the patients.
To address this limitation, we propose a hypothesis that snore sound contains
unique identity information which can be used for user recognition. We analyzed
various machine learning models: Gaussian Mixture Model (GMM), GMM-UBM
(Universial Background Model), and a Deep Neural Network (DNN) on MPSSC - an
open source snoring dataset to evaluate the validity of our hypothesis. Our
results are promising as we achieved around 90% accuracy in identification and
verification tasks. This work marks the first step towards understanding the
practicality of snore based user monitoring to enable multiple healthcare
applicaitons.",,
Automated Arrangements of Multi-Part Music for Sets of Monophonic Instruments,"[arxiv.Result.Author('Matthew Mccloskey'), arxiv.Result.Author('Gabrielle Curcio'), arxiv.Result.Author('Amulya Badineni'), arxiv.Result.Author('Kevin Mcgrath'), arxiv.Result.Author('Dimitris Papamichail')]",2023-01-28 04:13:45+00:00,"Arranging music for a different set of instruments that it was originally
written for is traditionally a tedious and time-consuming process, performed by
experts with intricate knowledge of the specific instruments and involving
significant experimentation. In this paper we study the problem of automating
music arrangements for music pieces written for monophonic instruments or
voices. We designed and implemented an algorithm that can always produce a
music arrangement when feasible by transposing the music piece to a different
scale, permuting the assigned parts to instruments/voices, and transposing
individual parts by one or more octaves. We also published open source software
written in Python that processes MusicXML files and allows musicians to
experiment with music arrangements. It is our hope that our software can serve
as a platform for future extensions that will include music reductions and
inclusion of polyphonic instruments.",,
Byte Pair Encoding for Symbolic Music,"[arxiv.Result.Author('Nathan Fradet'), arxiv.Result.Author('Jean-Pierre Briot'), arxiv.Result.Author('Fabien Chhel'), arxiv.Result.Author('Amal El Fallah Seghrouchni'), arxiv.Result.Author('Nicolas Gutowski')]",2023-01-27 20:22:18+00:00,"The symbolic music modality is nowadays mostly represented as discrete and
used with sequential models such as Transformers, for deep learning tasks.
Recent research put efforts on the tokenization, i.e. the conversion of data
into sequences of integers intelligible to such models. This can be achieved by
many ways as music can be composed of simultaneous tracks, of simultaneous
notes with several attributes. Until now, the proposed tokenizations are based
on small vocabularies describing the note attributes and time events, resulting
in fairly long token sequences. In this paper, we show how Byte Pair Encoding
(BPE) can improve the results of deep learning models while improving its
performances. We experiment on music generation and composer classification,
and study the impact of BPE on how models learn the embeddings, and show that
it can help to increase their isotropy, i.e., the uniformity of the variance of
their positions in the space.",Source code at https://github.com/Natooz/BPE-Symbolic-Music,
Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion,"[arxiv.Result.Author('Flavio Schneider'), arxiv.Result.Author('Zhijing Jin'), arxiv.Result.Author('Bernhard Schölkopf')]",2023-01-27 14:52:53+00:00,"The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
synthesis. One area that has yet to be fully explored is the application of
diffusion models to music generation. Music generation requires to handle
multiple aspects, including the temporal dimension, long-term structure,
multiple layers of overlapping sounds, and nuances that only trained listeners
can detect. In our work, we investigate the potential of diffusion models for
text-conditional music generation. We develop a cascading latent diffusion
approach that can generate multiple minutes of high-quality stereo music at
48kHz from textual descriptions. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open-source libraries
with the hope of facilitating future work in the field.
  We open-source the following: Music samples for this paper:
https://bit.ly/anonymous-mousai; all music samples for all models:
https://bit.ly/audio-diffusion; and codes:
https://github.com/archinetai/audio-diffusion-pytorch","Music samples for this paper: https://bit.ly/anonymous-mousai; all
  music samples for all models: https://bit.ly/audio-diffusion; and codes:
  https://github.com/archinetai/audio-diffusion-pytorch",
Pre-training for Speech Translation: CTC Meets Optimal Transport,"[arxiv.Result.Author('Phuong-Hang Le'), arxiv.Result.Author('Hongyu Gong'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Juan Pino'), arxiv.Result.Author('Benjamin Lecouteux'), arxiv.Result.Author('Didier Schwab')]",2023-01-27 14:03:09+00:00,"The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed for
reducing this gap, but most of them require architectural changes in ST
training. In this work, we propose to mitigate this issue at the pre-training
stage, requiring no change in the ST model. First, we show that the
connectionist temporal classification (CTC) loss can reduce the modality gap by
design. We provide a quantitative comparison with the more common cross-entropy
loss, showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models.",,
Four ways to recover the symbol of a non-binary localization operator,[arxiv.Result.Author('Simon Halvdansson')],2023-01-27 09:39:50+00:00,"We present a set of results on how the symbol of a localization operator can
be recovered from spectral information, the image of white noise or the image
of an orthonormal basis. This extends earlier results which have been limited
to the case where the symbol is a binary mask. Moreover, we present some
numerical aspects of the different methods and discuss their performance.","29 pages, 6 figures",
"Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech","[arxiv.Result.Author('Jarod Govers'), arxiv.Result.Author('Philip Feldman'), arxiv.Result.Author('Aaron Dant'), arxiv.Result.Author('Panos Patros')]",2023-01-27 07:59:31+00:00,"Social media is a modern person's digital voice to project and engage with
new ideas and mobilise communities $\unicode{x2013}$ a power shared with
extremists. Given the societal risks of unvetted content-moderating algorithms
for Extremism, Radicalisation, and Hate speech (ERH) detection, responsible
software engineering must understand the who, what, when, where, and why such
models are necessary to protect user safety and free expression. Hence, we
propose and examine the unique research field of ERH context mining to unify
disjoint studies. Specifically, we evaluate the start-to-finish design process
from socio-technical definition-building and dataset collection strategies to
technical algorithm design and performance. Our 2015-2021 51-study Systematic
Literature Review (SLR) provides the first cross-examination of textual,
network, and visual approaches to detecting extremist affiliation, hateful
content, and radicalisation towards groups and movements. We identify
consensus-driven ERH definitions and propose solutions to existing ideological
and geographic biases, particularly due to the lack of research in
Oceania/Australasia. Our hybridised investigation on Natural Language
Processing, Community Detection, and visual-text models demonstrates the
dominating performance of textual transformer-based algorithms. We conclude
with vital recommendations for ERH context mining researchers and propose an
uptake roadmap with guidelines for researchers, industries, and governments to
enable a safer cyberspace.","35-page main literature review, 14-page supplementary material.
  Submitted to ACM Computing Surveys (Dec 2021)",
"Sub-Standards and Mal-Practices: Misinformation's Role in Insular, Polarized, and Toxic Interactions","[arxiv.Result.Author('Hans W. A. Hanley'), arxiv.Result.Author('Zakir Durumeric')]",2023-01-27 01:32:22+00:00,"How do users and communities respond to news from unreliable sources? How
does news from these sources change online conversations? In this work, we
examine the role of misinformation in sparking political incivility and
toxicity on the social media platform Reddit. Utilizing the Google Jigsaw
Perspective API to identify toxicity, hate speech, and other forms of
incivility, we find that Reddit comments posted in response to misinformation
articles are 71.4% more likely to be toxic than comments responding to
authentic news articles. Identifying specific instances of commenters'
incivility and utilizing an exponential random graph model, we then show that
when reacting to a misinformation story, Reddit users are more likely to be
toxic to users of different political beliefs than in other settings. Finally,
utilizing a zero-inflated negative binomial regression, we identify that as the
toxicity of subreddits increases, users are more likely to comment on
misinformation-related Reddit submissions.",,
How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech,"[arxiv.Result.Author('Aditya Yedetore'), arxiv.Result.Author('Tal Linzen'), arxiv.Result.Author('Robert Frank'), arxiv.Result.Author('R. Thomas McCoy')]",2023-01-26 23:24:17+00:00,"When acquiring syntax, children consistently choose hierarchical rules over
competing non-hierarchical possibilities. Is this preference due to a learning
bias for hierarchical structure, or due to more general biases that interact
with hierarchical cues in children's linguistic input? We explore these
possibilities by training LSTMs and Transformers - two types of neural networks
without a hierarchical bias - on data similar in quantity and content to
children's linguistic input: text from the CHILDES corpus. We then evaluate
what these models have learned about English yes/no questions, a phenomenon for
which hierarchical structure is crucial. We find that, though they perform well
at capturing the surface statistics of child-directed speech (as measured by
perplexity), both model types generalize in a way more consistent with an
incorrect linear rule than the correct hierarchical rule. These results suggest
that human-like generalization from text alone requires stronger biases than
the general sequence-processing biases of standard neural network
architectures.",10 pages plus references and appendices,
On granularity of prosodic representations in expressive text-to-speech,"[arxiv.Result.Author('Mikolaj Babianski'), arxiv.Result.Author('Kamil Pokora'), arxiv.Result.Author('Raahil Shah'), arxiv.Result.Author('Rafal Sienkiewicz'), arxiv.Result.Author('Daniel Korzekwa'), arxiv.Result.Author('Viacheslav Klimkov')]",2023-01-26 22:24:21+00:00,"In expressive speech synthesis it is widely adopted to use latent prosody
representations to deal with variability of the data during training. Same text
may correspond to various acoustic realizations, which is known as a
one-to-many mapping problem in text-to-speech. Utterance, word, or
phoneme-level representations are extracted from target signal in an
auto-encoding setup, to complement phonetic input and simplify that mapping.
This paper compares prosodic embeddings at different levels of granularity and
examines their prediction from text. We show that utterance-level embeddings
have insufficient capacity and phoneme-level tend to introduce instabilities
when predicted from text. Word-level representations impose balance between
capacity and predictability. As a result, we close the gap in naturalness by
90% between synthetic speech and recordings on LibriTTS dataset, without
sacrificing intelligibility.",Accepted to IEEE SLT 2022,"2022 IEEE Spoken Language Technology Workshop (SLT), pp. 892-899"
MusicLM: Generating Music From Text,"[arxiv.Result.Author('Andrea Agostinelli'), arxiv.Result.Author('Timo I. Denk'), arxiv.Result.Author('Zalán Borsos'), arxiv.Result.Author('Jesse Engel'), arxiv.Result.Author('Mauro Verzetti'), arxiv.Result.Author('Antoine Caillon'), arxiv.Result.Author('Qingqing Huang'), arxiv.Result.Author('Aren Jansen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Marco Tagliasacchi'), arxiv.Result.Author('Matt Sharifi'), arxiv.Result.Author('Neil Zeghidour'), arxiv.Result.Author('Christian Frank')]",2023-01-26 18:58:53+00:00,"We introduce MusicLM, a model generating high-fidelity music from text
descriptions such as ""a calming violin melody backed by a distorted guitar
riff"". MusicLM casts the process of conditional music generation as a
hierarchical sequence-to-sequence modeling task, and it generates music at 24
kHz that remains consistent over several minutes. Our experiments show that
MusicLM outperforms previous systems both in audio quality and adherence to the
text description. Moreover, we demonstrate that MusicLM can be conditioned on
both text and a melody in that it can transform whistled and hummed melodies
according to the style described in a text caption. To support future research,
we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,
with rich text descriptions provided by human experts.","Supplementary material at
  https://google-research.github.io/seanet/musiclm/examples and
  https://kaggle.com/datasets/googleai/musiccaps",
A simple model for pink noise from amplitude modulations,"[arxiv.Result.Author('Masahiro Morikawa'), arxiv.Result.Author('Akika Nakamichi')]",2023-01-26 15:33:19+00:00,"We propose a simple model for the origin of pink noise (or 1/f fluctuation)
based on the beat of cooperative waves. These cooperative waves arise
spontaneously in a system with synchronization, resonance, and infrared
divergence. Many cooperative waves with close frequencies can produce signals
of arbitrary small frequencies from a system of small size. This beat mechanism
can be understood as amplitude modulation. The pink noise can appear after the
demodulation process, which produces a variety of pink noise in many fields.
The pink noise thus formed from the beat has nothing to do with dissipation or
long-time memory. We also suggest new ways of looking at pink noise in shallow
earthquakes, solar flares, and stellar activities.","12 pages, 9 figures",
A benchmark for toxic comment classification on Civil Comments dataset,"[arxiv.Result.Author('Corentin Duchene'), arxiv.Result.Author('Henri Jamet'), arxiv.Result.Author('Pierre Guillaume'), arxiv.Result.Author('Reda Dehak')]",2023-01-26 14:25:09+00:00,"Toxic comment detection on social media has proven to be essential for
content moderation. This paper compares a wide set of different models on a
highly skewed multi-label hate speech dataset. We consider inference time and
several metrics to measure performance and bias in our comparison. We show that
all BERTs have similar performance regardless of the size, optimizations or
language used to pre-train the models. RNNs are much faster at inference than
any of the BERT. BiLSTM remains a good compromise between performance and
inference time. RoBERTa with Focal Loss offers the best performance on biases
and AUROC. However, DistilBERT combines both good AUROC and a low inference
time. All models are affected by the bias of associating identities. BERT, RNN,
and XLNet are less sensitive than the CNN and Compact Convolutional
Transformers.",,"EGC 2023, vol. RNTI-E-39, pp.19-30"
Neuromorphic spintronics accelerated by an unconventional data-driven Thiele equation approach,"[arxiv.Result.Author('Anatole Moureaux'), arxiv.Result.Author('Simon De Wergifosse'), arxiv.Result.Author('Chloé Chopin'), arxiv.Result.Author('Jimmy Weber'), arxiv.Result.Author('Flavio Abreu Araujo')]",2023-01-26 10:30:31+00:00,"A hardware neural network based on a single spin-torque vortex
nano-oscillator is designed using time-multiplexing. The behavior of the
spin-torque vortex nano-oscillator is simulated with an improved ultra-fast and
quantitative model based on the Thiele equation approach. Different
mathematical and numerical adaptations are brought to the model in order to
increase the accuracy and the speed of the simulations. A benchmark task of
waveform classification is designed to assess the performance of the neural
network in the framework of reservoir computing and compare two different
versions of the model. The obtained results allow to conclude on the ability of
the system to effectively classify sine and square signals with high accuracy
and low root-mean-square error, reflecting high confidence cognition. Given the
high throughput of the simulations, two innovative parametric studies on the dc
bias current intensity and the level of noise in the system are performed to
demonstrate the value of our models. The efficiency of our system is also
tested during speech recognition and shows the agreement between these models
and the corresponding experimental measurements.","10 pages, 7 figures",
Affective Faces for Goal-Driven Dyadic Communication,"[arxiv.Result.Author('Scott Geng'), arxiv.Result.Author('Revant Teotia'), arxiv.Result.Author('Purva Tendulkar'), arxiv.Result.Author('Sachit Menon'), arxiv.Result.Author('Carl Vondrick')]",2023-01-26 05:00:09+00:00,"We introduce a video framework for modeling the association between verbal
and non-verbal communication during dyadic conversation. Given the input speech
of a speaker, our approach retrieves a video of a listener, who has facial
expressions that would be socially appropriate given the context. Our approach
further allows the listener to be conditioned on their own goals,
personalities, or backgrounds. Our approach models conversations through a
composition of large language models and vision-language models, creating
internal representations that are interpretable and controllable. To study
multimodal communication, we propose a new video dataset of unscripted
conversations covering diverse topics and demographics. Experiments and
visualizations show our approach is able to output listeners that are
significantly more socially appropriate than baselines. However, many
challenges remain, and we release our dataset publicly to spur further
progress. See our website for video results, data, and code:
https://realtalk.cs.columbia.edu.",,
Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning,"[arxiv.Result.Author('Mingyu Derek Ma'), arxiv.Result.Author('Jiun-Yu Kao'), arxiv.Result.Author('Shuyang Gao'), arxiv.Result.Author('Arpit Gupta'), arxiv.Result.Author('Di Jin'), arxiv.Result.Author('Tagyoung Chung'), arxiv.Result.Author('Nanyun Peng')]",2023-01-26 03:01:59+00:00,"Dialogue state tracking (DST) is an important step in dialogue management to
keep track of users' beliefs. Existing works fine-tune all language model (LM)
parameters to tackle the DST task, which requires significant data and
computing resources for training and hosting. The cost grows exponentially in
the real-world deployment where dozens of fine-tuned LM are used for different
domains and tasks. To reduce parameter size and better utilize cross-task
shared information, we propose to use soft prompt token embeddings to learn
task properties. Without tuning LM parameters, our method drastically reduces
the number of parameters needed to less than 0.5% of prior works while achieves
better low-resource DST performance.","5 pages, in the Second Workshop on Efficient Natural Language and
  Speech Processing (ENLSP) at NeurIPS 2022",
Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content,"[arxiv.Result.Author('Liam Hebert'), arxiv.Result.Author('Hong Yi Chen'), arxiv.Result.Author('Robin Cohen'), arxiv.Result.Author('Lukasz Golab')]",2023-01-25 23:32:32+00:00,"Our work advances an approach for predicting hate speech in social media,
drawing out the critical need to consider the discussions that follow a post to
successfully detect when hateful discourse may arise. Using graph transformer
networks, coupled with modelling attention and BERT-level natural language
processing, our approach can capture context and anticipate upcoming
anti-social behaviour. In this paper, we offer a detailed qualitative analysis
of this solution for hate speech detection in social networks, leading to
insights into where the method has the most impressive outcomes in comparison
with competitors and identifying scenarios where there are challenges to
achieving ideal performance. Included is an exploration of the kinds of posts
that permeate social media today, including the use of hateful images. This
suggests avenues for extending our model to be more comprehensive. A key
insight is that the focus on reasoning about the concept of context positions
us well to be able to support multi-modal analysis of online posts. We conclude
with a reflection on how the problem we are addressing relates especially well
to the theme of dynamic change, a critical concern for all AI solutions for
social impact. We also comment briefly on how mental health well-being can be
advanced with our work, through curated content attuned to the extent of hate
in posts.",Accepted at AAAI 2023 AI for Social Good,
Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives,"[arxiv.Result.Author('Tanvi Dinkar'), arxiv.Result.Author('Chloé Clavel'), arxiv.Result.Author('Ioana Vasilescu')]",2023-01-25 18:55:05+00:00,"Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers (""uh"", ""um"") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to synthesise a breadth of perspectives in
a holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.",To appear in TAL Journal,
Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation,"[arxiv.Result.Author('Shahar Lutati'), arxiv.Result.Author('Eliya Nachmani'), arxiv.Result.Author('Lior Wolf')]",2023-01-25 18:21:51+00:00,"The problem of speech separation, also known as the cocktail party problem,
refers to the task of isolating a single speech signal from a mixture of speech
signals. Previous work on source separation derived an upper bound for the
source separation task in the domain of human speech. This bound is derived for
deterministic models. Recent advancements in generative models challenge this
bound. We show how the upper bound can be generalized to the case of random
generative models. Applying a diffusion model Vocoder that was pretrained to
model single-speaker voices on the output of a deterministic separation model
leads to state-of-the-art separation results. It is shown that this requires
one to combine the output of the separation model with that of the diffusion
model. In our method, a linear combination is performed, in the frequency
domain, using weights that are inferred by a learned model. We show
state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple
benchmarks. In particular, for two speakers, our method is able to surpass what
was previously considered the upper performance bound.",,
"A Holistic Cascade System, benchmark, and Human Evaluation Protocol for Expressive Speech-to-Speech Translation","[arxiv.Result.Author('Wen-Chin Huang'), arxiv.Result.Author('Benjamin Peloquin'), arxiv.Result.Author('Justine Kao'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Hongyu Gong'), arxiv.Result.Author('Elizabeth Salesky'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Peng-Jen Chen')]",2023-01-25 14:27:00+00:00,"Expressive speech-to-speech translation (S2ST) aims to transfer prosodic
attributes of source speech to target speech while maintaining translation
accuracy. Existing research in expressive S2ST is limited, typically focusing
on a single expressivity aspect at a time. Likewise, this research area lacks
standard evaluation protocols and well-curated benchmark datasets. In this
work, we propose a holistic cascade system for expressive S2ST, combining
multiple prosody transfer techniques previously considered only in isolation.
We curate a benchmark expressivity test set in the TV series domain and
explored a second dataset in the audiobook domain. Finally, we present a human
evaluation protocol to assess multiple expressive dimensions across speech
pairs. Experimental results indicate that bi-lingual annotators can assess the
quality of expressive preservation in S2ST systems, and the holistic modeling
approach outperforms single-aspect systems. Audio samples can be accessed
through our demo webpage:
https://facebookresearch.github.io/speech_translation/cascade_expressive_s2st.",This is the full version of our submission to ICASSP 2023,
On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems,"[arxiv.Result.Author('Philippe Gonzalez'), arxiv.Result.Author('Tommy Sonne Alstrøm'), arxiv.Result.Author('Tobias May')]",2023-01-25 13:45:02+00:00,"The performance of neural network-based speech enhancement systems is
primarily influenced by the model architecture, whereas training times and
computational resource utilization are primarily affected by training
parameters such as the batch size. Since noisy and reverberant speech mixtures
can have different duration, a batching strategy is required to handle variable
size inputs during training, in particular for state-of-the-art end-to-end
systems. Such strategies usually strive a compromise between zero-padding and
data randomization, and can be combined with a dynamic batch size for a more
consistent amount of data in each batch. However, the effect of these practices
on resource utilization and more importantly network performance is not well
documented. This paper is an empirical study of the effect of different
batching strategies and batch sizes on the training statistics and speech
enhancement performance of a Conv-TasNet, evaluated in both matched and
mismatched conditions. We find that using a small batch size during training
improves performance in both conditions for all batching strategies. Moreover,
using sorted or bucket batching with a dynamic batch size allows for reduced
training time and GPU memory usage while achieving similar performance compared
to random batching with a fixed batch size.",,
Evaluation of the syllables pronunciation quality in speech rehabilitation through the solution of the classification problem,[arxiv.Result.Author('Evgeny Kostyuchenko')],2023-01-25 13:43:59+00:00,"The solution of the problem of assessing the quality of the pronunciation of
syllables during speech rehabilitation after surgical treatment of oncological
diseases of the organs of the speech-forming tract is considered in the work.
The assessment is carried out by solving the problem of classifying syllables
into two classes: before and immediately after surgical treatment. A classifier
is built on the basis of the LSTM neural network and trained on the records
before the operation and immediately after it, before the start of speech
rehabilitation. The measure of assessing the quality of syllables pronunciation
in the process of rehabilitation is the metric of belonging to the class before
the operation. A study is being made of the influence of taking into account
problematic phonemes, the gender of the patient, his individual characteristics
on the resulting estimates of the quality of pronunciation. A comparison with
existing types of syllable pronunciation quality assessments is carried out,
recommendations are given for the practical application of the resulting new
class of pronunciation quality assessments.",,
HEAR4Health: A blueprint for making computer audition a staple of modern healthcare,"[arxiv.Result.Author('Andreas Triantafyllopoulos'), arxiv.Result.Author('Alexander Kathan'), arxiv.Result.Author('Alice Baird'), arxiv.Result.Author('Lukas Christ'), arxiv.Result.Author('Alexander Gebhard'), arxiv.Result.Author('Maurice Gerczuk'), arxiv.Result.Author('Vincent Karas'), arxiv.Result.Author('Tobias Hübner'), arxiv.Result.Author('Xin Jing'), arxiv.Result.Author('Shuo Liu'), arxiv.Result.Author('Adria Mallol-Ragolta'), arxiv.Result.Author('Manuel Milling'), arxiv.Result.Author('Sandra Ottl'), arxiv.Result.Author('Anastasia Semertzidou'), arxiv.Result.Author('Srividya Tirunellai Rajamani'), arxiv.Result.Author('Tianhao Yan'), arxiv.Result.Author('Zijiang Yang'), arxiv.Result.Author('Judith Dineley'), arxiv.Result.Author('Shahin Amiriparian'), arxiv.Result.Author('Katrin D. Bartl-Pokorny'), arxiv.Result.Author('Anton Batliner'), arxiv.Result.Author('Florian B. Pokorny'), arxiv.Result.Author('Björn W. Schuller')]",2023-01-25 09:25:08+00:00,"Recent years have seen a rapid increase in digital medicine research in an
attempt to transform traditional healthcare systems to their modern,
intelligent, and versatile equivalents that are adequately equipped to tackle
contemporary challenges. This has led to a wave of applications that utilise AI
technologies; first and foremost in the fields of medical imaging, but also in
the use of wearables and other intelligent sensors. In comparison, computer
audition can be seen to be lagging behind, at least in terms of commercial
interest. Yet, audition has long been a staple assistant for medical
practitioners, with the stethoscope being the quintessential sign of doctors
around the world. Transforming this traditional technology with the use of AI
entails a set of unique challenges. We categorise the advances needed in four
key pillars: Hear, corresponding to the cornerstone technologies needed to
analyse auditory signals in real-life conditions; Earlier, for the advances
needed in computational and data efficiency; Attentively, for accounting to
individual differences and handling the longitudinal nature of medical data;
and, finally, Responsibly, for ensuring compliance to the ethical standards
accorded to the field of medicine.",,
ViDeBERTa: A powerful pre-trained language model for Vietnamese,"[arxiv.Result.Author('Cong Dao Tran'), arxiv.Result.Author('Nhut Huy Pham'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Truong Son Hy'), arxiv.Result.Author('Tu Vu')]",2023-01-25 07:26:54+00:00,"This paper presents ViDeBERTa, a new pre-trained monolingual language model
for Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and
ViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality
and diverse Vietnamese texts using DeBERTa architecture. Although many
successful pre-trained language models based on Transformer have been widely
proposed for the English language, there are still few pre-trained models for
Vietnamese, a low-resource language, that perform good results on downstream
tasks, especially Question answering. We fine-tune and evaluate our model on
three important natural language downstream tasks, Part-of-speech tagging,
Named-entity recognition, and Question answering. The empirical results
demonstrate that ViDeBERTa with far fewer parameters surpasses the previous
state-of-the-art models on multiple Vietnamese-specific natural language
understanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only
about 23% of PhoBERT_large with 370M parameters, still performs the same or
better results than the previous state-of-the-art model. Our ViDeBERTa models
are available at: https://github.com/HySonLab/ViDeBERTa.",,
BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing,"[arxiv.Result.Author('Jiali Wei'), arxiv.Result.Author('Ming Fan'), arxiv.Result.Author('Wenjing Jiao'), arxiv.Result.Author('Wuxia Jin'), arxiv.Result.Author('Ting Liu')]",2023-01-25 05:24:46+00:00,"Deep neural networks (DNNs) and natural language processing (NLP) systems
have developed rapidly and have been widely used in various real-world fields.
However, they have been shown to be vulnerable to backdoor attacks.
Specifically, the adversary injects a backdoor into the model during the
training phase, so that input samples with backdoor triggers are classified as
the target class. Some attacks have achieved high attack success rates on the
pre-trained language models (LMs), but there have yet to be effective defense
methods. In this work, we propose a defense method based on deep model mutation
testing. Our main justification is that backdoor samples are much more robust
than clean samples if we impose random mutations on the LMs and that backdoors
are generalizable. We first confirm the effectiveness of model mutation testing
in detecting backdoor samples and select the most appropriate mutation
operators. We then systematically defend against three extensively studied
backdoor attack levels (i.e., char-level, word-level, and sentence-level) by
detecting backdoor samples. We also make the first attempt to defend against
the latest style-level backdoor attacks. We evaluate our approach on three
benchmark datasets (i.e., IMDB, Yelp, and AG news) and three style transfer
datasets (i.e., SST-2, Hate-speech, and AG news). The extensive experimental
results demonstrate that our approach can detect backdoor samples more
efficiently and accurately than the three state-of-the-art defense approaches.",,
Multilingual Multiaccented Multispeaker TTS with RADTTS,"[arxiv.Result.Author('Rohan Badlani'), arxiv.Result.Author('Rafael Valle'), arxiv.Result.Author('Kevin J. Shih'), arxiv.Result.Author('João Felipe Santos'), arxiv.Result.Author('Siddharth Gururani'), arxiv.Result.Author('Bryan Catanzaro')]",2023-01-24 22:39:04+00:00,"We work to create a multilingual speech synthesis system which can generate
speech with the proper accent while retaining the characteristics of an
individual voice. This is challenging to do because it is expensive to obtain
bilingual training data in multiple languages, and the lack of such data
results in strong correlations that entangle speaker, language, and accent,
resulting in poor transfer capabilities. To overcome this, we present a
multilingual, multiaccented, multispeaker speech synthesis model based on
RADTTS with explicit control over accent, language, speaker and fine-grained
$F_0$ and energy features. Our proposed model does not rely on bilingual
training data. We demonstrate an ability to control synthesized accent for any
speaker in an open-source dataset comprising of 7 accents. Human subjective
evaluation demonstrates that our model can better retain a speaker's voice and
accent quality than controlled baselines while synthesizing fluent speech in
all target languages and accents in our dataset.","5 pages, submitted to ICASSP 2023",
WhisperWand: Simultaneous Voice and Gesture Tracking Interface,"[arxiv.Result.Author('Yang Bai'), arxiv.Result.Author('Irtaza Shahid'), arxiv.Result.Author('Harshvardhan Takawale'), arxiv.Result.Author('Nirupam Roy')]",2023-01-24 21:30:11+00:00,"This paper presents the design and implementation of WhisperWand, a
comprehensive voice and motion tracking interface for voice assistants.
Distinct from prior works, WhisperWand is a precise tracking interface that can
co-exist with the voice interface on low sampling rate voice assistants. Taking
handwriting as a specific application, it can also capture natural strokes and
the individualized style of writing while occupying only a single frequency.
The core technique includes an accurate acoustic ranging method called Cross
Frequency Continuous Wave (CFCW) sonar, enabling voice assistants to use
ultrasound as a ranging signal while using the regular microphone system of
voice assistants as a receiver. We also design a new optimization algorithm
that only requires a single frequency for time difference of arrival.
WhisperWand prototype achieves 73 um of median error for 1D ranging and 1.4 mm
of median error in 3D tracking of an acoustic beacon using the microphone array
used in voice assistants. Our implementation of an in-air handwriting interface
achieves 94.1% accuracy with automatic handwriting-to-text software, similar to
writing on paper (96.6%). At the same time, the error rate of voice-based user
authentication only increases from 6.26% to 8.28%.",,
Perceptual evaluation of listener envelopment using spatial granular synthesis,"[arxiv.Result.Author('Stefan Riedel'), arxiv.Result.Author('Matthias Frank'), arxiv.Result.Author('Franz Zotter')]",2023-01-24 18:36:13+00:00,"Listener envelopment refers to the sensation of being surrounded by sound,
either by multiple direct sound events or by a diffuse reverberant sound field.
More recently, a specific attribute for the sensation of being covered by sound
from elevated directions has been proposed by Sazdov et al. and was termed
listener engulfment. This contribution investigates the effect of the temporal
and directional density of sound events on listener envelopment and engulfment.
A spatial granular synthesis technique is used to precisely control the
temporal and directional density of sound events. Experimental results indicate
that a directionally uniform distribution of sound events at time intervals
$\Delta t < 20$ milliseconds is required to elicit a sensation of diffuse
envelopment, whereas longer time intervals lead to localized auditory events.
It shows that elevated loudspeaker layers do not increase envelopment, but
contribute specifically to listener engulfment. Lowpass-filtered stimuli
increase envelopment, but lead to a decreased control over engulfment. The
results can be exploited in the technical design and creative application of
spatial sound synthesis and reverberation algorithms.",Submitted to the Journal of the Audio Engineering Society (JAES),
ViHOS: Hate Speech Spans Detection for Vietnamese,"[arxiv.Result.Author('Phu Gia Hoang'), arxiv.Result.Author('Canh Duc Luu'), arxiv.Result.Author('Khanh Quoc Tran'), arxiv.Result.Author('Kiet Van Nguyen'), arxiv.Result.Author('Ngan Luu-Thuy Nguyen')]",2023-01-24 17:53:21+00:00,"The rise in hateful and offensive language directed at other users is one of
the adverse side effects of the increased use of social networking platforms.
This could make it difficult for human moderators to review tagged comments
filtered by classification systems. To help address this issue, we present the
ViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated
corpus containing 26k spans on 11k comments. We also provide definitions of
hateful and offensive spans in Vietnamese comments as well as detailed
annotation guidelines. Besides, we conduct experiments with various
state-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best
F1-scores in Single span detection and All spans detection, while
PhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,
our error analysis demonstrates the difficulties in detecting specific types of
spans in our data for future research.
  Disclaimer: This paper contains real comments that could be considered
profane, offensive, or abusive.",EACL 2023,
Mesostructures: Beyond Spectrogram Loss in Differentiable Time-Frequency Analysis,"[arxiv.Result.Author('Cyrus Vahidi'), arxiv.Result.Author('Han Han'), arxiv.Result.Author('Changhong Wang'), arxiv.Result.Author('Mathieu Lagrange'), arxiv.Result.Author('György Fazekas'), arxiv.Result.Author('Vincent Lostanlen')]",2023-01-24 17:50:19+00:00,"Computer musicians refer to mesostructures as the intermediate levels of
articulation between the microstructure of waveshapes and the macrostructure of
musical forms. Examples of mesostructures include melody, arpeggios,
syncopation, polyphonic grouping, and textural contrast. Despite their central
role in musical expression, they have received limited attention in deep
learning. Currently, autoencoders and neural audio synthesizers are only
trained and evaluated at the scale of microstructure: i.e., local amplitude
variations up to 100 milliseconds or so. In this paper, we formulate and
address the problem of mesostructural audio modeling via a composition of a
differentiable arpeggiator and time-frequency scattering. We empirically
demonstrate that time--frequency scattering serves as a differentiable model of
similarity between synthesis parameters that govern mesostructure. By exposing
the sensitivity of short-time spectral distances to time alignment, we motivate
the need for a time-invariant and multiscale differentiable time--frequency
model of similarity at the level of both local spectra and spectrotemporal
modulations.",,
Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses,"[arxiv.Result.Author('Yan Long'), arxiv.Result.Author('Pirouz Naghavi'), arxiv.Result.Author('Blas Kojusner'), arxiv.Result.Author('Kevin Butler'), arxiv.Result.Author('Sara Rampazzi'), arxiv.Result.Author('Kevin Fu')]",2023-01-24 15:00:47+00:00,"Our research discovers how the rolling shutter and movable lens structures
widely found in smartphone cameras modulate structure-borne sounds onto camera
images, creating a point-of-view (POV) optical-acoustic side channel for
acoustic eavesdropping. The movement of smartphone camera hardware leaks
acoustic information because images unwittingly modulate ambient sound as
imperceptible distortions. Our experiments find that the side channel is
further amplified by intrinsic behaviors of Complementary
metal-oxide-semiconductor (CMOS) rolling shutters and movable lenses such as in
Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes
the limits of acoustic information leakage caused by structure-borne sound that
perturbs the POV of smartphone cameras. In contrast with traditional
optical-acoustic eavesdropping on vibrating objects, this side channel requires
no line of sight and no object within the camera's field of view (images of a
ceiling suffice). Our experiments test the limits of this side channel with a
novel signal processing pipeline that extracts and recognizes the leaked
acoustic information. Our evaluation with 10 smartphones on a spoken digit
dataset reports 80.66%, 91.28%, and 99.67% accuracies on recognizing 10 spoken
digits, 20 speakers, and 2 genders respectively. We further systematically
discuss the possible defense strategies and implementations. By modeling,
measuring, and demonstrating the limits of acoustic eavesdropping from
smartphone camera image streams, our contributions explain the physics-based
causality and possible ways to reduce the threat on current and future devices.",,2023 IEEE Symposium on Security and Privacy (SP)
DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model,"[arxiv.Result.Author('Fan Zhang'), arxiv.Result.Author('Naye Ji'), arxiv.Result.Author('Fuxing Gao'), arxiv.Result.Author('Yongping Li')]",2023-01-24 14:44:03+00:00,"Speech-driven gesture synthesis is a field of growing interest in virtual
human creation. However, a critical challenge is the inherent intricate
one-to-many mapping between speech and gestures. Previous studies have explored
and achieved significant progress with generative models. Notwithstanding, most
synthetic gestures are still vastly less natural. This paper presents
DiffMotion, a novel speech-driven gesture synthesis architecture based on
diffusion models. The model comprises an autoregressive temporal encoder and a
denoising diffusion probability Module. The encoder extracts the temporal
context of the speech input and historical gestures. The diffusion module
learns a parameterized Markov chain to gradually convert a simple distribution
into a complex distribution and generates the gestures according to the
accompanied speech. Compared with baselines, objective and subjective
evaluations confirm that our approach can produce natural and diverse
gesticulation and demonstrate the benefits of diffusion-based models on
speech-driven gesture synthesis.","13 pages, 3 figures",
A Comparison of Temporal Encoders for Neuromorphic Keyword Spotting with Few Neurons,"[arxiv.Result.Author('Mattias Nilsson'), arxiv.Result.Author('Ton Juny Pina'), arxiv.Result.Author('Lyes Khacef'), arxiv.Result.Author('Foteini Liwicki'), arxiv.Result.Author('Elisabetta Chicca'), arxiv.Result.Author('Fredrik Sandin')]",2023-01-24 12:50:54+00:00,"With the expansion of AI-powered virtual assistants, there is a need for
low-power keyword spotting systems providing a ""wake-up"" mechanism for
subsequent computationally expensive speech recognition. One promising approach
is the use of neuromorphic sensors and spiking neural networks (SNNs)
implemented in neuromorphic processors for sparse event-driven sensing.
However, this requires resource-efficient SNN mechanisms for temporal encoding,
which need to consider that these systems process information in a streaming
manner, with physical time being an intrinsic property of their operation. In
this work, two candidate neurocomputational elements for temporal encoding and
feature extraction in SNNs described in recent literature - the spiking
time-difference encoder (TDE) and disynaptic excitatory-inhibitory (E-I)
elements - are comparatively investigated in a keyword-spotting task on
formants computed from spoken digits in the TIDIGITS dataset. While both
encoders improve performance over direct classification of the formant features
in the training data, enabling a complete binary classification with a logistic
regression model, they show no clear improvements on the test set.
Resource-efficient keyword spotting applications may benefit from the use of
these encoders, but further work on methods for learning the time constants and
weights is required to investigate their full potential.","This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible",
A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era,"[arxiv.Result.Author('Zhao Ren'), arxiv.Result.Author('Yi Chang'), arxiv.Result.Author('Thanh Tam Nguyen'), arxiv.Result.Author('Yang Tan'), arxiv.Result.Author('Kun Qian'), arxiv.Result.Author('Björn W. Schuller')]",2023-01-23 10:58:45+00:00,"Heart sound auscultation has been demonstrated to be beneficial in clinical
usage for early screening of cardiovascular diseases. Due to the high
requirement of well-trained professionals for auscultation, automatic
auscultation benefiting from signal processing and machine learning can help
auxiliary diagnosis and reduce the burdens of training professional clinicians.
Nevertheless, classic machine learning is limited to performance improvement in
the era of big data. Deep learning has achieved better performance than classic
machine learning in many research fields, as it employs more complex model
architectures with stronger capability of extracting effective representations.
Deep learning has been successfully applied to heart sound analysis in the past
years. As most review works about heart sound analysis were given before 2017,
the present survey is the first to work on a comprehensive overview to
summarise papers on heart sound analysis with deep learning in the past six
years 2017--2022. We introduce both classic machine learning and deep learning
for comparison, and further offer insights about the advances and future
research directions in deep learning for heart sound analysis.",,
Deep Attention-Based Alignment Network for Melody Generation from Incomplete Lyrics,"[arxiv.Result.Author('Gurunath Reddy M'), arxiv.Result.Author('Zhe Zhang'), arxiv.Result.Author('Yi Yu'), arxiv.Result.Author('Florian Harscoet'), arxiv.Result.Author('Simon Canales'), arxiv.Result.Author('Suhua Tang')]",2023-01-23 03:41:53+00:00,"We propose a deep attention-based alignment network, which aims to
automatically predict lyrics and melody with given incomplete lyrics as input
in a way similar to the music creation of humans. Most importantly, a deep
neural lyrics-to-melody net is trained in an encoder-decoder way to predict
possible pairs of lyrics-melody when given incomplete lyrics (few keywords).
The attention mechanism is exploited to align the predicted lyrics with the
melody during the lyrics-to-melody generation. The qualitative and quantitative
evaluation metrics reveal that the proposed method is indeed capable of
generating proper lyrics and corresponding melody for composing new songs given
a piece of incomplete seed lyrics.",arXiv admin note: substantial text overlap with arXiv:2011.06380,
Efficient Encoders for Streaming Sequence Tagging,"[arxiv.Result.Author('Ayush Kaushal'), arxiv.Result.Author('Aditya Gupta'), arxiv.Result.Author('Shyam Upadhyay'), arxiv.Result.Author('Manaal Faruqui')]",2023-01-23 02:20:39+00:00,"A naive application of state-of-the-art bidirectional encoders for streaming
sequence tagging would require encoding each token from scratch for each new
token in an incremental streaming input (like transcribed speech). The lack of
re-usability of previous computation leads to a higher number of Floating Point
Operations (or FLOPs) and higher number of unnecessary label flips. Increased
FLOPs consequently lead to higher wall-clock time and increased label flipping
leads to poorer streaming performance. In this work, we present a Hybrid
Encoder with Adaptive Restart (HEAR) that addresses these issues while
maintaining the performance of bidirectional encoders over the offline (or
complete) inputs while improving performance on streaming (or incomplete)
inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to
perform sequence tagging, along with an Adaptive Restart Module (ARM) to
selectively guide the restart of bidirectional portion of the encoder. Across
four sequence tagging tasks, HEAR offers FLOP savings in streaming settings
upto 71.1% and also outperforms bidirectional encoders for streaming
predictions by upto +10% streaming exact match.",EACL 2023,
"Estimation of Source and Receiver Positions, Room Geometry and Reflection Coefficients From a Single Room Impulse Response","[arxiv.Result.Author('Wangyang Yu'), arxiv.Result.Author('W. Bastiaan Kleijn')]",2023-01-22 20:46:10+00:00,"We propose an algorithm to estimate source and receiver positions, room
geometry and reflection coefficients from a single room impulse response
simultaneously. It is based on a symmetry analysis of the room impulse
response. The proposed method utilizes the times of arrivals of the direct
path, first order reflections and second order reflections. The proposed method
is robust to erroneous pulses and non-specular reflections. It can be applied
to any room with parallel walls as long as the required arrival times of
reflections are available. In contrast to the state-of-art method, we do not
restrict the location of source and receiver.",,
Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study,"[arxiv.Result.Author('Massa Baali'), arxiv.Result.Author('Tomoki Hayashi'), arxiv.Result.Author('Hamdy Mubarak'), arxiv.Result.Author('Soumi Maiti'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Wassim El-Hajj'), arxiv.Result.Author('Ahmed Ali')]",2023-01-22 10:41:58+00:00,"Several high-resource Text to Speech (TTS) systems currently produce natural,
well-established human-like speech. In contrast, low-resource languages,
including Arabic, have very limited TTS systems due to the lack of resources.
We propose a fully unsupervised method for building TTS, including automatic
data selection and pre-training/fine-tuning strategies for TTS training, using
broadcast news as a case study. We show how careful selection of data, yet
smaller amounts, can improve the efficiency of TTS system in generating more
natural speech than a system trained on a bigger dataset. We adopt to propose
different approaches for the: 1) data: we applied automatic annotations using
DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for
fixing transcriptions' errors; 2) model: we used transfer learning from
high-resource language in TTS model and fine-tuned it with one hour broadcast
recording then we used this model to guide a FastSpeech2-based Conformer model
for duration. Our objective evaluation shows 3.9% character error rate (CER),
while the groundtruth has 1.3% CER. As for the subjective evaluation, where 1
is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a
mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,
where many annotators recognized the voice of the broadcaster, which proves the
effectiveness of our proposed unsupervised method.",,
Dance2MIDI: Dance-driven multi-instruments music generation,"[arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Yi Ren')]",2023-01-22 08:35:51+00:00,"Dance-driven music generation aims to generate musical pieces conditioned on
dance videos. Previous works focus on monophonic or raw audio generation, while
the multiinstruments scenario is under-explored. The challenges of the
dance-driven multi-instruments music (MIDI) generation are two-fold: 1) no
publicly available multi-instruments MIDI and video paired dataset and 2) the
weak correlation between music and video. To tackle these challenges, we build
the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based on
our proposed dataset, we introduce a multi-instruments MIDI generation
framework (Dance2MIDI) conditioned on dance video. Specifically, 1) to model
the correlation between music and dance, we encode the dance motion using the
GCN, and 2) to generate harmonious and coherent music, we employ Transformer to
decode the MIDI sequence. We evaluate the generated music of our framework
trained on D2MIDI dataset and demonstrate that our method outperforms existing
methods. The data and code are available on
https://github.com/Dance2MIDI/Dance2MIDI",,
Leveraging Speaker Embeddings with Adversarial Multi-task Learning for Age Group Classification,"[arxiv.Result.Author('Kwangje Baeg'), arxiv.Result.Author('Yeong-Gwan Kim'), arxiv.Result.Author('Young-Sub Han'), arxiv.Result.Author('Byoung-Ki Jeon')]",2023-01-22 05:01:13+00:00,"Recently, researchers have utilized neural network-based speaker embedding
techniques in speaker-recognition tasks to identify speakers accurately.
However, speaker-discriminative embeddings do not always represent speech
features such as age group well. In an embedding model that has been highly
trained to capture speaker traits, the task of age group classification is
closer to speech information leakage. Hence, to improve age group
classification performance, we consider the use of speaker-discriminative
embeddings derived from adversarial multi-task learning to align features and
reduce the domain discrepancy in age subgroups. In addition, we investigated
different types of speaker embeddings to learn and generalize the
domain-invariant representations for age groups. Experimental results on the
VoxCeleb Enrichment dataset verify the effectiveness of our proposed adaptive
adversarial network in multi-objective scenarios and leveraging speaker
embeddings for the domain adaptation task.",,
Cellular Network Speech Enhancement: Removing Background and Transmission Noise,"[arxiv.Result.Author('Amanda Shu'), arxiv.Result.Author('Hamza Khalid'), arxiv.Result.Author('Haohui Liu'), arxiv.Result.Author('Shikhar Agnihotri'), arxiv.Result.Author('Joseph Konan'), arxiv.Result.Author('Ojas Bhargave')]",2023-01-22 00:18:10+00:00,"The primary objective of speech enhancement is to reduce background noise
while preserving the target's speech. A common dilemma occurs when a speaker is
confined to a noisy environment and receives a call with high background and
transmission noise. To address this problem, the Deep Noise Suppression (DNS)
Challenge focuses on removing the background noise with the next-generation
deep learning models to enhance the target's speech; however, researchers fail
to consider Voice Over IP (VoIP) applications their transmission noise.
Focusing on Google Meet and its cellular application, our work achieves
state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS
Challenge. This paper demonstrates how to beat industrial performance and
achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity,
perceptual quality, and intelligibility in various metrics.",,
New Challenges for Content Privacy in Speech and Audio,"[arxiv.Result.Author('Jennifer Williams'), arxiv.Result.Author('Karla Pizzi'), arxiv.Result.Author('Shuvayanti Das'), arxiv.Result.Author('Paul-Gauthier Noe')]",2023-01-21 09:16:09+00:00,"Privacy in speech and audio has many facets. A particularly under-developed
area of privacy in this domain involves consideration for information related
to content and context. Speech content can include words and their meaning or
even stylistic markers, pathological speech, intonation patterns, or emotion.
More generally, audio captured in-the-wild may contain background speech or
reveal contextual information such as markers of location, room
characteristics, paralinguistic sounds, or other audible events. Audio
recording devices and speech technologies are becoming increasingly commonplace
in everyday life. At the same time, commercialised speech and audio
technologies do not provide consumers with a range of privacy choices. Even
where privacy is regulated or protected by law, technical solutions to privacy
assurance and enforcement fall short. This position paper introduces three
important and timely research challenges for content privacy in speech and
audio. We highlight current gaps and opportunities, and identify focus areas,
that could have significant implications for developing ethical and safer
speech technologies.",Accepted for publication in ISCA SPSC Symposium 2022,
A Multi-Purpose Audio-Visual Corpus for Multi-Modal Persian Speech Recognition: the Arman-AV Dataset,"[arxiv.Result.Author('Javad Peymanfard'), arxiv.Result.Author('Samin Heydarian'), arxiv.Result.Author('Ali Lashini'), arxiv.Result.Author('Hossein Zeinali'), arxiv.Result.Author('Mohammad Reza Mohammadi'), arxiv.Result.Author('Nasser Mozayani')]",2023-01-21 05:13:30+00:00,"In recent years, significant progress has been made in automatic lip reading.
But these methods require large-scale datasets that do not exist for many
low-resource languages. In this paper, we have presented a new multipurpose
audio-visual dataset for Persian. This dataset consists of almost 220 hours of
videos with 1760 corresponding speakers. In addition to lip reading, the
dataset is suitable for automatic speech recognition, audio-visual speech
recognition, and speaker recognition. Also, it is the first large-scale lip
reading dataset in Persian. A baseline method was provided for each mentioned
task. In addition, we have proposed a technique to detect visemes (a visual
equivalent of a phoneme) in Persian. The visemes obtained by this method
increase the accuracy of the lip reading task by 7% relatively compared to the
previously proposed visemes, which can be applied to other languages as well.",,
Regeneration Learning: A Learning Paradigm for Data Generation,"[arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Tao Qin'), arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Tie-Yan Liu'), arxiv.Result.Author('Yoshua Bengio')]",2023-01-21 01:33:34+00:00,"Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.",,
Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,"[arxiv.Result.Author('Yinghao Aaron Li'), arxiv.Result.Author('Cong Han'), arxiv.Result.Author('Xilin Jiang'), arxiv.Result.Author('Nima Mesgarani')]",2023-01-20 21:36:16+00:00,"Large-scale pre-trained language models have been shown to be helpful in
improving the naturalness of text-to-speech (TTS) models by enabling them to
produce more naturalistic prosodic patterns. However, these models are usually
word-level or sup-phoneme-level and jointly trained with phonemes, making them
inefficient for the downstream TTS task where only phonemes are needed. In this
work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of
predicting the corresponding graphemes along with the regular masked phoneme
predictions. Subjective evaluations show that our phoneme-level BERT encoder
has significantly improved the mean opinion scores (MOS) of rated naturalness
of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS
baseline on out-of-distribution (OOD) texts.",,
Novel-View Acoustic Synthesis,"[arxiv.Result.Author('Changan Chen'), arxiv.Result.Author('Alexander Richard'), arxiv.Result.Author('Roman Shapovalov'), arxiv.Result.Author('Vamsi Krishna Ithapu'), arxiv.Result.Author('Natalia Neverova'), arxiv.Result.Author('Kristen Grauman'), arxiv.Result.Author('Andrea Vedaldi')]",2023-01-20 18:49:58+00:00,"We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the sound of that
scene from an unseen target viewpoint? We propose a neural rendering approach:
Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize
the sound of an arbitrary point in space by analyzing the input audio-visual
cues. To benchmark this task, we collect two first-of-their-kind large-scale
multi-view audio-visual datasets, one synthetic and one real. We show that our
model successfully reasons about the spatial cues and synthesizes faithful
audio on both datasets. To our knowledge, this work represents the very first
formulation, dataset, and approach to solve the novel-view acoustic synthesis
task, which has exciting potential applications ranging from AR/VR to art and
design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.",Project page: https://vision.cs.utexas.edu/projects/nvas,
Neural Architecture Search: Insights from 1000 Papers,"[arxiv.Result.Author('Colin White'), arxiv.Result.Author('Mahmoud Safari'), arxiv.Result.Author('Rhea Sukthanker'), arxiv.Result.Author('Binxin Ru'), arxiv.Result.Author('Thomas Elsken'), arxiv.Result.Author('Arber Zela'), arxiv.Result.Author('Debadeepta Dey'), arxiv.Result.Author('Frank Hutter')]",2023-01-20 18:47:24+00:00,"In the past decade, advances in deep learning have resulted in breakthroughs
in a variety of areas, including computer vision, natural language
understanding, speech recognition, and reinforcement learning. Specialized,
high-performing neural architectures are crucial to the success of deep
learning in these areas. Neural architecture search (NAS), the process of
automating the design of neural architectures for a given task, is an
inevitable next step in automating machine learning and has already outpaced
the best human-designed architectures on many tasks. In the past few years,
research in NAS has been progressing rapidly, with over 1000 papers released
since 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized
and comprehensive guide to neural architecture search. We give a taxonomy of
search spaces, algorithms, and speedup techniques, and we discuss resources
such as benchmarks, best practices, other surveys, and open-source libraries.",,
Adjoint-Based Identification of Sound Sources for Sound Reinforcement and Source Localization,"[arxiv.Result.Author('Mathias Lemke'), arxiv.Result.Author('Lewin Stein')]",2023-01-20 15:01:46+00:00,"The identification of sound sources is a common problem in acoustics.
Different parameters are sought, among these are signal and position of the
sources. We present an adjoint-based approach for sound source identification,
which employs computational aeroacoustic techniques. Two different applications
are presented as a proof-of-concept: optimization of a sound reinforcement
setup and the localization of (moving) sound sources.",,"Notes on Numerical Fluid Mechanics and Multidisciplinary Design,
  vol 145. Springer (2021)"
Language Agnostic Data-Driven Inverse Text Normalization,"[arxiv.Result.Author('Szu-Jui Chen'), arxiv.Result.Author('Debjyoti Paul'), arxiv.Result.Author('Yutong Pang'), arxiv.Result.Author('Peng Su'), arxiv.Result.Author('Xuedong Zhang')]",2023-01-20 10:33:03+00:00,"With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.",,
A Survey of research in Deep Learning for Robotics for Undergraduate research interns,"[arxiv.Result.Author('Narayanan PP'), arxiv.Result.Author('Palacode Narayana Iyer Anantharaman')]",2023-01-19 19:51:19+00:00,"Over the last several years, use cases for robotics based solutions have
diversified from factory floors to domestic applications. In parallel, Deep
Learning approaches are replacing traditional techniques in Computer Vision,
Natural Language Processing, Speech processing, etc. and are delivering robust
results. Our goal is to survey a number of research internship projects in the
broad area of 'Deep Learning as applied to Robotics' and present a concise view
for the benefit of aspiring student interns. In this paper, we survey the
research work done by Robotic Institute Summer Scholars (RISS), CMU. We
particularly focus on papers that use deep learning to solve core robotic
problems and also robotic solutions. We trust this would be useful particularly
for internship aspirants for the Robotics Institute, CMU","This document is a draft version at this stage and the final version
  will be created soon",
Everything is Connected: Graph Neural Networks,[arxiv.Result.Author('Petar Veličković')],2023-01-19 18:09:43+00:00,"In many ways, graphs are the main modality of data we receive from nature.
This is due to the fact that most of the patterns we see, both in natural and
artificial systems, are elegantly representable using the language of graph
structures. Prominent examples include molecules (represented as graphs of
atoms and bonds), social networks and transportation networks. This potential
has already been seen by key scientific and industrial groups, with
already-impacted application areas including traffic forecasting, drug
discovery, social network analysis and recommender systems. Further, some of
the most successful domains of application for machine learning in previous
years -- images, text and speech processing -- can be seen as special cases of
graph representation learning, and consequently there has been significant
exchange of information between these areas. The main aim of this short survey
is to enable the reader to assimilate the key concepts in the area, and
position graph representation learning in a proper context with related fields.","To appear in Current Opinion in Structural Biology. 14 pages, 1
  figure",
Complex Mapping between Neural Response Frequency and Hierarchical Linguistic Units in Natural Speech,"[arxiv.Result.Author('Yuran Zhang'), arxiv.Result.Author('Jiajie Zou'), arxiv.Result.Author('Nai Ding')]",2023-01-19 11:33:03+00:00,"When listening to connected speech, the brain can extract multiple levels of
linguistic units, such as syllables, words, and sentences. It has been
hypothesized that the time scale of cortical activity encoding each linguistic
unit is commensurate with the time scale of that linguistic unit in speech.
Evidence for the hypothesis originally comes from studies using the
frequency-tagging paradigm that presents each linguistic unit at a constant
rate, and more recently extends to studies on natural speech. For natural
speech, it is sometimes assumed that neural activity tracking the speech
envelope near 1 Hz encodes phrase-level information while neural activity
tracking the speech envelope around 4-5 Hz encodes syllable-level information.
Here, however, it is demonstrated using simulations that a neural response that
only tracks the onset of each syllable can strongly correlate with the speech
envelope both around 4-5 Hz and below 1 Hz. Further analyses reveal that the
1-Hz correlation mainly originates from the pauses in connected speech. The
results here suggest that a simple frequency-domain analysis cannot separate
the neural tracking of different linguistic units in natural speech.",,
SpotHitPy: A Study For ML-Based Song Hit Prediction Using Spotify,"[arxiv.Result.Author('Ioannis Dimolitsas'), arxiv.Result.Author('Spyridon Kantarelis'), arxiv.Result.Author('Afroditi Fouka')]",2023-01-19 10:13:52+00:00,"In this study, we approached the Hit Song Prediction problem, which aims to
predict which songs will become Billboard hits. We gathered a dataset of nearly
18500 hit and non-hit songs and extracted their audio features using the
Spotify Web API. We test four machine-learning models on our dataset. We were
able to predict the Billboard success of a song with approximately 86\%
accuracy. The most succesful algorithms were Random Forest and Support Vector
Machine.",,
THLNet: two-stage heterogeneous lightweight network for monaural speech enhancement,"[arxiv.Result.Author('Feng Dang'), arxiv.Result.Author('Qi Hu'), arxiv.Result.Author('Pengyuan Zhang')]",2023-01-19 08:17:22+00:00,"In this paper, we propose a two-stage heterogeneous lightweight network for
monaural speech enhancement. Specifically, we design a novel two-stage
framework consisting of a coarse-grained full-band mask estimation stage and a
fine-grained low-frequency refinement stage. Instead of using a hand-designed
real-valued filter, we use a novel learnable complex-valued rectangular
bandwidth (LCRB) filter bank as an extractor of compact features. Furthermore,
considering the respective characteristics of the proposed two-stage task, we
used a heterogeneous structure, i.e., a U-shaped subnetwork as the backbone of
CoarseNet and a single-scale subnetwork as the backbone of FineNet. We
conducted experiments on the VoiceBank + DEMAND and DNS datasets to evaluate
the proposed approach. The experimental results show that the proposed method
outperforms the current state-of-the-art methods, while maintaining relatively
small model size and low computational complexity.",,
Subject-Independent Classification of Brain Signals using Skip Connections,"[arxiv.Result.Author('Soowon Kim'), arxiv.Result.Author('Ji-Won Lee'), arxiv.Result.Author('Young-Eun Lee'), arxiv.Result.Author('Seo-Hyun Lee')]",2023-01-19 07:04:11+00:00,"Untapped potential for new forms of human-to-human communication can be found
in the active research field of studies on the decoding of brain signals of
human speech. A brain-computer interface system can be implemented using
electroencephalogram signals because it poses more less clinical risk and can
be acquired using portable instruments. One of the most interesting tasks for
the brain-computer interface system is decoding words from the raw
electroencephalogram signals. Before a brain-computer interface may be used by
a new user, current electroencephalogram-based brain-computer interface
research typically necessitates a subject-specific adaption stage. In contrast,
the subject-independent situation is one that is highly desired since it allows
a well-trained model to be applied to new users with little or no
precalibration. The emphasis is on creating an efficient decoder that may be
employed adaptively in subject-independent circumstances in light of this
crucial characteristic. Our proposal is to explicitly apply skip connections
between convolutional layers to enable the flow of mutual information between
layers. To do this, we add skip connections between layers, allowing the mutual
information to flow throughout the layers. The output of the encoder is then
passed through the fully-connected layer to finally represent the probabilities
of the 13 classes. In this study, overt speech was used to record the
electroencephalogram data of 16 participants. The results show that when the
skip connection is present, the classification performance improves notably.","4 pages, 3 figures, IEEE BCI winter conference 2023 accepted",
From English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition,"[arxiv.Result.Author('Chao-Han Huck Yang'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Nanxin Chen'), arxiv.Result.Author('Rohit Prabhavalkar'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Trevor Strohman')]",2023-01-19 02:37:56+00:00,"In this work, we propose a new parameter-efficient learning framework based
on neural model reprogramming for cross-lingual speech recognition, which can
\textbf{re-purpose} well-trained English automatic speech recognition (ASR)
models to recognize the other languages. We design different auxiliary neural
architectures focusing on learnable pre-trained feature enhancement that, for
the first time, empowers model reprogramming on ASR. Specifically, we
investigate how to select trainable components (i.e., encoder) of a
conformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments
on a seven-language multilingual LibriSpeech speech (MLS) task show that model
reprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of
its original trainable parameters from a full ASR model to perform competitive
results in a range of 11.9% to 8.1% WER averaged across different languages. In
addition, we discover different setups to make large-scale pre-trained ASR
succeed in both monolingual and multilingual speech recognition. Our methods
outperform existing ASR tuning architectures and their extension with
self-supervised losses (e.g., w2v-bert) in terms of lower WER and better
training efficiency.","Submitted to ICASSP 2023. The project was initiated in May 2022
  during a research internship at Google Research",
Warning: Humans Cannot Reliably Detect Speech Deepfakes,"[arxiv.Result.Author('Kimberly T. Mai'), arxiv.Result.Author('Sergi D. Bray'), arxiv.Result.Author('Toby Davies'), arxiv.Result.Author('Lewis D. Griffin')]",2023-01-19 00:17:48+00:00,"Speech deepfakes are artificial voices generated by machine learning models.
Previous literature has highlighted deepfakes as one of the biggest threats to
security arising from progress in AI due to their potential for misuse.
However, studies investigating human detection capabilities are limited. We
presented genuine and deepfake audio to $n$ = 529 individuals and asked them to
identify the deepfakes. We ran our experiments in English and Mandarin to
understand if language affects detection performance and decision-making
rationale. Detection capability is unreliable. Listeners only correctly spotted
the deepfakes 73% of the time, and there was no difference in detectability
between the two languages. Increasing listener awareness by providing examples
of speech deepfakes only improves results slightly. The difficulty of detecting
speech deepfakes confirms their potential for misuse and signals that defenses
against this threat are needed.",,
An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms,"[arxiv.Result.Author('Anastasia Natsiou'), arxiv.Result.Author('Luca Longo'), arxiv.Result.Author(""Sean O'Leary"")]",2023-01-18 17:19:04+00:00,"In audio processing applications, the generation of expressive sounds based
on high-level representations demonstrates a high demand. These representations
can be used to manipulate the timbre and influence the synthesis of creative
instrumental notes. Modern algorithms, such as neural networks, have inspired
the development of expressive synthesizers based on musical instrument timbre
compression. Unsupervised deep learning methods can achieve audio compression
by training the network to learn a mapping from waveforms or spectrograms to
low-dimensional representations. This study investigates the use of stacked
convolutional autoencoders for the compression of time-frequency audio
representations for a variety of instruments for a single pitch. Further
exploration of hyper-parameters and regularization techniques is demonstrated
to enhance the performance of the initial design. In an unsupervised manner,
the network is able to reconstruct a monophonic and harmonic sound based on
latent representations. In addition, we introduce an evaluation metric to
measure the similarity between the original and reconstructed samples.
Evaluating a deep generative model for the synthesis of sound is a challenging
task. Our approach is based on the accuracy of the generated frequencies as it
presents a significant metric for the perception of harmonic sounds. This work
is expected to accelerate future experiments on audio compression using neural
autoencoders.",,
Dereverberation in Acoustic Sensor Networks Using Weighted Prediction Error With Microphone-dependent Prediction Delays,"[arxiv.Result.Author('Anselm Lohmann'), arxiv.Result.Author('Toon van Waterschoot'), arxiv.Result.Author('Joerg Bitzer'), arxiv.Result.Author('Simon Doclo')]",2023-01-18 16:52:07+00:00,"In the last decades several multi-microphone speech dereverberation
algorithms have been proposed, among which the weighted prediction error (WPE)
algorithm. In the WPE algorithm, a prediction delay is required to reduce the
correlation between the prediction signals and the direct component in the
reference microphone signal. In compact arrays with closely-spaced microphones,
the prediction delay is often chosen microphone-independent. In acoustic sensor
networks with spatially distributed microphones, large
time-differences-of-arrival (TDOAs) of the speech source between the reference
microphone and other microphones typically occur. Hence, when using a
microphone-independent prediction delay the reference and prediction signals
may still be significantly correlated, leading to distortion in the
dereverberated output signal. In order to decorrelate the signals, in this
paper we propose to apply TDOA compensation with respect to the reference
microphone, resulting in microphone-dependent prediction delays for the WPE
algorithm. We consider both optimal TDOA compensation using crossband filtering
in the short-time Fourier transform domain as well as band-to-band and integer
delay approximations. Simulation results for different reverberation times
using oracle as well as estimated TDOAs clearly show the benefit of using
microphone-dependent prediction delays.",,
"Adapting Multilingual Speech Representation Model for a New, Underresourced Language through Multilingual Fine-tuning and Continued Pretraining","[arxiv.Result.Author('Karol Nowakowski'), arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Kyoko Murasaki'), arxiv.Result.Author('Jagna Nieuważny')]",2023-01-18 03:57:53+00:00,"In recent years, neural models learned through self-supervised pretraining on
large scale multilingual text or speech data have exhibited promising results
for underresourced languages, especially when a relatively large amount of data
from related language(s) is available. While the technology has a potential for
facilitating tasks carried out in language documentation projects, such as
speech transcription, pretraining a multilingual model from scratch for every
new language would be highly impractical. We investigate the possibility for
adapting an existing multilingual wav2vec 2.0 model for a new language,
focusing on actual fieldwork data from a critically endangered tongue: Ainu.
Specifically, we (i) examine the feasibility of leveraging data from similar
languages also in fine-tuning; (ii) verify whether the model's performance can
be improved by further pretraining on target language data. Our results show
that continued pretraining is the most effective method to adapt a wav2vec 2.0
model for a new language and leads to considerable reduction in error rates.
Furthermore, we find that if a model pretrained on a related speech variety or
an unrelated language with similar phonological characteristics is available,
multilingual fine-tuning using additional data from that language can have
positive impact on speech recognition performance when there is very little
labeled data in the target language.",14 pages,"Information Processing & Management, Volume 60, Issue 2, March
  2023, 103148, ISSN 0306-4573"
A variational autoencoder-based nonnegative matrix factorisation model for deep dictionary learning,"[arxiv.Result.Author('Hong-Bo Xie'), arxiv.Result.Author('Caoyuan Li'), arxiv.Result.Author('Shuliang Wang'), arxiv.Result.Author('Richard Yi Da Xu'), arxiv.Result.Author('Kerrie Mengersen')]",2023-01-18 02:36:03+00:00,"Construction of dictionaries using nonnegative matrix factorisation (NMF) has
extensive applications in signal processing and machine learning. With the
advances in deep learning, training compact and robust dictionaries using deep
neural networks, i.e., dictionaries of deep features, has been proposed. In
this study, we propose a probabilistic generative model which employs a
variational autoencoder (VAE) to perform nonnegative dictionary learning. In
contrast to the existing VAE models, we cast the model under a statistical
framework with latent variables obeying a Gamma distribution and design a new
loss function to guarantee the nonnegative dictionaries. We adopt an
acceptance-rejection sampling reparameterization trick to update the latent
variables iteratively. We apply the dictionaries learned from VAE-NMF to two
signal processing tasks, i.e., enhancement of speech and extraction of muscle
synergies. Experimental results demonstrate that VAE-NMF performs better in
learning the latent nonnegative dictionaries in comparison with
state-of-the-art methods.","7 pages, 2 figures",
Deep Unsupervised Phase-based 3D Incompressible Motion Estimation in Tagged-MRI,"[arxiv.Result.Author('Zhangxing Bian'), arxiv.Result.Author('Fangxu Xing'), arxiv.Result.Author('Jinglun Yu'), arxiv.Result.Author('Muhan Shao'), arxiv.Result.Author('Yihao Liu'), arxiv.Result.Author('Aaron Carass'), arxiv.Result.Author('Jonghye Woo'), arxiv.Result.Author('Jerry L. Prince')]",2023-01-18 00:16:30+00:00,"Tagged magnetic resonance imaging (MRI) has been used for decades to observe
and quantify the detailed motion of deforming tissue. However, this technique
faces several challenges such as tag fading, large motion, long computation
times, and difficulties in obtaining diffeomorphic incompressible flow fields.
To address these issues, this paper presents a novel unsupervised phase-based
3D motion estimation technique for tagged MRI. We introduce two key
innovations. First, we apply a sinusoidal transformation to the harmonic phase
input, which enables end-to-end training and avoids the need for phase
interpolation. Second, we propose a Jacobian determinant-based learning
objective to encourage incompressible flow fields for deforming biological
tissues. Our method efficiently estimates 3D motion fields that are accurate,
dense, and approximately diffeomorphic and incompressible. The efficacy of the
method is assessed using human tongue motion during speech, and includes both
healthy controls and patients that have undergone glossectomy. We show that the
method outperforms existing approaches, and also exhibits improvements in
speed, robustness to tag fading, and large tongue motion.",,
Multimodal Robot Programming by Demonstration: A Preliminary Exploration,"[arxiv.Result.Author('Gopika Ajaykumar'), arxiv.Result.Author('Chien-Ming Huang')]",2023-01-17 21:05:13+00:00,"Recent years have seen a growth in the number of industrial robots working
closely with end-users such as factory workers. This growing use of
collaborative robots has been enabled in part due to the availability of
end-user robot programming methods that allow users who are not robot
programmers to teach robots task actions. Programming by Demonstration (PbD) is
one such end-user programming method that enables users to bypass the
complexities of specifying robot motions using programming languages by instead
demonstrating the desired robot behavior. Demonstrations are often provided by
physically guiding the robot through the motions required for a task action in
a process known as kinesthetic teaching. Kinesthetic teaching enables users to
directly demonstrate task behaviors in the robot's configuration space, making
it a popular end-user robot programming method for collaborative robots known
for its low cognitive burden. However, because kinesthetic teaching restricts
the programmer's teaching to motion demonstrations, it fails to leverage
information from other modalities that humans naturally use when providing
physical task demonstrations to one other, such as gaze and speech.
Incorporating multimodal information into the traditional kinesthetic
programming workflow has the potential to enhance robot learning by
highlighting critical aspects of a program, reducing ambiguity, and improving
situational awareness for the robot learner and can provide insight into the
human programmer's intent and difficulties. In this extended abstract, we
describe a preliminary study on multimodal kinesthetic demonstrations and
future directions for using multimodal demonstrations to enhance robot learning
and user programming experiences.","6 pages, 6 figures, 2021 RSS Workshop on Accessibility of Robot
  Programming and the Work of the Future",
MooseNet: A trainable metric for synthesized speech with plda backend,"[arxiv.Result.Author('Ondřej Plátek'), arxiv.Result.Author('Ondřej Dušek')]",2023-01-17 18:53:15+00:00,"We present MooseNet, a trainable speech metric that predicts listeners' Mean
Opinion Score (MOS). We report improvements to the challenge baselines using
easy-to-use modeling techniques, which also scales for larger self-supervised
learning (SSL) model. We present two models. The first model is a Neural
Network (NN). As a second model, we propose a PLDA generative model on the top
layers of the first NN model, which improves the pure NN model. Ensembles from
our two models achieve the top 3 or 4 VoiceMOS leaderboard places on all system
and utterance level metrics.",,
The Newsbridge -Telecom SudParis VoxCeleb Speaker Recognition Challenge 2022 System Description,"[arxiv.Result.Author('Yannis Tevissen'), arxiv.Result.Author('Jérôme Boudy'), arxiv.Result.Author('Frédéric Petitpont')]",2023-01-17 15:52:39+00:00,"We describe the system used by our team for the VoxCeleb Speaker Recognition
Challenge 2022 (VoxSRC 2022) in the speaker diarization track. Our solution was
designed around a new combination of voice activity detection algorithms that
uses the strengths of several systems. We introduce a novel multi stream
approach with a decision protocol based on classifiers entropy. We called this
method a multi-stream voice activity detection and used it with standard
baseline diarization embeddings, clustering and resegmentation. With this work,
we successfully demonstrated that using a strong baseline and working only on
voice activity detection, one can achieved close to state-of-theart results.",,
2nd Swiss German Speech to Standard German Text Shared Task at SwissText 2022,"[arxiv.Result.Author('Michel Plüss'), arxiv.Result.Author('Yanick Schraner'), arxiv.Result.Author('Christian Scheller'), arxiv.Result.Author('Manfred Vogel')]",2023-01-17 10:31:11+00:00,"We present the results and findings of the 2nd Swiss German speech to
Standard German text shared task at SwissText 2022. Participants were asked to
build a sentence-level Swiss German speech to Standard German text system
specialized on the Grisons dialect. The objective was to maximize the BLEU
score on a test set of Grisons speech. 3 teams participated, with the
best-performing system achieving a BLEU score of 70.1.","3 pages, 0 figures, to appear in proceedings of SwissText 2022",
Syllable Subword Tokens for Open Vocabulary Speech Recognition in Malayalam,"[arxiv.Result.Author('Kavya Manohar'), arxiv.Result.Author('A. R. Jayan'), arxiv.Result.Author('Rajeev Rajan')]",2023-01-17 07:29:47+00:00,"In a hybrid automatic speech recognition (ASR) system, a pronunciation
lexicon (PL) and a language model (LM) are essential to correctly retrieve
spoken word sequences. Being a morphologically complex language, the vocabulary
of Malayalam is so huge and it is impossible to build a PL and an LM that cover
all diverse word forms. Usage of subword tokens to build PL and LM, and
combining them to form words after decoding, enables the recovery of many out
of vocabulary words. In this work we investigate the impact of using syllables
as subword tokens instead of words in Malayalam ASR, and evaluate the relative
improvement in lexicon size, model memory requirement and word error rate.",,
Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer,"[arxiv.Result.Author('Zhanheng Yang'), arxiv.Result.Author('Sining Sun'), arxiv.Result.Author('Xiong Wang'), arxiv.Result.Author('Yike Zhang'), arxiv.Result.Author('Long Ma'), arxiv.Result.Author('Lei Xie')]",2023-01-17 07:29:26+00:00,"It is difficult for an end-to-end (E2E) ASR system to recognize words such as
named entities appearing infrequently in the training data. A widely used
method to mitigate this issue is feeding contextual information into the
acoustic model. A contextual word list is necessary, which lists all possible
contextual word candidates. Previous works have proven that the size and
quality of the list are crucial. A compact and accurate list can boost the
performance significantly. In this paper, we propose an efficient approach to
obtain a high quality contextual word list for a unified streaming and
non-streaming based Conformer-Transducer (C-T) model. Specifically, we make use
of the phone-level streaming output to first filter the predefined contextual
word list. During the subsequent non-streaming inference, the words in the
filtered list are regarded as contextual information fused into non-casual
encoder and decoder to generate the final recognition results. Our approach can
take advantage of streaming recognition hypothesis, improve the accuracy of the
contextual ASR system and speed up the inference process as well. Experiments
on two datasets demonstrates over 20% relative character error rate reduction
(CERR) comparing to the baseline system. Meanwile, the RTF of our system can be
stabilized within 0.15 when the size of the contextual word list grows over
6,000.",,
Audio2Gestures: Generating Diverse Gestures from Audio,"[arxiv.Result.Author('Jing Li'), arxiv.Result.Author('Di Kang'), arxiv.Result.Author('Wenjie Pei'), arxiv.Result.Author('Xuefei Zhe'), arxiv.Result.Author('Ying Zhang'), arxiv.Result.Author('Linchao Bao'), arxiv.Result.Author('Zhenyu He')]",2023-01-17 04:09:58+00:00,"People may perform diverse gestures affected by various mental and physical
factors when speaking the same sentences. This inherent one-to-many
relationship makes co-speech gesture generation from audio particularly
challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to
predict the average of all possible target motions, easily resulting in
plain/boring motions during inference. So we propose to explicitly model the
one-to-many audio-to-motion mapping by splitting the cross-modal latent code
into shared code and motion-specific code. The shared code is expected to be
responsible for the motion component that is more correlated to the audio while
the motion-specific code is expected to capture diverse motion information that
is more independent of the audio. However, splitting the latent code into two
parts poses extra training difficulties. Several crucial training
losses/strategies, including relaxed motion loss, bicycle constraint, and
diversity loss, are designed to better train the VAE.
  Experiments on both 3D and 2D motion datasets verify that our method
generates more realistic and diverse motions than previous state-of-the-art
methods, quantitatively and qualitatively. Besides, our formulation is
compatible with discrete cosine transformation (DCT) modeling and other popular
backbones (\textit{i.e.} RNN, Transformer). As for motion losses and
quantitative motion evaluation, we find structured losses/metrics
(\textit{e.g.} STFT) that consider temporal and/or spatial context complement
the most commonly used point-wise losses (\textit{e.g.} PCK), resulting in
better motion dynamics and more nuanced motion details. Finally, we demonstrate
that our method can be readily used to generate motion sequences with
user-specified motion clips on the timeline.",arXiv admin note: substantial text overlap with arXiv:2108.06720,
BayesSpeech: A Bayesian Transformer Network for Automatic Speech Recognition,[arxiv.Result.Author('Will Rieger')],2023-01-16 16:19:04+00:00,"Recent developments using End-to-End Deep Learning models have been shown to
have near or better performance than state of the art Recurrent Neural Networks
(RNNs) on Automatic Speech Recognition tasks. These models tend to be lighter
weight and require less training time than traditional RNN-based approaches.
However, these models take frequentist approach to weight training. In theory,
network weights are drawn from a latent, intractable probability distribution.
We introduce BayesSpeech for end-to-end Automatic Speech Recognition.
BayesSpeech is a Bayesian Transformer Network where these intractable
posteriors are learned through variational inference and the local
reparameterization trick without recurrence. We show how the introduction of
variance in the weights leads to faster training time and near state-of-the-art
performance on LibriSpeech-960.",,
CRYPTEXT: Database and Interactive Toolkit of Human-Written Text Perturbations in the Wild,"[arxiv.Result.Author('Thai Le'), arxiv.Result.Author('Ye Yiran'), arxiv.Result.Author('Yifan Hu'), arxiv.Result.Author('Dongwon Lee')]",2023-01-16 16:04:09+00:00,"User-generated textual contents on the Internet are often noisy, erroneous,
and not in correct forms in grammar. In fact, some online users choose to
express their opinions online through carefully perturbed texts, especially in
controversial topics (e.g., politics, vaccine mandate) or abusive contexts
(e.g., cyberbullying, hate-speech). However, to the best of our knowledge,
there is no framework that explores these online ``human-written"" perturbations
(as opposed to algorithm-generated perturbations). Therefore, we introduce an
interactive system called CRYPTEXT. CRYPTEXT is a data-intensive application
that provides the users with a database and several tools to extract and
interact with human-written perturbations. Specifically, CRYPTEXT helps look
up, perturb, and normalize (i.e., de-perturb) texts. CRYPTEXT also provides an
interactive interface to monitor and analyze text perturbations online. A short
demo video is available at: https://youtu.be/8WT3G8xjIoI",Accepted to IEEE International Conference on Data Engineering 2023,
Using Kaldi for Automatic Speech Recognition of Conversational Austrian German,"[arxiv.Result.Author('Julian Linke'), arxiv.Result.Author('Saskia Wepner'), arxiv.Result.Author('Gernot Kubin'), arxiv.Result.Author('Barbara Schuppler')]",2023-01-16 15:28:28+00:00,"As dialogue systems are becoming more and more interactional and social, also
the accurate automatic speech recognition (ASR) of conversational speech is of
increasing importance. This shifts the focus from short, spontaneous,
task-oriented dialogues to the much higher complexity of casual face-to-face
conversations. However, the collection and annotation of such conversations is
a time-consuming process and data is sparse for this specific speaking style.
This paper presents ASR experiments with read and conversational Austrian
German as target. In order to deal with having only limited resources available
for conversational German and, at the same time, with a large variation among
speakers with respect to pronunciation characteristics, we improve a
Kaldi-based ASR system by incorporating a (large) knowledge-based pronunciation
lexicon, while exploring different data-based methods to restrict the number of
pronunciation variants for each lexical entry. We achieve best WER of 0.4% on
Austrian German read speech and best average WER of 48.5% on conversational
speech. We find that by using our best pronunciation lexicon a similarly high
performance can be achieved than by increasing the size of the data used for
the language model by approx. 360% to 760%. Our findings indicate that for
low-resource scenarios -- despite the general trend in speech technology
towards using data-based methods only -- knowledge-based approaches are a
successful, efficient method.","10 pages, 2 figures, 4 tables",
Msanii: High Fidelity Music Synthesis on a Shoestring Budget,[arxiv.Result.Author('Kinyugo Maina')],2023-01-16 15:18:26+00:00,"In this paper, we present Msanii, a novel diffusion-based model for
synthesizing long-context, high-fidelity music efficiently. Our model combines
the expressiveness of mel spectrograms, the generative capabilities of
diffusion models, and the vocoding capabilities of neural vocoders. We
demonstrate the effectiveness of Msanii by synthesizing tens of seconds (190
seconds) of stereo music at high sample rates (44.1 kHz) without the use of
concatenative synthesis, cascading architectures, or compression techniques. To
the best of our knowledge, this is the first work to successfully employ a
diffusion-based model for synthesizing such long music samples at high sample
rates. Our demo can be found https://kinyugo.github.io/msanii-demo and our code
https://github.com/Kinyugo/msanii .","15 pages, 8 figures, for demo see
  https://kinyugo.github.io/msanii-demo/ and for code, see
  https://github.com/Kinyugo/msanii, this paper is a work in progress",
Multi-resolution location-based training for multi-channel continuous speech separation,"[arxiv.Result.Author('Hassan Taherian'), arxiv.Result.Author('DeLiang Wang')]",2023-01-16 15:02:06+00:00,"The performance of automatic speech recognition (ASR) systems severely
degrades when multi-talker speech overlap occurs. In meeting environments,
speech separation is typically performed to improve the robustness of ASR
systems. Recently, location-based training (LBT) was proposed as a new training
criterion for multi-channel talker-independent speaker separation. Assuming
fixed array geometry, LBT outperforms widely-used permutation-invariant
training in fully overlapped utterances and matched reverberant conditions.
This paper extends LBT to conversational multi-channel speaker separation. We
introduce multi-resolution LBT to estimate the complex spectrograms from low to
high time and frequency resolutions. With multi-resolution LBT, convolutional
kernels are assigned consistently based on speaker locations in physical space.
Evaluation results show that multi-resolution LBT consistently outperforms
other competitive methods on the recorded LibriCSS corpus.",Submitted to ICASSP 23,
OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,"[arxiv.Result.Author('Jeongkyun Park'), arxiv.Result.Author('Jung-Wook Hwang'), arxiv.Result.Author('Kwanghee Choi'), arxiv.Result.Author('Seung-Hyun Lee'), arxiv.Result.Author('Jun Hwan Ahn'), arxiv.Result.Author('Rae-Hong Park'), arxiv.Result.Author('Hyung-Min Park')]",2023-01-16 11:40:50+00:00,"Inspired by humans comprehending speech in a multi-modal manner, various
audio-visual datasets have been constructed. However, most existing datasets
focus on English, induce dependencies with various prediction models during
dataset preparation, and have only a small number of multi-view videos. To
mitigate the limitations, we recently developed the Open Large-scale Korean
Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly
available audio-visual speech datasets. The dataset contains 1,150 hours of
transcribed audio from 1,107 Korean speakers in a studio setup with nine
different viewpoints and various noise situations. We also provide the
pre-trained baseline models for two tasks, audio-visual speech recognition and
lip reading. We conducted experiments based on the models to verify the
effectiveness of multi-modal and multi-view training over uni-modal and
frontal-view-only training. We expect the OLKAVS dataset to facilitate
multi-modal research in broader areas such as Korean speech recognition,
speaker recognition, pronunciation level classification, and mouth motion
analysis.",,
Improving Target Speaker Extraction with Sparse LDA-transformed Speaker Embeddings,"[arxiv.Result.Author('Kai Liu'), arxiv.Result.Author('Xucheng Wan'), arxiv.Result.Author('Ziqing Du'), arxiv.Result.Author('Huan Zhou')]",2023-01-16 06:30:48+00:00,"As a practical alternative of speech separation, target speaker extraction
(TSE) aims to extract the speech from the desired speaker using additional
speaker cue extracted from the speaker. Its main challenge lies in how to
properly extract and leverage the speaker cue to benefit the extracted speech
quality. The cue extraction method adopted in majority existing TSE studies is
to directly utilize discriminative speaker embedding, which is extracted from
the pre-trained models for speaker verification. Although the high speaker
discriminability is a most desirable property for speaker verification task, we
argue that it may be too sophisticated for TSE. In this study, we propose that
a simplified speaker cue with clear class separability might be preferred for
TSE. To verify our proposal, we introduce several forms of speaker cues,
including naive speaker embedding (such as, x-vector and xi-vector) and new
speaker embeddings produced from sparse LDA-transform. Corresponding TSE models
are built by integrating these speaker cues with SepFormer (one SOTA speech
separation model). Performances of these TSE models are examined on the
benchmark WSJ0-2mix dataset. Experimental results validate the effectiveness
and generalizability of our proposal, showing up to 9.9% relative improvement
in SI-SDRi. Moreover, with SI-SDRi of 19.4 dB and PESQ of 3.78, our best TSE
system significantly outperforms the current SOTA systems and offers the top
TSE results reported till date on the WSJ0-2mix.",ACCEPTED by NCMMSC 2022,
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models,"[arxiv.Result.Author('Zhiqiu Lin'), arxiv.Result.Author('Samuel Yu'), arxiv.Result.Author('Zhiyi Kuang'), arxiv.Result.Author('Deepak Pathak'), arxiv.Result.Author('Deva Ramanan')]",2023-01-16 05:40:42+00:00,"The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.",Project website: https://linzhiqiu.github.io/papers/cross_modal/,
What artificial intelligence might teach us about the origin of human language,[arxiv.Result.Author('Alexander Kilpatrick')],2023-01-15 23:25:29+00:00,"This study explores an interesting pattern emerging from research that
combines artificial intelligence with sound symbolism. In these studies,
supervised machine learning algorithms are trained to classify samples based on
the sounds of referent names. Machine learning algorithms are efficient
learners of sound symbolism, but they tend to bias one category over the other.
The pattern is this: when a category arguably represents greater threat, the
algorithms tend to overpredict to that category. A hypothesis, framed by error
management theory, is presented that proposes that this may be evidence of an
adaptation to preference cautious behaviour. This hypothesis is tested by
constructing extreme gradient boosted (XGBoost) models using the sounds that
make up the names of Chinese, Japanese and Korean Pokemon and observing
classification error distribution.",ICPHS2023 Conference Submission. 5 pages,
Training one model to detect heart and lung sound events from single point auscultations,"[arxiv.Result.Author('Leander Melms'), arxiv.Result.Author('Robert R. Ilesan'), arxiv.Result.Author('Ulrich Köhler'), arxiv.Result.Author('Olaf Hildebrandt'), arxiv.Result.Author('Regina Conradt'), arxiv.Result.Author('Jens Eckstein'), arxiv.Result.Author('Cihan Atila'), arxiv.Result.Author('Sami Matrood'), arxiv.Result.Author('Bernhard Schieffer'), arxiv.Result.Author('Jürgen R. Schaefer'), arxiv.Result.Author('Tobias Müller'), arxiv.Result.Author('Julius Obergassel'), arxiv.Result.Author('Nadine Schlicker'), arxiv.Result.Author('Martin C. Hirsch')]",2023-01-15 12:13:03+00:00,"Objective: This work proposes a semi-supervised training approach for
detecting lung and heart sounds simultaneously with only one trained model and
in invariance to the auscultation point. Methods: We use open-access data from
the 2016 Physionet/CinC Challenge, the 2022 George Moody Challenge, and from
the lung sound database HF_V1. We first train specialist single-task models
using foreground ground truth (GT) labels from different auscultation databases
to identify background sound events in the respective lung and heart
auscultation databases. The pseudo-labels generated in this way were combined
with the ground truth labels in a new training iteration, such that a new model
was subsequently trained to detect foreground and background signals. Benchmark
tests ensured that the newly trained model could detect both, lung, and heart
sound events in different auscultation sites without regressing on the original
task. We also established hand-validated labels for the respective background
signal in heart and lung sound auscultations to evaluate the models. Results:
In this work, we report for the first time results for i) a multi-class
prediction for lung sound events and ii) for simultaneous detection of heart
and lung sound events and achieve competitive results using only one model. The
combined multi-task model regressed slightly in heart sound detection and
gained significantly in lung sound detection accuracy with an overall macro F1
score of 39.2% over six classes, representing a 6.7% improvement over the
single-task baseline models. Conclusion/Significance: To the best of our
knowledge, this is the first approach developed to date for measuring heart and
lung sound events invariant to both, the auscultation site and capturing
device. Hence, our model is capable of performing lung and heart sound
detection from any auscultation location.","14 pages, 8 figures",
Learning Audio-Driven Viseme Dynamics for 3D Face Animation,"[arxiv.Result.Author('Linchao Bao'), arxiv.Result.Author('Haoxian Zhang'), arxiv.Result.Author('Yue Qian'), arxiv.Result.Author('Tangli Xue'), arxiv.Result.Author('Changhai Chen'), arxiv.Result.Author('Xuefei Zhe'), arxiv.Result.Author('Di Kang')]",2023-01-15 09:55:46+00:00,"We present a novel audio-driven facial animation approach that can generate
realistic lip-synchronized 3D facial animations from the input audio. Our
approach learns viseme dynamics from speech videos, produces animator-friendly
viseme curves, and supports multilingual speech inputs. The core of our
approach is a novel parametric viseme fitting algorithm that utilizes phoneme
priors to extract viseme parameters from speech videos. With the guidance of
phonemes, the extracted viseme curves can better correlate with phonemes, thus
more controllable and friendly to animators. To support multilingual speech
inputs and generalizability to unseen voices, we take advantage of deep audio
feature models pretrained on multiple languages to learn the mapping from audio
to viseme curves. Our audio-to-curves mapping achieves state-of-the-art
performance even when the input audio suffers from distortions of volume,
pitch, speed, or noise. Lastly, a viseme scanning approach for acquiring
high-fidelity viseme assets is presented for efficient speech animation
production. We show that the predicted viseme curves can be applied to
different viseme-rigged characters to yield various personalized animations
with realistic and natural facial motions. Our approach is artist-friendly and
can be easily integrated into typical animation production workflows including
blendshape or bone based animation.",Project page: https://linchaobao.github.io/viseme2023/,
Rationalizing Predictions by Adversarial Information Calibration,"[arxiv.Result.Author('Lei Sha'), arxiv.Result.Author('Oana-Maria Camburu'), arxiv.Result.Author('Thomas Lukasiewicz')]",2023-01-15 03:13:09+00:00,"Explaining the predictions of AI models is paramount in safety-critical
applications, such as in legal or medical domains. One form of explanation for
a prediction is an extractive rationale, i.e., a subset of features of an
instance that lead the model to give its prediction on that instance. For
example, the subphrase ``he stole the mobile phone'' can be an extractive
rationale for the prediction of ``Theft''. Previous works on generating
extractive rationales usually employ a two-phase model: a selector that selects
the most important features (i.e., the rationale) followed by a predictor that
makes the prediction based exclusively on the selected features. One
disadvantage of these works is that the main signal for learning to select
features comes from the comparison of the answers given by the predictor to the
ground-truth answers. In this work, we propose to squeeze more information from
the predictor via an information calibration method. More precisely, we train
two models jointly: one is a typical neural model that solves the task at hand
in an accurate but black-box manner, and the other is a selector-predictor
model that additionally produces a rationale for its prediction. The first
model is used as a guide for the second model. We use an adversarial technique
to calibrate the information extracted by the two models such that the
difference between them is an indicator of the missed or over-selected
features. In addition, for natural language tasks, we propose a
language-model-based regularizer to encourage the extraction of fluent
rationales. Experimental results on a sentiment analysis task, a hate speech
recognition task as well as on three tasks from the legal domain show the
effectiveness of our approach to rationale extraction.",arXiv admin note: substantial text overlap with arXiv:2012.08884,"Artificial Intelligence, Volume 315, February 2023"
An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores,"[arxiv.Result.Author('Xin Jin'), arxiv.Result.Author('Wu Zhou'), arxiv.Result.Author('Jinyu Wang'), arxiv.Result.Author('Duo Xu'), arxiv.Result.Author('Yiqing Rong'), arxiv.Result.Author('Shuai Cui')]",2023-01-14 12:30:16+00:00,"Computational aesthetics evaluation has made great achievements in the field
of visual arts, but the research work on music still needs to be explored.
Although the existing work of music generation is very substantial, the quality
of music score generated by AI is relatively poor compared with that created by
human composers. The music scores created by AI are usually monotonous and
devoid of emotion. Based on Birkhoff's aesthetic measure, this paper proposes
an objective quantitative evaluation method for homophony music score aesthetic
quality assessment. The main contributions of our work are as follows: first,
we put forward a homophony music score aesthetic model to objectively evaluate
the quality of music score as a baseline model; second, we put forward eight
basic music features and four music aesthetic features.",,
Acoustic correlates of the syllabic rhythm of speech: Modulation spectrum or local features of the temporal envelope,"[arxiv.Result.Author('Yuran Zhang'), arxiv.Result.Author('Jiajie Zou'), arxiv.Result.Author('Nai Ding')]",2023-01-14 11:32:52+00:00,"The syllable is a perceptually salient unit in speech. Since both the
syllable and its acoustic correlate, i.e., the speech envelope, have a
preferred range of rhythmicity between 4 and 8 Hz, it is hypothesized that
theta-band neural oscillations play a major role in extracting syllables based
on the envelope. A literature survey, however, reveals inconsistent evidence
about the relationship between speech envelope and syllables, and the current
study revisits this question by analyzing large speech corpora. It is shown
that the center frequency of speech envelope, characterized by the modulation
spectrum, reliably correlates with the rate of syllables only when the analysis
is pooled over minutes of speech recordings. In contrast, in the time domain, a
component of the speech envelope is reliably phase-locked to syllable onsets.
Based on a speaker-independent model, the timing of syllable onsets explains
about 24% variance of the speech envelope. These results indicate that local
features in the speech envelope, instead of the modulation spectrum, are a more
reliable acoustic correlate of syllables.",,
Modulation spectral features for speech emotion recognition using deep neural networks,"[arxiv.Result.Author('Premjeet Singh'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Goutam Saha')]",2023-01-14 09:36:49+00:00,"This work explores the use of constant-Q transform based modulation spectral
features (CQT-MSF) for speech emotion recognition (SER). The human perception
and analysis of sound comprise of two important cognitive parts: early auditory
analysis and cortex-based processing. The early auditory analysis considers
spectrogram-based representation whereas cortex-based analysis includes
extraction of temporal modulations from the spectrogram. This temporal
modulation representation of spectrogram is called modulation spectral feature
(MSF). As the constant-Q transform (CQT) provides higher resolution at emotion
salient low-frequency regions of speech, we find that CQT-based spectrogram,
together with its temporal modulations, provides a representation enriched with
emotion-specific information. We argue that CQT-MSF when used with a
2-dimensional convolutional network can provide a time-shift invariant and
deformation insensitive representation for SER. Our results show that CQT-MSF
outperforms standard mel-scale based spectrogram and its modulation features on
two popular SER databases, Berlin EmoDB and RAVDESS. We also show that our
proposed feature outperforms the shift and deformation invariant scattering
transform coefficients, hence, showing the importance of joint hand-crafted and
self-learned feature extraction instead of reliance on complete hand-crafted
features. Finally, we perform Grad-CAM analysis to visually inspect the
contribution of constant-Q modulation features over SER.",Accepted for publication in Elsevier's Speech Communication Journal,"Volume 146, January 2023, Pages 53-69"
Music Playlist Title Generation Using Artist Information,"[arxiv.Result.Author('Haven Kim'), arxiv.Result.Author('SeungHeon Doh'), arxiv.Result.Author('Junwon Lee'), arxiv.Result.Author('Juhan Nam')]",2023-01-14 00:19:39+00:00,"Automatically generating or captioning music playlist titles given a set of
tracks is of significant interest in music streaming services as customized
playlists are widely used in personalized music recommendation, and
well-composed text titles attract users and help their music discovery. We
present an encoder-decoder model that generates a playlist title from a
sequence of music tracks. While previous work takes track IDs as tokenized
input for playlist title generation, we use artist IDs corresponding to the
tracks to mitigate the issue from the long-tail distribution of tracks included
in the playlist dataset. Also, we introduce a chronological data split method
to deal with newly-released tracks in real-world scenarios. Comparing the track
IDs and artist IDs as input sequences, we show that the artist-based approach
significantly enhances the performance in terms of word overlap, semantic
relevance, and diversity.",AAAI-23 Workshop on Creative AI Across Modalities,
From stage to page: language independent bootstrap measures of distinctiveness in fictional speech,"[arxiv.Result.Author('Artjoms Šeļa'), arxiv.Result.Author('Ben Nagy'), arxiv.Result.Author('Joanna Byszuk'), arxiv.Result.Author('Laura Hernández-Lorenzo'), arxiv.Result.Author('Botond Szemes'), arxiv.Result.Author('Maciej Eder')]",2023-01-13 16:58:43+00:00,"Stylometry is mostly applied to authorial style. Recently, researchers have
begun investigating the style of characters, finding that the variation remains
within authorial bounds. We address the stylistic distinctiveness of characters
in drama. Our primary contribution is methodological; we introduce and evaluate
two non-parametric methods to produce a summary statistic for character
distinctiveness that can be usefully applied and compared across languages and
times. Our first method is based on bootstrap distances between 3-gram
probability distributions, the second (reminiscent of 'unmasking' techniques)
on word keyness curves. Both methods are validated and explored by applying
them to a reasonably large corpus (a subset of DraCor): we analyse 3301
characters drawn from 2324 works, covering five centuries and four languages
(French, German, Russian, and the works of Shakespeare). Both methods appear
useful; the 3-gram method is statistically more powerful but the word keyness
method offers rich interpretability. Both methods are able to capture
phonological differences such as accent or dialect, as well as broad
differences in topic and lexical richness. Based on exploratory analysis, we
find that smaller characters tend to be more distinctive, and that women are
cross-linguistically more distinctive than men, with this latter finding
carefully interrogated using multiple regression. This greater distinctiveness
stems from a historical tendency for female characters to be restricted to an
'internal narrative domain' covering mainly direct discourse and
family/romantic themes. It is hoped that direct, comparable statistical
measures will form a basis for more sophisticated future studies, and advances
in theory.",,
Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge,"[arxiv.Result.Author('Saturnino Luz'), arxiv.Result.Author('Fasih Haider'), arxiv.Result.Author('Davida Fromm'), arxiv.Result.Author('Ioulietta Lazarou'), arxiv.Result.Author('Ioannis Kompatsiaris'), arxiv.Result.Author('Brian MacWhinney')]",2023-01-13 14:09:13+00:00,"This Signal Processing Grand Challenge (SPGC) targets a difficult automatic
prediction problem of societal and medical relevance, namely, the detection of
Alzheimer's Dementia (AD). Participants were invited to employ signal
processing and machine learning methods to create predictive models based on
spontaneous speech data. The Challenge has been designed to assess the extent
to which predictive models built based on speech in one language (English)
generalise to another language (Greek). To the best of our knowledge no work
has investigated acoustic features of the speech signal in multilingual AD
detection. Our baseline system used conventional machine learning algorithms
with Active Data Representation of acoustic features, achieving accuracy of
73.91% on AD detection, and 4.95 root mean squared error on cognitive score
prediction.",ICASSP 2023 SPGC description,
Automated speech- and text-based classification of neuropsychiatric conditions in a multidiagnostic setting,"[arxiv.Result.Author('Lasse Hansen'), arxiv.Result.Author('Roberta Rocca'), arxiv.Result.Author('Arndis Simonsen'), arxiv.Result.Author('Alberto Parola'), arxiv.Result.Author('Vibeke Bliksted'), arxiv.Result.Author('Nicolai Ladegaard'), arxiv.Result.Author('Dan Bang'), arxiv.Result.Author('Kristian Tylén'), arxiv.Result.Author('Ethan Weed'), arxiv.Result.Author('Søren Dinesen Østergaard'), arxiv.Result.Author('Riccardo Fusaroli')]",2023-01-13 08:24:21+00:00,"Speech patterns have been identified as potential diagnostic markers for
neuropsychiatric conditions. However, most studies only compare a single
clinical group to healthy controls, whereas clinical practice often requires
differentiating between multiple potential diagnoses (multiclass settings). To
address this, we assembled a dataset of repeated recordings from 420
participants (67 with major depressive disorder, 106 with schizophrenia and 46
with autism, as well as matched controls), and tested the performance of a
range of conventional machine learning models and advanced Transformer models
on both binary and multiclass classification, based on voice and text features.
  While binary models performed comparably to previous research (F1 scores
between 0.54-0.75 for autism spectrum disorder, ASD; 0.67-0.92 for major
depressive disorder, MDD; and 0.71-0.83 for schizophrenia); when
differentiating between multiple diagnostic groups performance decreased
markedly (F1 scores between 0.35-0.44 for ASD, 0.57-0.75 for MDD, 0.15-0.66 for
schizophrenia, and 0.38-0.52 macro F1). Combining voice and text-based models
yielded increased performance, suggesting that they capture complementary
diagnostic information.
  Our results indicate that models trained on binary classification may learn
to rely on markers of generic differences between clinical and non-clinical
populations, or markers of clinical features that overlap across conditions,
rather than identifying markers specific to individual conditions. We provide
recommendations for future research in the field, suggesting increased focus on
developing larger transdiagnostic datasets that include more fine-grained
clinical features, and that can support the development of models that better
capture the complexity of neuropsychiatric conditions and naturalistic
diagnostic assessment.","24 pages, 5 figures",
A Comprehensive Review of Data-Driven Co-Speech Gesture Generation,"[arxiv.Result.Author('Simbarashe Nyatsanga'), arxiv.Result.Author('Taras Kucherenko'), arxiv.Result.Author('Chaitanya Ahuja'), arxiv.Result.Author('Gustav Eje Henter'), arxiv.Result.Author('Michael Neff')]",2023-01-13 00:20:05+00:00,"Gestures that accompany speech are an essential part of natural and efficient
embodied human communication. The automatic generation of such co-speech
gestures is a long-standing problem in computer animation and is considered an
enabling technology in film, games, virtual social spaces, and for interaction
with social robots. The problem is made challenging by the idiosyncratic and
non-periodic nature of human co-speech gesture motion, and by the great
diversity of communicative functions that gestures encompass. Gesture
generation has seen surging interest recently, owing to the emergence of more
and larger datasets of human gesture motion, combined with strides in
deep-learning-based generative models, that benefit from the growing
availability of data. This review article summarizes co-speech gesture
generation research, with a particular focus on deep generative models. First,
we articulate the theory describing human gesticulation and how it complements
speech. Next, we briefly discuss rule-based and classical statistical gesture
synthesis, before delving into deep learning approaches. We employ the choice
of input modalities as an organizing principle, examining systems that generate
gestures from audio, text, and non-linguistic input. We also chronicle the
evolution of the related training data sets in terms of size, diversity, motion
quality, and collection method. Finally, we identify key research challenges in
gesture generation, including data availability and quality; producing
human-like motion; grounding the gesture in the co-occurring speech in
interaction with other speakers, and in the environment; performing gesture
evaluation; and integration of gesture synthesis into applications. We
highlight recent approaches to tackling the various key challenges, as well as
the limitations of these approaches, and point toward areas of future
development.",,
Rock Guitar Tablature Generation via Natural Language Processing,[arxiv.Result.Author('Josue Casco-Rodriguez')],2023-01-12 21:12:08+00:00,"Deep learning has recently empowered and democratized generative modeling of
images and text, with additional concurrent works exploring the possibility of
generating more complex forms of data, such as audio. However, the high
dimensionality, long-range dependencies, and lack of standardized datasets
currently makes generative modeling of audio and music very challenging. We
propose to model music as a series of discrete notes upon which we can use
autoregressive natural language processing techniques for successful generative
modeling. While previous works used similar pipelines on data such as sheet
music and MIDI, we aim to extend such approaches to the under-studied medium of
guitar tablature. Specifically, we develop the first work to our knowledge that
models one specific genre as guitar tablature: heavy rock. Unlike other works
in guitar tablature generation, we have a freely available public demo at
https://huggingface.co/spaces/josuelmet/Metal_Music_Interpolator",,
A Dataset of Kurdish (Sorani) Named Entities -- An Amendment to Kurdish-BLARK Named Entities,"[arxiv.Result.Author('Sazan Salar'), arxiv.Result.Author('Hossein Hassani')]",2023-01-12 12:13:44+00:00,"Named Entity Recognition (NER) is one of the essential applications of
Natural Language Processing (NLP). It is also an instrument that plays a
significant role in many other NLP applications, such as Machine Translation
(MT), Information Retrieval (IR), and Part of Speech Tagging (POST). Kurdish is
an under-resourced language from the NLP perspective. Particularly, in all the
categories, the lack of NER resources hinders other aspects of Kurdish
processing. In this work, we present a data set that covers several categories
of NEs in Kurdish (Sorani). The dataset is a significant amendment to a
previously developed dataset in the Kurdish BLARK (Basic Language Resource
Kit). It covers 11 categories and 33261 entries in total. The dataset is
publicly available for non-commercial use under CC BY-NC-SA 4.0 license at
https://kurdishblark.github.io/.","The dataset is available at
  https://github.com/KurdishBLARK/KurdishNamedEntities",
Improving mesh-based motion compensation by using edge adaptive graph-based compensated wavelet lifting for medical data sets,"[arxiv.Result.Author('Daniela Lanz'), arxiv.Result.Author('André Kaup')]",2023-01-12 06:45:25+00:00,"Medical applications like Computed Tomography (CT) or Magnetic Resonance
Tomography (MRT) often require an efficient scalable representation of their
huge output volumes in the further processing chain of medical routine. A
downscaled version of such a signal can be obtained by using image and video
coders based on wavelet transforms. The visual quality of the resulting lowpass
band, which shall be used as a representative, can be improved by applying
motion compensation methods during the transform. This paper presents a new
approach of using the distorted edge lengths of a mesh-based compensated grid
instead of the approximated intensity values of the underlying frame to perform
a motion compensation. We will show that an edge adaptive graph-based
compensation and its usage for compensated wavelet lifting improves the visual
quality of the lowpass band by approximately 2.5 dB compared to the traditional
mesh-based compensation, while the additional filesize required for coding the
motion information doesn't change.",,"IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), New Orleans, LA, USA, 2017, pp. 1507-1511"
LiteLSTM Architecture Based on Weights Sharing for Recurrent Neural Networks,"[arxiv.Result.Author('Nelly Elsayed'), arxiv.Result.Author('Zag ElSayed'), arxiv.Result.Author('Anthony S. Maida')]",2023-01-12 03:39:59+00:00,"Long short-term memory (LSTM) is one of the robust recurrent neural network
architectures for learning sequential data. However, it requires considerable
computational power to learn and implement both software and hardware aspects.
This paper proposed a novel LiteLSTM architecture based on reducing the LSTM
computation components via the weights sharing concept to reduce the overall
architecture computation cost and maintain the architecture performance. The
proposed LiteLSTM can be significant for processing large data where
time-consuming is crucial while hardware resources are limited, such as the
security of IoT devices and medical data processing. The proposed model was
evaluated and tested empirically on three different datasets from the computer
vision, cybersecurity, speech emotion recognition domains. The proposed
LiteLSTM has comparable accuracy to the other state-of-the-art recurrent
architecture while using a smaller computation budget.","Under the second reviewing round in the SN Computer Science Journal.
  Extended version of the LiteLSTM Architecture for Deep Recurrent Neural
  Networks paper that have been published in the IEEE ISCAS 2022 conference.
  arXiv admin note: substantial text overlap with arXiv:2201.11624",
Modelling low-resource accents without accent-specific TTS frontend,"[arxiv.Result.Author('Georgi Tinchev'), arxiv.Result.Author('Marta Czarnowska'), arxiv.Result.Author('Kamil Deja'), arxiv.Result.Author('Kayoko Yanagisawa'), arxiv.Result.Author('Marius Cotescu')]",2023-01-11 18:00:29+00:00,"This work focuses on modelling a speaker's accent that does not have a
dedicated text-to-speech (TTS) frontend, including a grapheme-to-phoneme (G2P)
module. Prior work on modelling accents assumes a phonetic transcription is
available for the target accent, which might not be the case for low-resource,
regional accents. In our work, we propose an approach whereby we first augment
the target accent data to sound like the donor voice via voice conversion, then
train a multi-speaker multi-accent TTS model on the combination of recordings
and synthetic data, to generate the donor's voice speaking in the target
accent. Throughout the procedure, we use a TTS frontend developed for the same
language but a different accent. We show qualitative and quantitative analysis
where the proposed strategy achieves state-of-the-art results compared to other
generative models. Our work demonstrates that low resource accents can be
modelled with relatively little data and without developing an accent-specific
TTS frontend. Audio samples of our model converting to multiple accents are
available on our web page.","The first two authors contributed equally to this work. In Review.
  Samples available on https://bit.ly/3V52ZrF",
Improving And Analyzing Neural Speaker Embeddings for ASR,"[arxiv.Result.Author('Christoph Lüscher'), arxiv.Result.Author('Jingjing Xu'), arxiv.Result.Author('Mohammad Zeineldeen'), arxiv.Result.Author('Ralf Schlüter'), arxiv.Result.Author('Hermann Ney')]",2023-01-11 16:56:03+00:00,"Neural speaker embeddings encode the speaker's speech characteristics through
a DNN model and are prevalent for speaker verification tasks. However, few
studies have investigated the usage of neural speaker embeddings for an ASR
system. In this work, we present our efforts w.r.t integrating neural speaker
embeddings into a conformer based hybrid HMM ASR system. For ASR, our improved
embedding extraction pipeline in combination with the Weighted-Simple-Add
integration method results in x-vector and c-vector reaching on par performance
with i-vectors. We further compare and analyze different speaker embeddings. We
present our acoustic model improvements obtained by switching from newbob
learning rate schedule to one cycle learning schedule resulting in a ~3%
relative WER reduction on Switchboard, additionally reducing the overall
training time by 17%. By further adding neural speaker embeddings, we gain
additional ~3% relative WER improvement on Hub5'00. Our best Conformer-based
hybrid ASR system with speaker embeddings achieves 9.0% WER on Hub5'00 and
Hub5'01 with training on SWB 300h.",Submitted to ICASSP 2023,
Topological data analysis hearing the shapes of drums and bells,[arxiv.Result.Author('Guo-Wei Wei')],2023-01-11 15:40:53+00:00,"Mark Kac asked a famous question in 1966 entitled Can one hear the shape of a
drum?, a spectral geometry problem that has intrigued mathematicians for the
last six decades and is important to many other fields, such as architectural
acoustics, audio forensics, pattern recognition, radiology, and imaging
science. A related question is how to hear the shape of a drum. We show that
the answer was given in the set of 65 Zenghouyi chime bells dated back to
475-433 B.C. in China. The set of chime bells gradually varies their sizes and
weights to enable melodies, intervals, and temperaments. The same design
principle was used in many other musical instruments, such as xylophones, pan
flutes, pianos, etc. We reveal that there is a fascinating connection between
the progression pattern of many musical instruments and filtration (or spectral
sequence) in topological data analysis (TDA). We argue that filtration-induced
evolutionary de Rham-Hodge theory provides a new mathematical foundation for
musical instruments. Its discrete counterpart, persistent Laplacians and many
other persistent topological Laplacians, including persistent sheaf Laplacians
and persistent path Laplacians are briefly discussed.","4 pages, 2 figures",
WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning,"[arxiv.Result.Author('Kejun Zhang'), arxiv.Result.Author('Xinda Wu'), arxiv.Result.Author('Tieyao Zhang'), arxiv.Result.Author('Zhijie Huang'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Qihao Liang'), arxiv.Result.Author('Songruoyao Wu'), arxiv.Result.Author('Lingyun Sun')]",2023-01-11 14:33:42+00:00,"Although deep learning has revolutionized music generation, existing methods
for structured melody generation follow an end-to-end left-to-right
note-by-note generative paradigm and treat each note equally. Here, we present
WuYun, a knowledge-enhanced deep learning architecture for improving the
structure of generated melodies, which first generates the most structurally
important notes to construct a melodic skeleton and subsequently infills it
with dynamically decorative notes into a full-fledged melody. Specifically, we
use music domain knowledge to extract melodic skeletons and employ sequence
learning to reconstruct them, which serve as additional knowledge to provide
auxiliary guidance for the melody generation process. We demonstrate that WuYun
can generate melodies with better long-term structure and musicality and
outperforms other state-of-the-art methods by 0.51 on average on all subjective
evaluation metrics. Our study provides a multidisciplinary lens to design
melodic hierarchical structures and bridge the gap between data-driven and
knowledge-based approaches for numerous music generation tasks.",,
Perceive and predict: self-supervised speech representation based loss functions for speech enhancement,"[arxiv.Result.Author('George Close'), arxiv.Result.Author('William Ravenscroft'), arxiv.Result.Author('Thomas Hain'), arxiv.Result.Author('Stefan Goetze')]",2023-01-11 10:20:56+00:00,"Recent work in the domain of speech enhancement has explored the use of
self-supervised speech representations to aid in the training of neural speech
enhancement models. However, much of this work focuses on using the deepest or
final outputs of self supervised speech representation models, rather than the
earlier feature encodings. The use of self supervised representations in such a
way is often not fully motivated. In this work it is shown that the distance
between the feature encodings of clean and noisy speech correlate strongly with
psychoacoustically motivated measures of speech quality and intelligibility, as
well as with human Mean Opinion Score (MOS) ratings. Experiments using this
distance as a loss function are performed and improved performance over the use
of STFT spectrogram distance based loss as well as other common loss functions
from speech enhancement literature is demonstrated using objective measures
such as perceptual evaluation of speech quality (PESQ) and short-time objective
intelligibility (STOI).","4 pages, submitted to ICASSP 2023",
Dual Learning for Large Vocabulary On-Device ASR,"[arxiv.Result.Author('Cal Peyser'), arxiv.Result.Author('Ronny Huang'), arxiv.Result.Author('Tara Sainath'), arxiv.Result.Author('Rohit Prabhavalkar'), arxiv.Result.Author('Michael Picheny'), arxiv.Result.Author('Kyunghyun Cho')]",2023-01-11 06:32:28+00:00,"Dual learning is a paradigm for semi-supervised machine learning that seeks
to leverage unsupervised data by solving two opposite tasks at once. In this
scheme, each model is used to generate pseudo-labels for unlabeled examples
that are used to train the other model. Dual learning has seen some use in
speech processing by pairing ASR and TTS as dual tasks. However, these results
mostly address only the case of using unpaired examples to compensate for very
small supervised datasets, and mostly on large, non-streaming models. Dual
learning has not yet been proven effective for using unsupervised data to
improve realistic on-device streaming models that are already trained on large
supervised corpora. We provide this missing piece though an analysis of an
on-device-sized streaming conformer trained on the entirety of Librispeech,
showing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%
with an LM.",,
Rethinking complex-valued deep neural networks for monaural speech enhancement,"[arxiv.Result.Author('Haibin Wu'), arxiv.Result.Author('Ke Tan'), arxiv.Result.Author('Buye Xu'), arxiv.Result.Author('Anurag Kumar'), arxiv.Result.Author('Daniel Wong')]",2023-01-11 05:59:50+00:00,"Despite multiple efforts made towards adopting complex-valued deep neural
networks (DNNs), it remains an open question whether complex-valued DNNs are
generally more effective than real-valued DNNs for monaural speech enhancement.
This work is devoted to presenting a critical assessment by systematically
examining complex-valued DNNs against their real-valued counterparts.
Specifically, we investigate complex-valued DNN atomic units, including linear
layers, convolutional layers, long short-term memory (LSTM), and gated linear
units. By comparing complex- and real-valued versions of fundamental building
blocks in the recently developed gated convolutional recurrent network (GCRN),
we show how different mechanisms for basic blocks affect the performance. We
also find that the use of complex-valued operations hinders the model capacity
when the model size is small. In addition, we examine two recent complex-valued
DNNs, i.e. deep complex convolutional recurrent network (DCCRN) and deep
complex U-Net (DCUNET). Evaluation results show that both DNNs produce
identical performance to their real-valued counterparts while requiring much
more computation. Based on these comprehensive comparisons, we conclude that
complex-valued DNNs do not provide a performance gain over their real-valued
counterparts for monaural speech enhancement, and thus are less desirable due
to their higher computational costs.",,
Predicting Hateful Discussions on Reddit using Graph Transformer Networks and Communal Context,"[arxiv.Result.Author('Liam Hebert'), arxiv.Result.Author('Lukasz Golab'), arxiv.Result.Author('Robin Cohen')]",2023-01-10 23:47:13+00:00,"We propose a system to predict harmful discussions on social media platforms.
Our solution uses contextual deep language models and proposes the novel idea
of integrating state-of-the-art Graph Transformer Networks to analyze all
conversations that follow an initial post. This framework also supports
adapting to future comments as the conversation unfolds. In addition, we study
whether a community-specific analysis of hate speech leads to more effective
detection of hateful discussions. We evaluate our approach on 333,487 Reddit
discussions from various communities. We find that community-specific modeling
improves performance two-fold and that models which capture wider-discussion
context improve accuracy by 28\% (35\% for the most hateful content) compared
to limited context models.",Accepted and Presented at WI-IAT 22,
Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,"[arxiv.Result.Author('Dan Bigioi'), arxiv.Result.Author('Shubhajit Basak'), arxiv.Result.Author('Hugh Jordan'), arxiv.Result.Author('Rachel McDonnell'), arxiv.Result.Author('Peter Corcoran')]",2023-01-10 12:01:20+00:00,"In this paper we propose a method for end-to-end speech driven video editing
using a denoising diffusion model. Given a video of a person speaking, we aim
to re-synchronise the lip and jaw motion of the person in response to a
separate auditory speech recording without relying on intermediate structural
representations such as facial landmarks or a 3D face model. We show this is
possible by conditioning a denoising diffusion model with audio spectral
features to generate synchronised facial motion. We achieve convincing results
on the task of unstructured single-speaker video editing, achieving a word
error rate of 45% using an off the shelf lip reading model. We further
demonstrate how our approach can be extended to the multi-speaker domain. To
our knowledge, this is the first work to explore the feasibility of applying
denoising diffusion models to the task of audio-driven video editing.","8 Pages, code and project page available here:
  https://danbigioi.github.io/DiffusionVideoEditing/",
Streaming Punctuation: A Novel Punctuation Technique Leveraging Bidirectional Context for Continuous Speech Recognition,"[arxiv.Result.Author('Piyush Behre'), arxiv.Result.Author('Sharman Tan'), arxiv.Result.Author('Padma Varadharajan'), arxiv.Result.Author('Shuangyu Chang')]",2023-01-10 07:07:20+00:00,"While speech recognition Word Error Rate (WER) has reached human parity for
English, continuous speech recognition scenarios such as voice typing and
meeting transcriptions still suffer from segmentation and punctuation problems,
resulting from irregular pausing patterns or slow speakers. Transformer
sequence tagging models are effective at capturing long bi-directional context,
which is crucial for automatic punctuation. Automatic Speech Recognition (ASR)
production systems, however, are constrained by real-time requirements, making
it hard to incorporate the right context when making punctuation decisions.
Context within the segments produced by ASR decoders can be helpful but
limiting in overall punctuation performance for a continuous speech session. In
this paper, we propose a streaming approach for punctuation or re-punctuation
of ASR output using dynamic decoding windows and measure its impact on
punctuation and segmentation accuracy across scenarios. The new system tackles
over-segmentation issues, improving segmentation F0.5-score by 13.9%. Streaming
punctuation achieves an average BLEUscore improvement of 0.66 for the
downstream task of Machine Translation (MT).",arXiv admin note: substantial text overlap with arXiv:2210.05756,"International Journal on Natural Language Computing (IJNLC) 11
  (6), 2022, 13"
UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion,"[arxiv.Result.Author('Haogeng Liu'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Ruibo Fu'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Zhengqi Wen'), arxiv.Result.Author('Jianhua Tao')]",2023-01-10 06:06:57+00:00,"Text-to-speech (TTS) and voice conversion (VC) are two different tasks both
aiming at generating high quality speaking voice according to different input
modality. Due to their similarity, this paper proposes UnifySpeech, which
brings TTS and VC into a unified framework for the first time. The model is
based on the assumption that speech can be decoupled into three independent
components: content information, speaker information, prosody information. Both
TTS and VC can be regarded as mining these three parts of information from the
input and completing the reconstruction of speech. For TTS, the speech content
information is derived from the text, while in VC it's derived from the source
speech, so all the remaining units are shared except for the speech content
extraction module in the two tasks. We applied vector quantization and domain
constrain to bridge the gap between the content domains of TTS and VC.
Objective and subjective evaluation shows that by combining the two task, TTS
obtains better speaker modeling ability while VC gets hold of impressive speech
content decoupling capability.",,
Generative Emotional AI for Speech Emotion Recognition: The Case for Synthetic Emotional Speech Augmentation,"[arxiv.Result.Author('Abdullah Shahid'), arxiv.Result.Author('Siddique Latif'), arxiv.Result.Author('Junaid Qadir')]",2023-01-10 02:03:26+00:00,"Despite advances in deep learning, current state-of-the-art speech emotion
recognition (SER) systems still have poor performance due to a lack of speech
emotion datasets. This paper proposes augmenting SER systems with synthetic
emotional speech generated by an end-to-end text-to-speech (TTS) system based
on an extended Tacotron architecture. The proposed TTS system includes encoders
for speaker and emotion embeddings, a sequence-to-sequence text generator for
creating Mel-spectrograms, and a WaveRNN to generate audio from the
Mel-spectrograms. Extensive experiments show that the quality of the generated
emotional speech can significantly improve SER performance on multiple
datasets, as demonstrated by a higher mean opinion score (MOS) compared to the
baseline. The generated samples were also effective at augmenting SER
performance.",Under review,
Scaling Laws for Generative Mixed-Modal Language Models,"[arxiv.Result.Author('Armen Aghajanyan'), arxiv.Result.Author('Lili Yu'), arxiv.Result.Author('Alexis Conneau'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Karen Hambardzumyan'), arxiv.Result.Author('Susan Zhang'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Omer Levy'), arxiv.Result.Author('Luke Zettlemoyer')]",2023-01-10 00:20:06+00:00,"Generative language models define distributions over sequences of tokens that
can represent essentially any combination of data modalities (e.g., any
permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens
for language or code, and so on). To better understand the scaling properties
of such mixed-modal models, we conducted over 250 experiments using seven
different modalities and model sizes ranging from 8 million to 30 billion,
trained on 5-100 billion tokens. We report new mixed-modal scaling laws that
unify the contributions of individual modalities and the interactions between
them. Specifically, we explicitly model the optimal synergy and competition due
to data and model size as an additive term to previous uni-modal scaling laws.
We also find four empirical phenomena observed during the training, such as
emergent coordinate-ascent style training that naturally alternates between
modalities, guidelines for selecting critical hyper-parameters, and connections
between mixed-modal competition and training stability. Finally, we test our
scaling law by training a 30B speech-text model, which significantly
outperforms the corresponding unimodal models. Overall, our research provides
valuable insights into the design and training of mixed-modal generative
models, an important new class of unified models that have unique
distributional properties.",,
Latent Autoregressive Source Separation,"[arxiv.Result.Author('Emilian Postolache'), arxiv.Result.Author('Giorgio Mariani'), arxiv.Result.Author('Michele Mancusi'), arxiv.Result.Author('Andrea Santilli'), arxiv.Result.Author('Luca Cosmo'), arxiv.Result.Author('Emanuele Rodolà')]",2023-01-09 17:32:00+00:00,"Autoregressive models have achieved impressive results over a wide range of
domains in terms of generation quality and downstream task performance. In the
continuous domain, a key factor behind this success is the usage of quantized
latent spaces (e.g., obtained via VQ-VAE autoencoders), which allow for
dimensionality reduction and faster inference times. However, using existing
pre-trained models to perform new non-trivial tasks is difficult since it
requires additional fine-tuning or extensive training to elicit prompting. This
paper introduces LASS as a way to perform vector-quantized Latent
Autoregressive Source Separation (i.e., de-mixing an input signal into its
constituent sources) without requiring additional gradient-based optimization
or modifications of existing models. Our separation method relies on the
Bayesian formulation in which the autoregressive models are the priors, and a
discrete (non-parametric) likelihood function is constructed by performing
frequency counts over latent sums of addend tokens. We test our method on
images and audio with several sampling strategies (e.g., ancestral, beam
search) showing competitive results with existing approaches in terms of
separation quality while offering at the same time significant speedups in
terms of inference time and scalability to higher dimensional data.",Accepted to AAAI 2023,
FullStop:Punctuation and Segmentation Prediction for Dutch with Transformers,"[arxiv.Result.Author('Vincent Vandeghinste'), arxiv.Result.Author('Oliver Guhr')]",2023-01-09 13:12:05+00:00,"When applying automated speech recognition (ASR) for Belgian Dutch (Van Dyck
et al. 2021), the output consists of an unsegmented stream of words, without
any punctuation. A next step is to perform segmentation and insert punctuation,
making the ASR output more readable and easy to manually correct. As far as we
know there is no publicly available punctuation insertion system for Dutch that
functions at a usable level. The model we present here is an extension of the
models of Guhr et al. (2021) for Dutch and is made publicly available. We
trained a sequence classification model, based on the Dutch language model
RobBERT (Delobelle et al. 2020). For every word in the input sequence, the
models predicts a punctuation marker that follows the word. We have also
extended a multilingual model, for cases where the language is unknown or where
code switching applies. When performing the task of segmentation, the
application of the best models onto out of domain test data, a sliding window
of 200 words of the ASR output stream is sent to the classifier, and
segmentation is applied when the system predicts a segmenting punctuation sign
with a ratio above threshold. Results show to be much better than a machine
translation baseline approach.",18 pages,
Self-supervised enhancement of stimulus-evoked brain response data,"[arxiv.Result.Author('Bernd Accou'), arxiv.Result.Author('Hugo Van hamme'), arxiv.Result.Author('Tom Francart')]",2023-01-09 13:09:51+00:00,"Stimulus-evoked brain response data has a notoriously low signal-to-noise
ratio (SNR) and high inter-subject variability. Multiple techniques have been
proposed to alleviate this problem, such as averaging, denoising source
separation (DSS) and (multiway) canonical correlation analysis ((M)CCA), but
all these methods have significant limitations. We propose a novel paradigm for
the self-supervised enhancement of stimulus-related brain response data.
Different time-aligned stimulus-evoked brain responses to the same stimulus are
randomly shifted in time and independently enhanced. Both enhanced brain
responses are compared using a model that predicts the shift in time between
the brain responses. Using a model based on a multi-view convolutional neural
network as an enhancement module, we show the efficacy of our method for a
downstream task of decoding the speech envelope from auditory EEG. A
significant relative improvement of 32% (p<0.001) was found when using the
enhanced EEG versus normal EEG. While the shown example concerns EEG in
response to auditory stimulation, conceptually, our method applies to other
modalities (such as MEG) and other tasks (such as visual stimulus-response
modelling).",Submission to ICASSP 2023,
MAQA: A Multimodal QA Benchmark for Negation,"[arxiv.Result.Author('Judith Yue Li'), arxiv.Result.Author('Aren Jansen'), arxiv.Result.Author('Qingqing Huang'), arxiv.Result.Author('Joonseok Lee'), arxiv.Result.Author('Ravi Ganti'), arxiv.Result.Author('Dima Kuzmin')]",2023-01-09 10:11:23+00:00,"Multimodal learning can benefit from the representation power of pretrained
Large Language Models (LLMs). However, state-of-the-art transformer based LLMs
often ignore negations in natural language and there is no existing benchmark
to quantitatively evaluate whether multimodal transformers inherit this
weakness. In this study, we present a new multimodal question answering (QA)
benchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017)
with the goal of systematically evaluating if multimodal transformers can
perform complex reasoning to recognize new concepts as negation of previously
learned concepts. We show that with standard fine-tuning approach multimodal
transformers are still incapable of correctly interpreting negation
irrespective of model size. However, our experiments demonstrate that
augmenting the original training task distributions with negated QA examples
allow the model to reliably reason with negation. To do this, we describe a
novel data generation procedure that prompts the 540B-parameter PaLM model to
automatically generate negated QA examples as compositions of easily accessible
video tags. The generated examples contain more natural linguistic patterns and
the gains compared to template-based task augmentation approach are
significant.",NeurIPS 2022 SyntheticData4ML Workshop,
Introducing Model Inversion Attacks on Automatic Speaker Recognition,"[arxiv.Result.Author('Karla Pizzi'), arxiv.Result.Author('Franziska Boenisch'), arxiv.Result.Author('Ugur Sahin'), arxiv.Result.Author('Konstantin Böttinger')]",2023-01-09 08:51:15+00:00,"Model inversion (MI) attacks allow to reconstruct average per-class
representations of a machine learning (ML) model's training data. It has been
shown that in scenarios where each class corresponds to a different individual,
such as face classifiers, this represents a severe privacy risk. In this work,
we explore a new application for MI: the extraction of speakers' voices from a
speaker recognition system. We present an approach to (1) reconstruct audio
samples from a trained ML model and (2) extract intermediate voice feature
representations which provide valuable insights into the speakers' biometrics.
  Therefore, we propose an extension of MI attacks which we call sliding model
inversion. Our sliding MI extends standard MI by iteratively inverting
overlapping chunks of the audio samples and thereby leveraging the sequential
properties of audio data for enhanced inversion performance. We show that one
can use the inverted audio data to generate spoofed audio samples to
impersonate a speaker, and execute voice-protected commands for highly secured
systems on their behalf. To the best of our knowledge, our work is the first
one extending MI attacks to audio data, and our results highlight the security
risks resulting from the extraction of the biometric data in that setup.","for associated pdf, see
  https://www.isca-speech.org/archive/pdfs/spsc_2022/pizzi22_spsc.pdf","Proc. 2nd Symposium on Security and Privacy in Speech
  Communication, 2022"
Applying Automated Machine Translation to Educational Video Courses,[arxiv.Result.Author('Linden Wang')],2023-01-09 01:44:29+00:00,"We studied the capability of automated machine translation in the online
video education space by automatically translating Khan Academy videos with
state of the art translation models and applying Text-to-Speech synthesis to
build engaging videos in target languages. We also analyzed and established a
reliable translation confidence estimator based on round-trip translations in
order to efficiently manage translation quality and reduce human translation
effort. Finally, we developed a deployable system to deliver translated videos
to end users and collect user corrections for iterative improvement.","13 pages, 5 figures",
Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture,"[arxiv.Result.Author('Pim Jordi Verschuuren'), arxiv.Result.Author('Jie Gao'), arxiv.Result.Author('Adelize van Eeden'), arxiv.Result.Author('Stylianos Oikonomou'), arxiv.Result.Author('Anil Bandhakavi')]",2023-01-09 00:19:11+00:00,"In this paper, we present the Logically submissions to De-Factify 2 challenge
(DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our
submissions to this challenge including explored evidence retrieval and
selection techniques, pre-trained cross-modal and unimodal models, and a
cross-modal veracity model based on the well established Transformer Encoder
(TE) architecture which is heavily relies on the concept of self-attention.
Exploratory analysis is also conducted on this Factify 2 data set that uncovers
the salient multi-modal patterns and hypothesis motivating the architecture
proposed in this work. A series of preliminary experiments were done to
investigate and benchmarking different pre-trained embedding models, evidence
retrieval settings and thresholds. The final system, a standard two-stage
evidence based veracity detection system, yields weighted avg. 0.79 on both val
set and final blind test set on the task 1, which achieves 3rd place with a
small margin to the top performing system on the leaderboard among 9
participants.","Accepted in AAAI'23: Second Workshop on Multimodal Fact-Checking and
  Hate Speech Detection, February 2023, Washington, DC, USA",
Equivariant and Steerable Neural Networks: A review with special emphasis on the symmetric group,"[arxiv.Result.Author('Patrick Krüger'), arxiv.Result.Author('Hanno Gottschalk')]",2023-01-08 11:05:31+00:00,"Convolutional neural networks revolutionized computer vision and natrual
language processing. Their efficiency, as compared to fully connected neural
networks, has its origin in the architecture, where convolutions reflect the
translation invariance in space and time in pattern or speech recognition
tasks. Recently, Cohen and Welling have put this in the broader perspective of
invariance under symmetry groups, which leads to the concept of group
equivaiant neural networks and more generally steerable neural networks. In
this article, we review the architecture of such networks including equivariant
layers and filter banks, activation with capsules and group pooling. We apply
this formalism to the symmetric group, for which we work out a number of
details on representations and capsules that are not found in the literature.",,
Analyzing the Representational Geometry of Acoustic Word Embeddings,"[arxiv.Result.Author('Badr M. Abdullah'), arxiv.Result.Author('Dietrich Klakow')]",2023-01-08 10:22:50+00:00,"Acoustic word embeddings (AWEs) are vector representations such that
different acoustic exemplars of the same word are projected nearby in the
embedding space. In addition to their use in speech technology applications
such as spoken term discovery and keyword spotting, AWE models have been
adopted as models of spoken-word processing in several cognitively motivated
studies and have been shown to exhibit human-like performance in some auditory
processing tasks. Nevertheless, the representational geometry of AWEs remains
an under-explored topic that has not been studied in the literature. In this
paper, we take a closer analytical look at AWEs learned from English speech and
study how the choice of the learning objective and the architecture shapes
their representational profile. To this end, we employ a set of analytic
techniques from machine learning and neuroscience in three different analyses:
embedding space uniformity, word discriminability, and representational
consistency. Our main findings highlight the prominent role of the learning
objective on shaping the representation profile compared to the model
architecture.","In BlackboxNLP workshop, EMNLP 2022 [ oral presentation ]",
SpeeChain: A Speech Toolkit for Large-Scale Machine Speech Chain,"[arxiv.Result.Author('Heli Qi'), arxiv.Result.Author('Sashi Novitasari'), arxiv.Result.Author('Andros Tjandra'), arxiv.Result.Author('Sakriani Sakti'), arxiv.Result.Author('Satoshi Nakamura')]",2023-01-08 03:16:56+00:00,"This paper introduces SpeeChain, an open-source Pytorch-based toolkit
designed to develop the machine speech chain for large-scale use. This first
release focuses on the TTS-to-ASR chain, a core component of the machine speech
chain, that refers to the TTS data augmentation by unspoken text for ASR. To
build an efficient pipeline for the large-scale TTS-to-ASR chain, we implement
easy-to-use multi-GPU batch-level model inference, multi-dataloader batch
generation, and on-the-fly data selection techniques. In this paper, we first
explain the overall procedure of the TTS-to-ASR chain and the difficulties of
each step. Then, we present a detailed ablation study on different types of
unlabeled data, data filtering thresholds, batch composition, and
real-synthetic data ratios. Our experimental results on train_clean_460 of
LibriSpeech demonstrate that our TTS-to-ASR chain can significantly improve WER
in a semi-supervised setting.",Submitted to ICASSP 2023,
Perceptual-Neural-Physical Sound Matching,"[arxiv.Result.Author('Han Han'), arxiv.Result.Author('Vincent Lostanlen'), arxiv.Result.Author('Mathieu Lagrange')]",2023-01-07 16:17:48+00:00,"Sound matching algorithms seek to approximate a target waveform by parametric
audio synthesis. Deep neural networks have achieved promising results in
matching sustained harmonic tones. However, the task is more challenging when
targets are nonstationary and inharmonic, e.g., percussion. We attribute this
problem to the inadequacy of loss function. On one hand, mean square error in
the parametric domain, known as ""P-loss"", is simple and fast but fails to
accommodate the differing perceptual significance of each parameter. On the
other hand, mean square error in the spectrotemporal domain, known as ""spectral
loss"", is perceptually motivated and serves in differentiable digital signal
processing (DDSP). Yet, spectral loss has more local minima than P-loss and its
gradient may be computationally expensive; hence a slow convergence. Against
this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the
optimal quadratic approximation of spectral loss while being as fast as P-loss
during training. We instantiate PNP with physical modeling synthesis as decoder
and joint time-frequency scattering transform (JTFS) as spectral
representation. We demonstrate its potential on matching synthetic drum sounds
in comparison with other loss functions.",,
TunesFormer: Forming Tunes with Control Codes,"[arxiv.Result.Author('Shangda Wu'), arxiv.Result.Author('Maosong Sun')]",2023-01-07 16:11:55+00:00,"In recent years, deep learning techniques have been applied to music
generation systems with promising results. However, one of the main challenges
in this field has been the lack of annotated datasets, making it difficult for
models to learn musical forms in compositions. To address this issue, we
present TunesFormer, a Transformer-based melody generation system that is
trained on a large dataset of 285,449 ABC tunes. By utilizing specific symbols
commonly found in ABC notation to indicate section boundaries, TunesFormer can
understand and generate melodies with given musical forms based on control
codes. Our objective evaluations demonstrate the effectiveness of the control
codes in achieving controlled musical forms, and subjective experiments show
that the generated melodies are of comparable quality to human compositions.
Our results also provide insights into the optimal placement of control codes
and their impact on the generated melodies. TunesFormer presents a promising
approach for generating melodies with desired musical forms through the use of
deep learning techniques.","9 pages, 11 figures",
Using External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition,"[arxiv.Result.Author('David M. Chan'), arxiv.Result.Author('Shalini Ghosh'), arxiv.Result.Author('Ariya Rastrow'), arxiv.Result.Author('Björn Hoffmeister')]",2023-01-06 22:32:50+00:00,"Despite improvements to the generalization performance of automated speech
recognition (ASR) models, specializing ASR models for downstream tasks remains
a challenging task, primarily due to reduced data availability (necessitating
increased data collection), and rapidly shifting data distributions (requiring
more frequent model fine-tuning). In this work, we investigate the potential of
leveraging external knowledge, particularly through off-policy key-value stores
generated with text-to-speech methods, to allow for flexible post-training
adaptation to new data distributions. In our approach, audio embeddings
captured from text-to-speech, along with semantic text embeddings, are used to
bias ASR via an approximate k-nearest-neighbor (KNN) based attentive fusion
step. Our experiments on LibiriSpeech and in-house voice assistant/search
datasets show that the proposed approach can reduce domain adaptation time by
up to 1K GPU-hours while providing up to 3% WER improvement compared to a
fine-tuning baseline, suggesting a promising approach for adapting production
ASR systems in challenging zero and few-shot scenarios.",,
Multimodal Lyrics-Rhythm Matching,"[arxiv.Result.Author('Callie C. Liao'), arxiv.Result.Author('Duoduo Liao'), arxiv.Result.Author('Jesse Guessford')]",2023-01-06 22:24:53+00:00,"Despite the recent increase in research on artificial intelligence for music,
prominent correlations between key components of lyrics and rhythm such as
keywords, stressed syllables, and strong beats are not frequently studied. Ths
is likely due to challenges such as audio misalignment, inaccuracies in
syllabic identification, and most importantly, the need for cross-disciplinary
knowledge. To address this lack of research, we propose a novel multimodal
lyrics-rhythm matching approach in this paper that specifically matches key
components of lyrics and music with each other without any language
limitations. We use audio instead of sheet music with readily available
metadata, which creates more challenges yet increases the application
flexibility of our method. Furthermore, our approach creatively generates
several patterns involving various multimodalities, including music strong
beats, lyrical syllables, auditory changes in a singer's pronunciation, and
especially lyrical keywords, which are utilized for matching key lyrical
elements with key rhythmic elements. This advantageous approach not only
provides a unique way to study auditory lyrics-rhythm correlations including
efficient rhythm-based audio alignment algorithms, but also bridges
computational linguistics with music as well as music cognition. Our
experimental results reveal an 0.81 probability of matching on average, and
around 30% of the songs have a probability of 0.9 or higher of keywords landing
on strong beats, including 12% of the songs with a perfect landing. Also, the
similarity metrics are used to evaluate the correlation between lyrics and
rhythm. It shows that nearly 50% of the songs have 0.70 similarity or higher.
In conclusion, our approach contributes significantly to the lyrics-rhythm
relationship by computationally unveiling insightful correlations.",,
A new conversational interaction concept for document creation and editing on mobile devices for visually impaired users,"[arxiv.Result.Author('Alireza Darvishy'), arxiv.Result.Author('Hans-Peter Hutter'), arxiv.Result.Author('Edin Beljulji'), arxiv.Result.Author('Zeno Heeb')]",2023-01-06 14:56:45+00:00,"This paper describes the ongoing development of a conversational interaction
concept that allows visually impaired users to easily create and edit text
documents on mobile devices using mainly voice input. In order to verify the
concept, a prototype app was developed and tested for both iOS and Android
systems, based on the natural-language understanding (NLU) platform Google
Dialogflow. The app and interaction concept were repeatedly tested by users
with and without visual impairments. Based on their feedback, the concept was
continuously refined, adapted and improved on both mobile platforms. In an
iterative user-centred design approach, the following research questions were
investigated: Can a visually impaired user rely mainly on speech commands to
efficiently create and edit a document on mobile devices? User testing found
that an interaction concept based on conversational speech commands was easy
and intuitive for visually impaired users. However, it was also found that
relying on speech commands alone created its own obstacles, and that a
combination of gestures and voice interaction would be more robust. Future
research and more extensive useability tests should be carried out among
visually impaired users in order to optimize the interaction concept.",,
Multi-Genre Music Transformer -- Composing Full Length Musical Piece,[arxiv.Result.Author('Abhinav Kaushal Keshari')],2023-01-06 05:27:55+00:00,"In the task of generating music, the art factor plays a big role and is a
great challenge for AI. Previous work involving adversarial training to produce
new music pieces and modeling the compatibility of variety in music (beats,
tempo, musical stems) demonstrated great examples of learning this task. Though
this was limited to generating mashups or learning features from tempo and key
distributions to produce similar patterns. Compound Word Transformer was able
to represent music generation task as a sequence generation challenge involving
musical events defined by compound words. These musical events give a more
accurate description of notes progression, chord change, harmony and the art
factor. The objective of the project is to implement a Multi-Genre Transformer
which learns to produce music pieces through more adaptive learning process
involving more challenging task where genres or form of the composition is also
considered. We built a multi-genre compound word dataset, implemented a linear
transformer which was trained on this dataset. We call this Multi-Genre
Transformer, which was able to generate full length new musical pieces which is
diverse and comparable to original tracks. The model trains 2-5 times faster
than other models discussed.",,
CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior,"[arxiv.Result.Author('Jinbo Xing'), arxiv.Result.Author('Menghan Xia'), arxiv.Result.Author('Yuechen Zhang'), arxiv.Result.Author('Xiaodong Cun'), arxiv.Result.Author('Jue Wang'), arxiv.Result.Author('Tien-Tsin Wong')]",2023-01-06 05:04:32+00:00,"Speech-driven 3D facial animation has been widely studied, yet there is still
a gap to achieving realism and vividness due to the highly ill-posed nature and
scarcity of audio-visual data. Existing works typically formulate the
cross-modal mapping into a regression task, which suffers from the
regression-to-mean problem leading to over-smoothed facial motions. In this
paper, we propose to cast speech-driven facial animation as a code query task
in a finite proxy space of the learned codebook, which effectively promotes the
vividness of the generated motions by reducing the cross-modal mapping
uncertainty. The codebook is learned by self-reconstruction over real facial
motions and thus embedded with realistic facial motion priors. Over the
discrete motion space, a temporal autoregressive model is employed to
sequentially synthesize facial motions from the input speech signal, which
guarantees lip-sync as well as plausible facial expressions. We demonstrate
that our approach outperforms current state-of-the-art methods both
qualitatively and quantitatively. Also, a user study further justifies our
superiority in perceptual quality.","15 pages, Project Page:
  https://doubiiu.github.io/projects/codetalker/",
Unsupervised Broadcast News Summarization; a comparative study on Maximal Marginal Relevance (MMR) and Latent Semantic Analysis (LSA),"[arxiv.Result.Author('Majid Ramezani'), arxiv.Result.Author('Mohammad-Salar Shahryari'), arxiv.Result.Author('Amir-Reza Feizi-Derakhshi'), arxiv.Result.Author('Mohammad-Reza Feizi-Derakhshi')]",2023-01-05 20:13:44+00:00,"The methods of automatic speech summarization are classified into two groups:
supervised and unsupervised methods. Supervised methods are based on a set of
features, while unsupervised methods perform summarization based on a set of
rules. Latent Semantic Analysis (LSA) and Maximal Marginal Relevance (MMR) are
considered the most important and well-known unsupervised methods in automatic
speech summarization. This study set out to investigate the performance of two
aforementioned unsupervised methods in transcriptions of Persian broadcast news
summarization. The results show that in generic summarization, LSA outperforms
MMR, and in query-based summarization, MMR outperforms LSA in broadcast news
summarization.","This is a preprint of an article accepted in ""28th International
  Computer Conference (2023), Computer Society of Iran, Sharif University of
  Technology, Tehran""",
Singing voice synthesis based on frame-level sequence-to-sequence models considering vocal timing deviation,"[arxiv.Result.Author('Miku Nishihara'), arxiv.Result.Author('Yukiya Hono'), arxiv.Result.Author('Kei Hashimoto'), arxiv.Result.Author('Yoshihiko Nankaku'), arxiv.Result.Author('Keiichi Tokuda')]",2023-01-05 19:00:10+00:00,"This paper proposes singing voice synthesis (SVS) based on frame-level
sequence-to-sequence models considering vocal timing deviation. In SVS, it is
essential to synchronize the timing of singing with temporal structures
represented by scores, taking into account that there are differences between
actual vocal timing and note start timing. In many SVS systems including our
previous work, phoneme-level score features are converted into frame-level ones
on the basis of phoneme boundaries obtained by external aligners to take into
account vocal timing deviations. Therefore, the sound quality is affected by
the aligner accuracy in this system. To alleviate this problem, we introduce an
attention mechanism with frame-level features. In the proposed system, the
attention mechanism absorbs alignment errors in phoneme boundaries.
Additionally, we evaluate the system with pseudo-phoneme-boundaries defined by
heuristic rules based on musical scores when there is no aligner. The
experimental results show the effectiveness of the proposed system.","5 pages, 4 figures, submitted to ICASSP 2023",
Automatic Sound Event Detection and Classification of Great Ape Calls Using Neural Networks,"[arxiv.Result.Author('Zifan Jiang'), arxiv.Result.Author('Adrian Soldati'), arxiv.Result.Author('Isaac Schamberg'), arxiv.Result.Author('Adriano R. Lameira'), arxiv.Result.Author('Steven Moran')]",2023-01-05 18:33:40+00:00,"We present a novel approach to automatically detect and classify great ape
calls from continuous raw audio recordings collected during field research. Our
method leverages deep pretrained and sequential neural networks, including
wav2vec 2.0 and LSTM, and is validated on three data sets from three different
great ape lineages (orangutans, chimpanzees, and bonobos). The recordings were
collected by different researchers and include different annotation schemes,
which our pipeline preprocesses and trains in a uniform fashion. Our results
for call detection and classification attain high accuracy. Our method is aimed
to be generalizable to other animal species, and more generally, sound event
detection tasks. To foster future research, we make our pipeline and methods
publicly available.",,
Do Users Want Platform Moderation or Individual Control? Examining the Role of Third-Person Effects and Free Speech Support in Shaping Moderation Preferences,"[arxiv.Result.Author('Shagun Jhaver'), arxiv.Result.Author('Amy Zhang')]",2023-01-05 18:21:01+00:00,"This study examines social media users' preferences for the use of
platform-wide moderation in comparison to user-controlled, personalized
moderation tools to regulate three categories of norm-violating content - hate
speech, sexually explicit content, and violent content. Via a nationally
representative survey of 984 US adults, we explore the influence of
third-person effects and support for freedom of expression on this choice. We
find that perceived negative effects on others negatively predict while free
speech support positively predicts a preference for having personal moderation
settings over platform-directed moderation for regulating each speech category.
Our findings show that platform governance initiatives need to account for both
actual and perceived media effects of norm-violating speech categories to
increase user satisfaction. Our analysis suggests that users do not view
personal moderation tools as an infringement on others' free speech but as a
means to assert greater agency over their social media feeds.",,
Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,"[arxiv.Result.Author('Chengyi Wang'), arxiv.Result.Author('Sanyuan Chen'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Ziqiang Zhang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Yanqing Liu'), arxiv.Result.Author('Huaming Wang'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Furu Wei')]",2023-01-05 15:37:15+00:00,"We introduce a language modeling approach for text to speech synthesis (TTS).
Specifically, we train a neural codec language model (called Vall-E) using
discrete codes derived from an off-the-shelf neural audio codec model, and
regard TTS as a conditional language modeling task rather than continuous
signal regression as in previous work. During the pre-training stage, we scale
up the TTS training data to 60K hours of English speech which is hundreds of
times larger than existing systems. Vall-E emerges in-context learning
capabilities and can be used to synthesize high-quality personalized speech
with only a 3-second enrolled recording of an unseen speaker as an acoustic
prompt. Experiment results show that Vall-E significantly outperforms the
state-of-the-art zero-shot TTS system in terms of speech naturalness and
speaker similarity. In addition, we find Vall-E could preserve the speaker's
emotion and acoustic environment of the acoustic prompt in synthesis. See
https://aka.ms/valle for demos of our work.",Working in progress,
Expressive Speech-driven Facial Animation with controllable emotions,"[arxiv.Result.Author('Yutong Chen'), arxiv.Result.Author('Junhong Zhao'), arxiv.Result.Author('Wei-Qiang Zhang')]",2023-01-05 11:17:19+00:00,"It is in high demand to generate facial animation with high realism, but it
remains a challenging task. Existing approaches of speech-driven facial
animation can produce satisfactory mouth movement and lip synchronization, but
show weakness in dramatic emotional expressions and flexibility in emotion
control. This paper presents a novel deep learning-based approach for
expressive facial animation generation from speech that can exhibit
wide-spectrum facial expressions with controllable emotion type and intensity.
We propose an emotion controller module to learn the relationship between the
emotion variations (e.g., types and intensity) and the corresponding facial
expression parameters. It enables emotion-controllable facial animation, where
the target expression can be continuously adjusted as desired. The qualitative
and quantitative evaluations show that the animation generated by our method is
rich in facial emotional expressiveness while retaining accurate lip movement,
outperforming other state-of-the-art methods.",,
A Novel Exploitative and Explorative GWO-SVM Algorithm for Smart Emotion Recognition,"[arxiv.Result.Author('Xucun Yan'), arxiv.Result.Author('Zihuai Lin'), arxiv.Result.Author('Zhiyun Lin'), arxiv.Result.Author('Branka Vucetic')]",2023-01-05 03:16:28+00:00,"Emotion recognition or detection is broadly utilized in patient-doctor
interactions for diseases such as schizophrenia and autism and the most typical
techniques are speech detection and facial recognition. However, features
extracted from these behavior-based emotion recognitions are not reliable since
humans can disguise their emotions. Recording voices or tracking facial
expressions for a long term is also not efficient. Therefore, our aim is to
find a reliable and efficient emotion recognition scheme, which can be used for
non-behavior-based emotion recognition in real-time. This can be solved by
implementing a single-channel electrocardiogram (ECG) based emotion recognition
scheme in a lightweight embedded system. However, existing schemes have
relatively low accuracy. Therefore, we propose a reliable and efficient emotion
recognition scheme - exploitative and explorative grey wolf optimizer based SVM
(X - GWO - SVM) for ECG-based emotion recognition. Two datasets, one raw
self-collected iRealcare dataset, and the widely-used benchmark WESAD dataset
are used in the X - GWO - SVM algorithm for emotion recognition. This work
demonstrates that the X - GWO - SVM algorithm can be used for emotion
recognition and the algorithm exhibits superior performance in reliability
compared to the use of other supervised machine learning methods in earlier
works. It can be implemented in a lightweight embedded system, which is much
more efficient than existing solutions based on deep neural networks.",,
Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,"[arxiv.Result.Author('Chao Feng'), arxiv.Result.Author('Ziyang Chen'), arxiv.Result.Author('Andrew Owens')]",2023-01-04 18:59:49+00:00,"Manipulated videos often contain subtle inconsistencies between their visual
and audio signals. We propose a video forensics method, based on anomaly
detection, that can identify these inconsistencies, and that can be trained
solely using real, unlabeled data. We train an autoregressive model to generate
sequences of audio-visual features, using feature sets that capture the
temporal synchronization between video frames and sound. At test time, we then
flag videos that the model assigns low probability. Despite being trained
entirely on real videos, our model obtains strong performance on the task of
detecting manipulated speech videos. Project site:
https://cfeng16.github.io/audio-visual-forensics",,
On the Convergence of Stochastic Gradient Descent in Low-precision Number Formats,"[arxiv.Result.Author('Matteo Cacciola'), arxiv.Result.Author('Antonio Frangioni'), arxiv.Result.Author('Masoud Asgharian'), arxiv.Result.Author('Alireza Ghaffari'), arxiv.Result.Author('Vahid Partovi Nia')]",2023-01-04 14:54:15+00:00,"Deep learning models are dominating almost all artificial intelligence tasks
such as vision, text, and speech processing. Stochastic Gradient Descent (SGD)
is the main tool for training such models, where the computations are usually
performed in single-precision floating-point number format. The convergence of
single-precision SGD is normally aligned with the theoretical results of real
numbers since they exhibit negligible error. However, the numerical error
increases when the computations are performed in low-precision number formats.
This provides compelling reasons to study the SGD convergence adapted for
low-precision computations. We present both deterministic and stochastic
analysis of the SGD algorithm, obtaining bounds that show the effect of number
format. Such bounds can provide guidelines as to how SGD convergence is
affected when constraints render the possibility of performing high-precision
computations remote.",,
Grid-Based Decimation for Wavelet Transforms with Stably Invertible Implementation,"[arxiv.Result.Author('Nicki Holighaus'), arxiv.Result.Author('Günther Koliander'), arxiv.Result.Author('Clara Hollomey'), arxiv.Result.Author('Friedrich Pillichshammer')]",2023-01-04 14:32:09+00:00,"The constant center frequency to bandwidth ratio (Q-factor) of wavelet
transforms provides a very natural representation for audio data. However,
invertible wavelet transforms have either required non-uniform decimation --
leading to irregular data structures that are cumbersome to work with -- or
require excessively high oversampling with unacceptable computational overhead.
Here, we present a novel decimation strategy for wavelet transforms that leads
to stable representations with oversampling rates close to one and uniform
decimation. Specifically, we show that finite implementations of the resulting
representation are energy-preserving in the sense of frame theory. The obtained
wavelet coefficients can be stored in a timefrequency matrix with a natural
interpretation of columns as time frames and rows as frequency channels. This
matrix structure immediately grants access to a large number of algorithms that
are successfully used in time-frequency audio processing, but could not
previously be used jointly with wavelet transforms. We demonstrate the
application of our method in processing based on nonnegative matrix
factorization, in onset detection, and in phaseless reconstruction.",,"IEEE/ACM Transactions on Audio, Speech and Language Processing,
  31:789--801, January 2023"
Validity in Music Information Research Experiments,"[arxiv.Result.Author('Bob L. T. Sturm'), arxiv.Result.Author('Arthur Flexer')]",2023-01-04 12:52:47+00:00,"Validity is the truth of an inference made from evidence, such as data
collected in an experiment, and is central to working scientifically. Given the
maturity of the domain of music information research (MIR), validity in our
opinion should be discussed and considered much more than it has been so far.
Considering validity in one's work can improve its scientific and engineering
value. Puzzling MIR phenomena like adversarial attacks and performance glass
ceilings become less mysterious through the lens of validity. In this article,
we review the subject of validity in general, considering the four major types
of validity from a key reference: Shadish et al. 2002. We ground our discussion
of these types with a prototypical MIR experiment: music classification using
machine learning. Through this MIR experimentalists can be guided to make valid
inferences from data collected from their experiments.",,
Audio-Visual Efficient Conformer for Robust Speech Recognition,"[arxiv.Result.Author('Maxime Burchi'), arxiv.Result.Author('Radu Timofte')]",2023-01-04 05:36:56+00:00,"End-to-end Automatic Speech Recognition (ASR) systems based on neural
networks have seen large improvements in recent years. The availability of
large scale hand-labeled datasets and sufficient computing resources made it
possible to train powerful deep neural networks, reaching very low Word Error
Rate (WER) on academic benchmarks. However, despite impressive performance on
clean audio samples, a drop of performance is often observed on noisy speech.
In this work, we propose to improve the noise robustness of the recently
proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based
architecture by processing both audio and visual modalities. We improve
previous lip reading methods using an Efficient Conformer back-end on top of a
ResNet-18 visual front-end and by adding intermediate CTC losses between
blocks. We condition intermediate block features on early predictions using
Inter CTC residual modules to relax the conditional independence assumption of
CTC-based models. We also replace the Efficient Conformer grouped attention by
a more efficient and simpler attention mechanism that we call patch attention.
We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and
visual modalities allows to better recognize speech in the presence of
environmental noise and significantly accelerate training, reaching lower WER
with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)
model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on
LRS2 and LRS3 test sets. Code and pretrained models are available at
https://github.com/burchim/AVEC.",,
Object Segmentation with Audio Context,"[arxiv.Result.Author('Kaihui Zheng'), arxiv.Result.Author('Yuqing Ren'), arxiv.Result.Author('Zixin Shen'), arxiv.Result.Author('Tianxu Qin')]",2023-01-04 01:33:42+00:00,"Visual objects often have acoustic signatures that are naturally synchronized
with them in audio-bearing video recordings. For this project, we explore the
multimodal feature aggregation for video instance segmentation task, in which
we integrate audio features into our video segmentation model to conduct an
audio-visual learning scheme. Our method is based on existing video instance
segmentation method which leverages rich contextual information across video
frames. Since this is the first attempt to investigate the audio-visual
instance segmentation, a novel dataset, including 20 vocal classes with
synchronized video and audio recordings, is collected. By utilizing combined
decoder to fuse both video and audio features, our model shows a slight
improvements compared to the base model. Additionally, we managed to show the
effectiveness of different modules by conducting extensive ablations.","Research project for Introduction to Deep Learning (11785) at
  Carnegie Mellon University",
An ensemble-based framework for mispronunciation detection of Arabic phonemes,"[arxiv.Result.Author('Sukru Selim Calik'), arxiv.Result.Author('Ayhan Kucukmanisa'), arxiv.Result.Author('Zeynep Hilal Kilimci')]",2023-01-03 22:17:08+00:00,"Determination of mispronunciations and ensuring feedback to users are
maintained by computer-assisted language learning (CALL) systems. In this work,
we introduce an ensemble model that defines the mispronunciation of Arabic
phonemes and assists learning of Arabic, effectively. To the best of our
knowledge, this is the very first attempt to determine the mispronunciations of
Arabic phonemes employing ensemble learning techniques and conventional machine
learning models, comprehensively. In order to observe the effect of feature
extraction techniques, mel-frequency cepstrum coefficients (MFCC), and Mel
spectrogram are blended with each learning algorithm. To show the success of
proposed model, 29 letters in the Arabic phonemes, 8 of which are hafiz, are
voiced by a total of 11 different person. The amount of data set has been
enhanced employing the methods of adding noise, time shifting, time stretching,
pitch shifting. Extensive experiment results demonstrate that the utilization
of voting classifier as an ensemble algorithm with Mel spectrogram feature
extraction technique exhibits remarkable classification result with 95.9% of
accuracy.",,
Modeling the Rhythm from Lyrics for Melody Generation of Pop Song,"[arxiv.Result.Author('Daiyu Zhang'), arxiv.Result.Author('Ju-Chiang Wang'), arxiv.Result.Author('Katerina Kosta'), arxiv.Result.Author('Jordan B. L. Smith'), arxiv.Result.Author('Shicen Zhou')]",2023-01-03 21:30:20+00:00,"Creating a pop song melody according to pre-written lyrics is a typical
practice for composers. A computational model of how lyrics are set as melodies
is important for automatic composition systems, but an end-to-end
lyric-to-melody model would require enormous amounts of paired training data.
To mitigate the data constraints, we adopt a two-stage approach, dividing the
task into lyric-to-rhythm and rhythm-to-melody modules. However, the
lyric-to-rhythm task is still challenging due to its multimodality. In this
paper, we propose a novel lyric-to-rhythm framework that includes
part-of-speech tags to achieve better text setting, and a Transformer
architecture designed to model long-term syllable-to-note associations. For the
rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer,
which has achieved state-of-the-art results. Experiments for Chinese
lyric-to-melody generation show that the proposed framework is able to model
key characteristics of rhythm and pitch distributions in the dataset, and in a
subjective evaluation, the melodies generated by our system were rated as
similar to or better than those of a state-of-the-art alternative.",Published in ISMIR 2022,
Language Models are Drummers: Drum Composition with Natural Language Pre-Training,"[arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Chris Callison-Burch')]",2023-01-03 15:47:53+00:00,"Automatic music generation with artificial intelligence typically requires a
large amount of data which is hard to obtain for many less common genres and
musical instruments. To tackle this issue, we present ongoing work and
preliminary findings on the possibility for deep models to transfer knowledge
from language to music, by finetuning large language models pre-trained on a
massive text corpus on only hundreds of MIDI files of drum performances. We
show that by doing so, one of the largest, state-of-the-art models (GPT3) is
capable of generating reasonable drum grooves, while models that are not
pre-trained (Transformer) shows no such ability beyond naive repetition.
Evaluating generated music is a challenging task, more so is evaluating drum
grooves with little precedence in literature. Hence, we propose a tailored
structural evaluation method and analyze drum grooves produced by GPT3 compared
to those played by human professionals, exposing the strengths and weaknesses
of such generation by language-to-music transfer. Our findings suggest that
language-to-music transfer learning with large language models is viable and
promising.","Accepted to the 1st workshop on Creative AI across Modalities in AAAI
  2023",
StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,"[arxiv.Result.Author('Yifeng Ma'), arxiv.Result.Author('Suzhen Wang'), arxiv.Result.Author('Zhipeng Hu'), arxiv.Result.Author('Changjie Fan'), arxiv.Result.Author('Tangjie Lv'), arxiv.Result.Author('Yu Ding'), arxiv.Result.Author('Zhidong Deng'), arxiv.Result.Author('Xin Yu')]",2023-01-03 13:16:24+00:00,"Different people speak with diverse personalized speaking styles. Although
existing one-shot talking head methods have made significant progress in lip
sync, natural facial expressions, and stable head motions, they still cannot
generate diverse speaking styles in the final talking head videos. To tackle
this problem, we propose a one-shot style-controllable talking face generation
framework. In a nutshell, we aim to attain a speaking style from an arbitrary
reference speaking video and then drive the one-shot portrait to speak with the
reference speaking style and another piece of audio. Specifically, we first
develop a style encoder to extract dynamic facial motion patterns of a style
reference video and then encode them into a style code. Afterward, we introduce
a style-controllable decoder to synthesize stylized facial animations from the
speech content and style code. In order to integrate the reference speaking
style into generated videos, we design a style-aware adaptive transformer,
which enables the encoded style code to adjust the weights of the feed-forward
layers accordingly. Thanks to the style-aware adaptation mechanism, the
reference speaking style can be better embedded into synthesized videos during
decoding. Extensive experiments demonstrate that our method is capable of
generating talking head videos with diverse speaking styles from only one
portrait image and an audio clip while achieving authentic visual effects.
Project Page: https://github.com/FuxiVirtualHuman/styletalk.",Accepted at AAAI2023. Demo: https://youtu.be/mO2Tjcwr4u8,
Supervised Acoustic Embeddings And Their Transferability Across Languages,"[arxiv.Result.Author('Sreepratha Ram'), arxiv.Result.Author('Hanan Aldarmaki')]",2023-01-03 09:37:24+00:00,"In speech recognition, it is essential to model the phonetic content of the
input signal while discarding irrelevant factors such as speaker variations and
noise, which is challenging in low-resource settings. Self-supervised
pre-training has been proposed as a way to improve both supervised and
unsupervised speech recognition, including frame-level feature representations
and Acoustic Word Embeddings (AWE) for variable-length segments. However,
self-supervised models alone cannot learn perfect separation of the linguistic
content as they are trained to optimize indirect objectives. In this work, we
experiment with different pre-trained self-supervised features as input to AWE
models and show that they work best within a supervised framework. Models
trained on English can be transferred to other languages with no adaptation and
outperform self-supervised models trained solely on the target languages.",Presented at ICNLSP 2022,
Interpretation and Analysis of the Steady-State Neural Response to Complex Sequential Structures: a Methodological Note,[arxiv.Result.Author('Nai Ding')],2023-01-03 06:38:58+00:00,"Frequency tagging is a powerful approach to investigate the neural processing
of sensory features, and is recently adapted to study the neural correlates of
superordinate structures, i.e., chunks, in complex sequences such as speech and
music. The nesting of sequence structures, the necessity to control the
periodicity in sensory features, and the low-frequency nature of sequence
structures pose new challenges for data analysis and interpretation. Here, I
discuss how to interpret the frequency of a sequential structure, and factors
that need to be considered when analyzing the periodicity in a signal. Finally,
a safe procedure is recommended for the analysis of frequency-tagged responses.",,
e-Inu: Simulating A Quadruped Robot With Emotional Sentience,"[arxiv.Result.Author('Abhiruph Chakravarty'), arxiv.Result.Author('Jatin Karthik Tripathy'), arxiv.Result.Author('Sibi Chakkaravarthy S'), arxiv.Result.Author('Aswani Kumar Cherukuri'), arxiv.Result.Author('S. Anitha'), arxiv.Result.Author('Firuz Kamalov'), arxiv.Result.Author('Annapurna Jonnalagadda')]",2023-01-03 06:28:45+00:00,"Quadruped robots are currently used in industrial robotics as mechanical aid
to automate several routine tasks. However, presently, the usage of such a
robot in a domestic setting is still very much a part of the research. This
paper discusses the understanding and virtual simulation of such a robot
capable of detecting and understanding human emotions, generating its gait, and
responding via sounds and expression on a screen. To this end, we use a
combination of reinforcement learning and software engineering concepts to
simulate a quadruped robot that can understand emotions, navigate through
various terrains and detect sound sources, and respond to emotions using
audio-visual feedback. This paper aims to establish the framework of simulating
a quadruped robot that is emotionally intelligent and can primarily respond to
audio-visual stimuli using motor or audio response. The emotion detection from
the speech was not as performant as ERANNs or Zeta Policy learning, still
managing an accuracy of 63.5%. The video emotion detection system produced
results that are almost at par with the state of the art, with an accuracy of
99.66%. Due to its ""on-policy"" learning process, the PPO algorithm was
extremely rapid to learn, allowing the simulated dog to demonstrate a
remarkably seamless gait across the different cadences and variations. This
enabled the quadruped robot to respond to generated stimuli, allowing us to
conclude that it functions as predicted and satisfies the aim of this work.",,
Hyperuniform disordered parametric loudspeaker array,"[arxiv.Result.Author('Kun Tang'), arxiv.Result.Author('Yuqi Wang'), arxiv.Result.Author('Shaobo Wang'), arxiv.Result.Author('Da Gao'), arxiv.Result.Author('Haojie Li'), arxiv.Result.Author('Xindong Liang'), arxiv.Result.Author('Patrick Sebbah'), arxiv.Result.Author('Jin Zhang'), arxiv.Result.Author('Junhui Shi')]",2023-01-03 02:14:58+00:00,"A steerable parametric loudspeaker array is known for its directivity and
narrow beam width. However, it often suffers from the grating lobes due to
periodic array distributions. Here we propose the array configuration of
hyperuniform disorder, which is short-range random while correlated at large
scales, as a promising alternative distribution of acoustic antennas in phased
arrays. Angle-resolved measurements reveal that the proposed array suppresses
grating lobes and maintains a minimal radiation region in the vicinity of the
main lobe for the primary frequency waves. These distinctive emission features
benefit the secondary frequency wave in canceling the grating lobes regardless
of the frequencies of the primary waves. Besides that, the hyperuniform
disordered array is duplicatable, which facilitates extra-large array design
without any additional computational efforts.",,
Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling,"[arxiv.Result.Author('Amitay Sicherman'), arxiv.Result.Author('Yossi Adi')]",2023-01-02 10:36:40+00:00,"This work profoundly analyzes discrete self-supervised speech representations
through the eyes of Generative Spoken Language Modeling (GSLM). Following the
findings of such an analysis, we propose practical improvements to the discrete
unit for the GSLM. First, we start comprehending these units by analyzing them
in three axes: interpretation, visualization, and resynthesis. Our analysis
finds a high correlation between the speech units to phonemes and phoneme
families, while their correlation with speaker or gender is weaker.
Additionally, we found redundancies in the extracted units and claim that one
reason may be the units' context. Following this analysis, we propose a new,
unsupervised metric to measure unit redundancies. Finally, we use this metric
to develop new methods that improve the robustness of units clustering and show
significant improvement considering zero-resource speech metrics such as ABX.
Code and analysis tools are available under the following link.",,
Towards Voice Reconstruction from EEG during Imagined Speech,"[arxiv.Result.Author('Young-Eun Lee'), arxiv.Result.Author('Seo-Hyun Lee'), arxiv.Result.Author('Sang-Ho Kim'), arxiv.Result.Author('Seong-Whan Lee')]",2023-01-02 05:10:31+00:00,"Translating imagined speech from human brain activity into voice is a
challenging and absorbing research issue that can provide new means of human
communication via brain signals. Endeavors toward reconstructing speech from
brain activity have shown their potential using invasive measures of spoken
speech data, however, have faced challenges in reconstructing imagined speech.
In this paper, we propose NeuroTalk, which converts non-invasive brain signals
of imagined speech into the user's own voice. Our model was trained with spoken
speech EEG which was generalized to adapt to the domain of imagined speech,
thus allowing natural correspondence between the imagined speech and the voice
as a ground truth. In our framework, automatic speech recognition decoder
contributed to decomposing the phonemes of generated speech, thereby displaying
the potential of voice reconstruction from unseen words. Our results imply the
potential of speech synthesis from human EEG signals, not only from spoken
speech but also from the brain signals of imagined speech.","9 pages, 4 figures, accepted paper of AAAI 2023 in main track",
EmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine Learning Classification Methodologies,[arxiv.Result.Author('Fred W. Buhl')],2023-01-02 03:02:10+00:00,"Vocal Bursts -- short, non-speech vocalizations that convey emotions, such as
laughter, cries, sighs, moans, and groans -- are an often-overlooked aspect of
speech emotion recognition, but an important aspect of human vocal
communication. One barrier to study of these interesting vocalizations is a
lack of large datasets. I am pleased to introduce the EmoGator dataset, which
consists of 32,040 samples from 365 speakers, 16.91 hours of audio; each sample
classified into one of 30 distinct emotion categories by the speaker. Several
different approaches to construct classifiers to identify emotion categories
will be discussed, and directions for future research will be suggested. Data
set is available for download from https://github.com/fredbuhl/EmoGator.","12 pages, 4 tables, 2 figures",
Unsupervised Acoustic Scene Mapping Based on Acoustic Features and Dimensionality Reduction,"[arxiv.Result.Author('Idan Cohen'), arxiv.Result.Author('Ofir Lindenbaum'), arxiv.Result.Author('Sharon Gannot')]",2023-01-01 17:46:09+00:00,"Classical methods for acoustic scene mapping require the estimation of time
difference of arrival (TDOA) between microphones. Unfortunately, TDOA
estimation is very sensitive to reverberation and additive noise. We introduce
an unsupervised data-driven approach that exploits the natural structure of the
data. Our method builds upon local conformal autoencoders (LOCA) - an offline
deep learning scheme for learning standardized data coordinates from
measurements. Our experimental setup includes a microphone array that measures
the transmitted sound source at multiple locations across the acoustic
enclosure. We demonstrate that LOCA learns a representation that is isometric
to the spatial locations of the microphones. The performance of our method is
evaluated using a series of realistic simulations and compared with other
dimensionality-reduction schemes. We further assess the influence of
reverberation on the results of LOCA and show that it demonstrates considerable
robustness.",,
Quantum Representations of Sound: from mechanical waves to quantum circuits,"[arxiv.Result.Author('Paulo V. Itaborai'), arxiv.Result.Author('Eduardo R. Miranda')]",2023-01-01 17:10:30+00:00,"By the time of writing, quantum audio still is a very young area of study,
even within the quantum signal processing community. This chapter introduces
the state of the art in quantum audio and discusses methods for the quantum
representation of audio signals. Currently, no quantum representation strategy
claims to be the best one for audio applications. Each one presents advantages
and disadvantages. It can be argued that future quantum audio representation
schemes will make use of multiple strategies aimed at specific applications.
NOTE: This is an unedited abridged version of the pre-submission draft of a
chapter, with the same title, published in the book Quantum Computer Music:
Foundations, Methods and Advanced Concepts, by E. R. Miranda (pp. 223 - 274).
Please refer to the version in this book for application examples and a
discussion on sound synthesis methods based on quantum audio representation and
their potential for developing new types of musical instruments.
https://link.springer.com/book/10.1007/978-3-031-13909-3","29 pages,26 figures. Accompanying Python package is available:
  https://pypi.org/project/quantumaudio/",
Semantic Operator Prediction and Applications,[arxiv.Result.Author('Farshad Noravesh')],2023-01-01 13:20:57+00:00,"In the present paper, semantic parsing challenges are briefly introduced and
QDMR formalism in semantic parsing is implemented using sequence to sequence
model with attention but uses only part of speech(POS) as a representation of
words of a sentence to make the training as simple and as fast as possible and
also avoiding curse of dimensionality as well as overfitting. It is shown how
semantic operator prediction could be augmented with other models like the
CopyNet model or the recursive neural net model.",,
ExploreADV: Towards exploratory attack for Neural Networks,"[arxiv.Result.Author('Tianzuo Luo'), arxiv.Result.Author('Yuyi Zhong'), arxiv.Result.Author('Siaucheng Khoo')]",2023-01-01 07:17:03+00:00,"Although deep learning has made remarkable progress in processing various
types of data such as images, text and speech, they are known to be susceptible
to adversarial perturbations: perturbations specifically designed and added to
the input to make the target model produce erroneous output. Most of the
existing studies on generating adversarial perturbations attempt to perturb the
entire input indiscriminately. In this paper, we propose ExploreADV, a general
and flexible adversarial attack system that is capable of modeling regional and
imperceptible attacks, allowing users to explore various kinds of adversarial
examples as needed. We adapt and combine two existing boundary attack methods,
DeepFool and Brendel\&Bethge Attack, and propose a mask-constrained adversarial
attack system, which generates minimal adversarial perturbations under the
pixel-level constraints, namely ``mask-constraints''. We study different ways
of generating such mask-constraints considering the variance and importance of
the input features, and show that our adversarial attack system offers users
good flexibility to focus on sub-regions of inputs, explore imperceptible
perturbations and understand the vulnerability of pixels/regions to adversarial
attacks. We demonstrate our system to be effective based on extensive
experiments and user study.",,
Sample-Efficient Unsupervised Domain Adaptation of Speech Recognition Systems A case study for Modern Greek,"[arxiv.Result.Author('Georgios Paraskevopoulos'), arxiv.Result.Author('Theodoros Kouzelis'), arxiv.Result.Author('Georgios Rouvalis'), arxiv.Result.Author('Athanasios Katsamanis'), arxiv.Result.Author('Vassilis Katsouros'), arxiv.Result.Author('Alexandros Potamianos')]",2022-12-31 22:57:30+00:00,"Modern speech recognition systems exhibits rapid performance degradation
under domain shift. This issue is especially prevalent in data-scarce settings,
such as low-resource languages, where diversity of training data is limited. In
this work we propose M2DS2, a simple and sample-efficient finetuning strategy
for large pretrained speech models, based on mixed source and target domain
self-supervision. We find that including source domain self-supervision
stabilizes training and avoids mode collapse of the latent representations. For
evaluation, we collect HParl, a $120$ hour speech corpus for Greek, consisting
of plenary sessions in the Greek Parliament. We merge HParl with two popular
Greek corpora to create GREC-MD, a test-bed for multi-domain evaluation of
Greek ASR systems. In our experiments we find that, while other Unsupervised
Domain Adaptation baselines fail in this resource-constrained environment,
M2DS2 yields significant improvements for cross-domain adaptation, even when a
only a few hours of in-domain audio are available. When we relax the problem in
a weakly supervised setting, we find that independent adaptation for audio
using M2DS2 and language using simple LM augmentation techniques is
particularly effective, yielding word error rates comparable to the fully
supervised baselines.",,
Definition and clinical validation of Pain Patient States from high-dimensional mobile data: application to a chronic pain cohort,"[arxiv.Result.Author('Jenna M. Reinen'), arxiv.Result.Author('Carla Agurto'), arxiv.Result.Author('Guillermo Cecchi'), arxiv.Result.Author('Jeffrey L. Rogers'), arxiv.Result.Author('NAVITAS'), arxiv.Result.Author('ENVISION Studies Physician Author Group'), arxiv.Result.Author('Boston Scientific Research Scientists Consortium')]",2022-12-31 22:11:37+00:00,"The technical capacity to monitor patients with a mobile device has
drastically expanded, but data produced from this approach are often difficult
to interpret. We present a solution to produce a meaningful representation of
patient status from large, complex data streams, leveraging both a data-driven
approach, and use clinical knowledge to validate results. Data were collected
from a clinical trial enrolling chronic pain patients, and included
questionnaires, voice recordings, actigraphy, and standard health assessments.
The data were reduced using a clustering analysis. In an initial exploratory
analysis with only questionnaire data, we found up to 3 stable cluster
solutions that grouped symptoms on a positive to negative spectrum. Objective
features (actigraphy, speech) expanded the cluster solution granularity. Using
a 5 state solution with questionnaire and actigraphy data, we found significant
correlations between cluster properties and assessments of disability and
quality-of-life. The correlation coefficient values showed an ordinal
distinction, confirming the cluster ranking on a negative to positive spectrum.
This suggests we captured novel, distinct Pain Patient States with this
approach, even when multiple clusters were equated on pain magnitude. Relative
to using complex time courses of many variables, Pain Patient States holds
promise as an interpretable, useful, and actionable metric for a clinician or
caregiver to simplify and provide timely delivery of care.",,"2022 IEEE International Conference on Digital Health, 47-53"
Computational Charisma -- A Brick by Brick Blueprint for Building Charismatic Artificial Intelligence,"[arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Shahin Amiriparian'), arxiv.Result.Author('Anton Batliner'), arxiv.Result.Author('Alexander Gebhard'), arxiv.Result.Author('Maurice Gerzcuk'), arxiv.Result.Author('Vincent Karas'), arxiv.Result.Author('Alexander Kathan'), arxiv.Result.Author('Lennart Seizer'), arxiv.Result.Author('Johanna Löchner')]",2022-12-31 07:27:01+00:00,"Charisma is considered as one's ability to attract and potentially also
influence others. Clearly, there can be considerable interest from an
artificial intelligence's (AI) perspective to provide it with such skill.
Beyond, a plethora of use cases opens up for computational measurement of human
charisma, such as for tutoring humans in the acquisition of charisma, mediating
human-to-human conversation, or identifying charismatic individuals in big
social data. A number of models exist that base charisma on various dimensions,
often following the idea that charisma is given if someone could and would help
others. Examples include influence (could help) and affability (would help) in
scientific studies or power (could help), presence, and warmth (both would
help) as a popular concept. Modelling high levels in these dimensions for
humanoid robots or virtual agents, seems accomplishable. Beyond, also automatic
measurement appears quite feasible with the recent advances in the related
fields of Affective Computing and Social Signal Processing. Here, we,
thereforem present a blueprint for building machines that can appear
charismatic, but also analyse the charisma of others. To this end, we first
provide the psychological perspective including different models of charisma
and behavioural cues of it. We then switch to conversational charisma in spoken
language as an exemplary modality that is essential for human-human and
human-computer conversations. The computational perspective then deals with the
recognition and generation of charismatic behaviour by AI. This includes an
overview of the state of play in the field and the aforementioned blueprint. We
then name exemplary use cases of computational charismatic skills before
switching to ethical aspects and concluding this overview and perspective on
building charisma-enabled AI.",,
Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition,"[arxiv.Result.Author('Yukun Feng'), arxiv.Result.Author('Ming Tu'), arxiv.Result.Author('Rui Xia'), arxiv.Result.Author('Chuanzeng Huang'), arxiv.Result.Author('Yuxuan Wang')]",2022-12-30 22:26:57+00:00,"Recent studies have shown that using an external Language Model (LM) benefits
the end-to-end Automatic Speech Recognition (ASR). However, predicting tokens
that appear less frequently in the training set is still quite challenging. The
long-tail prediction problems have been widely studied in many applications,
but only been addressed by a few studies for ASR and LMs. In this paper, we
propose a new memory augmented lookup dictionary based Transformer architecture
for LM. The newly introduced lookup dictionary incorporates rich contextual
information in training set, which is vital to correctly predict long-tail
tokens. With intensive experiments on Chinese and English data sets, our
proposed method is proved to outperform the baseline Transformer LM by a great
margin on both word/character error rate and tail tokens error rate. This is
achieved without impact on the decoding efficiency. Overall, we demonstrate the
effectiveness of our proposed method in boosting the ASR decoding performance,
especially for long-tail tokens.",Submitted to ICASSP 2023,
Imitator: Personalized Speech-driven 3D Facial Animation,"[arxiv.Result.Author('Balamurugan Thambiraja'), arxiv.Result.Author('Ikhsanul Habibie'), arxiv.Result.Author('Sadegh Aliakbarian'), arxiv.Result.Author('Darren Cosker'), arxiv.Result.Author('Christian Theobalt'), arxiv.Result.Author('Justus Thies')]",2022-12-30 19:00:02+00:00,"Speech-driven 3D facial animation has been widely explored, with applications
in gaming, character animation, virtual reality, and telepresence systems.
State-of-the-art methods deform the face topology of the target actor to sync
the input audio without considering the identity-specific speaking style and
facial idiosyncrasies of the target actor, thus, resulting in unrealistic and
inaccurate lip movements. To address this, we present Imitator, a speech-driven
facial expression synthesis method, which learns identity-specific details from
a short input video and produces novel facial expressions matching the
identity-specific speaking style and facial idiosyncrasies of the target actor.
Specifically, we train a style-agnostic transformer on a large facial
expression dataset which we use as a prior for audio-driven facial expressions.
Based on this prior, we optimize for identity-specific speaking style based on
a short reference video. To train the prior, we introduce a novel loss function
based on detected bilabial consonants to ensure plausible lip closures and
consequently improve the realism of the generated expressions. Through detailed
experiments and a user study, we show that our approach produces temporally
coherent facial expressions from input audio while preserving the speaking
style of the target actors.",https://youtu.be/JhXTdjiUCUw,
Blind Restoration of Real-World Audio by 1D Operational GANs,"[arxiv.Result.Author('Turker Ince'), arxiv.Result.Author('Serkan Kiranyaz'), arxiv.Result.Author('Ozer Can Devecioglu'), arxiv.Result.Author('Muhammad Salman Khan'), arxiv.Result.Author('Muhammad Chowdhury'), arxiv.Result.Author('Moncef Gabbouj')]",2022-12-30 10:11:57+00:00,"Objective: Despite numerous studies proposed for audio restoration in the
literature, most of them focus on an isolated restoration problem such as
denoising or dereverberation, ignoring other artifacts. Moreover, assuming a
noisy or reverberant environment with limited number of fixed
signal-to-distortion ratio (SDR) levels is a common practice. However,
real-world audio is often corrupted by a blend of artifacts such as
reverberation, sensor noise, and background audio mixture with varying types,
severities, and duration. In this study, we propose a novel approach for blind
restoration of real-world audio signals by Operational Generative Adversarial
Networks (Op-GANs) with temporal and spectral objective metrics to enhance the
quality of restored audio signal regardless of the type and severity of each
artifact corrupting it. Methods: 1D Operational-GANs are used with generative
neuron model optimized for blind restoration of any corrupted audio signal.
Results: The proposed approach has been evaluated extensively over the
benchmark TIMIT-RAR (speech) and GTZAN-RAR (non-speech) datasets corrupted with
a random blend of artifacts each with a random severity to mimic real-world
audio signals. Average SDR improvements of over 7.2 dB and 4.9 dB are achieved,
respectively, which are substantial when compared with the baseline methods.
Significance: This is a pioneer study in blind audio restoration with the
unique capability of direct (time-domain) restoration of real-world audio
whilst achieving an unprecedented level of performance for a wide SDR range and
artifact types. Conclusion: 1D Op-GANs can achieve robust and computationally
effective real-world audio restoration with significantly improved performance.
The source codes and the generated real-world audio datasets are shared
publicly with the research community in a dedicated GitHub repository1.",,
Defense Against Adversarial Attacks on Audio DeepFake Detection,"[arxiv.Result.Author('Piotr Kawa'), arxiv.Result.Author('Marcin Plata'), arxiv.Result.Author('Piotr Syga')]",2022-12-30 08:41:06+00:00,"Audio DeepFakes are artificially generated utterances created using deep
learning methods with the main aim to fool the listeners, most of such audio is
highly convincing. Their quality is sufficient to pose a serious threat in
terms of security and privacy, such as the reliability of news or defamation.
To prevent the threats, multiple neural networks-based methods to detect
generated speech have been proposed. In this work, we cover the topic of
adversarial attacks, which decrease the performance of detectors by adding
superficial (difficult to spot by a human) changes to input data. Our
contribution contains evaluating the robustness of 3 detection architectures
against adversarial attacks in two scenarios (white-box and using
transferability mechanism) and enhancing it later by the use of adversarial
training performed by our novel adaptive training method.",Submitted to ICASSP 2023,
ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech,"[arxiv.Result.Author('Zehua Chen'), arxiv.Result.Author('Yihan Wu'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Jiawei Chen'), arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Yang Cui'), arxiv.Result.Author('Ke Wang'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Danilo Mandic')]",2022-12-30 02:31:35+00:00,"Denoising Diffusion Probabilistic Models (DDPMs) are emerging in
text-to-speech (TTS) synthesis because of their strong capability of generating
high-fidelity samples. However, their iterative refinement process in
high-dimensional data space results in slow inference speed, which restricts
their application in real-time systems. Previous works have explored speeding
up by minimizing the number of inference steps but at the cost of sample
quality. In this work, to improve the inference speed for DDPM-based TTS model
while achieving high sample quality, we propose ResGrad, a lightweight
diffusion model which learns to refine the output spectrogram of an existing
TTS model (e.g., FastSpeech 2) by predicting the residual between the model
output and the corresponding ground-truth speech. ResGrad has several
advantages: 1) Compare with other acceleration methods for DDPM which need to
synthesize speech from scratch, ResGrad reduces the complexity of task by
changing the generation target from ground-truth mel-spectrogram to the
residual, resulting into a more lightweight model and thus a smaller real-time
factor. 2) ResGrad is employed in the inference process of the existing TTS
model in a plug-and-play way, without re-training this model. We verify ResGrad
on the single-speaker dataset LJSpeech and two more challenging datasets with
multiple speakers (LibriTTS) and high sampling rate (VCTK). Experimental
results show that in comparison with other speed-up methods of DDPMs: 1)
ResGrad achieves better sample quality with the same inference speed measured
by real-time factor; 2) with similar speech quality, ResGrad synthesizes speech
faster than baseline methods by more than 10 times. Audio samples are available
at https://resgrad1.github.io/.","13 pages, 5 figures",
Multi-modal deep learning system for depression and anxiety detection,"[arxiv.Result.Author('Brian Diep'), arxiv.Result.Author('Marija Stanojevic'), arxiv.Result.Author('Jekaterina Novikova')]",2022-12-30 00:02:58+00:00,"Traditional screening practices for anxiety and depression pose an impediment
to monitoring and treating these conditions effectively. However, recent
advances in NLP and speech modelling allow textual, acoustic, and hand-crafted
language-based features to jointly form the basis of future mental health
screening and condition detection. Speech is a rich and readily available
source of insight into an individual's cognitive state and by leveraging
different aspects of speech, we can develop new digital biomarkers for
depression and anxiety. To this end, we propose a multi-modal system for the
screening of depression and anxiety from self-administered speech tasks. The
proposed model integrates deep-learned features from audio and text, as well as
hand-crafted features that are informed by clinically-validated domain
knowledge. We find that augmenting hand-crafted features with deep-learned
features improves our overall classification F1 score comparing to a baseline
of hand-crafted features alone from 0.58 to 0.63 for depression and from 0.54
to 0.57 for anxiety. The findings of our work suggest that speech-based
biomarkers for depression and anxiety hold significant promise in the future of
digital health.",accepted to the PAI4MH workshop at NeurIPS 2022,
Transformers in Action Recognition: A Review on Temporal Modeling,"[arxiv.Result.Author('Elham Shabaninia'), arxiv.Result.Author('Hossein Nezamabadi-pour'), arxiv.Result.Author('Fatemeh Shafizadegan')]",2022-12-29 11:03:19+00:00,"In vision-based action recognition, spatio-temporal features from different
modalities are used for recognizing activities. Temporal modeling is a long
challenge of action recognition. However, there are limited methods such as
pre-computed motion features, three-dimensional (3D) filters, and recurrent
neural networks (RNN) for modeling motion information in deep-based approaches.
Recently, transformers success in modeling long-range dependencies in natural
language processing (NLP) tasks has gotten great attention from other domains;
including speech, image, and video, to rely entirely on self-attention without
using sequence-aligned RNNs or convolutions. Although the application of
transformers to action recognition is relatively new, the amount of research
proposed on this topic within the last few years is astounding. This paper
especially reviews recent progress in deep learning methods for modeling
temporal variations. It focuses on action recognition methods that use
transformers for temporal modeling, discussing their main features, used
modalities, and identifying opportunities and challenges for future research.",,
StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models,"[arxiv.Result.Author('Yinghao Aaron Li'), arxiv.Result.Author('Cong Han'), arxiv.Result.Author('Nima Mesgarani')]",2022-12-29 08:56:20+00:00,"One-shot voice conversion (VC) aims to convert speech from any source speaker
to an arbitrary target speaker with only a few seconds of reference speech from
the target speaker. This relies heavily on disentangling the speaker's identity
and speech content, a task that still remains challenging. Here, we propose a
novel approach to learning disentangled speech representation by transfer
learning from style-based text-to-speech (TTS) models. With cycle consistent
and adversarial training, the style-based TTS models can perform
transcription-guided one-shot VC with high fidelity and similarity. By learning
an additional mel-spectrogram encoder through a teacher-student knowledge
transfer and novel data augmentation scheme, our approach results in
disentangled speech representation without needing the input text. The
subjective evaluation shows that our approach can significantly outperform the
previous state-of-the-art one-shot voice conversion models in both naturalness
and similarity.",SLT 2022,
Macro-block dropout for improved regularization in training end-to-end speech recognition models,"[arxiv.Result.Author('Chanwoo Kim'), arxiv.Result.Author('Sathish Indurti'), arxiv.Result.Author('Jinhwan Park'), arxiv.Result.Author('Wonyong Sung')]",2022-12-29 02:09:49+00:00,"This paper proposes a new regularization algorithm referred to as macro-block
dropout. The overfitting issue has been a difficult problem in training large
neural network models. The dropout technique has proven to be simple yet very
effective for regularization by preventing complex co-adaptations during
training. In our work, we define a macro-block that contains a large number of
units from the input to a Recurrent Neural Network (RNN). Rather than applying
dropout to each unit, we apply random dropout to each macro-block. This
algorithm has the effect of applying different drop out rates for each layer
even if we keep a constant average dropout rate, which has better
regularization effects. In our experiments using Recurrent Neural
Network-Transducer (RNN-T), this algorithm shows relatively 4.30 % and 6.13 %
Word Error Rates (WERs) improvement over the conventional dropout on
LibriSpeech test-clean and test-other. With an Attention-based Encoder-Decoder
(AED) model, this algorithm shows relatively 4.36 % and 5.85 % WERs improvement
over the conventional dropout on the same test sets.","Accepted for presentation at The 2022 IEEE Spoken Language Technology
  Workshop (SLT 2022)",
Leveraging World Knowledge in Implicit Hate Speech Detection,[arxiv.Result.Author('Jessica Lin')],2022-12-28 21:23:55+00:00,"While much attention has been paid to identifying explicit hate speech,
implicit hateful expressions that are disguised in coded or indirect language
are pervasive and remain a major challenge for existing hate speech detection
systems. This paper presents the first attempt to apply Entity Linking (EL)
techniques to both explicit and implicit hate speech detection, where we show
that such real world knowledge about entity mentions in a text does help models
better detect hate speech, and the benefit of adding it into the model is more
pronounced when explicit entity triggers (e.g., rally, KKK) are present. We
also discuss cases where real world knowledge does not add value to hate speech
detection, which provides more insights into understanding and modeling the
subtleties of hate speech.",,
Distributed Active Noise Control System Based on a Block Diffusion FxLMS Algorithm with Bidirectional Communication,"[arxiv.Result.Author('Tianyou Li'), arxiv.Result.Author('Hongji Duan'), arxiv.Result.Author('Sipei Zhao'), arxiv.Result.Author('Jing Lu'), arxiv.Result.Author('Ian S. Burnett')]",2022-12-28 10:39:16+00:00,"Recently, distributed active noise control systems based on diffusion
adaptation have attracted significant research interest due to their balance
between computational complexity and stability compared to conventional
centralized and decentralized adaptation schemes. However, the existing
diffusion FxLMS algorithm employs node-specific adaptation and
neighborhood-wide combination, and assumes that the control filters of neighbor
nodes are similar to each other. This assumption is not true in practical
applications, and it leads to inferior performance to the centralized
controller approach. In contrast, this paper proposes a Block Diffusion FxLMS
algorithm with bidirectional communication, which uses neighborhood-wide
adaptation and node-specific combination to update the control filters.
Simulation results validate that the proposed algorithm converges to the
solution of the centralized controller with reduced computational burden.",,
Learning to Detect Noisy Labels Using Model-Based Features,"[arxiv.Result.Author('Zhihao Wang'), arxiv.Result.Author('Zongyu Lin'), arxiv.Result.Author('Peiqi Liu'), arxiv.Result.Author('Guidong ZHeng'), arxiv.Result.Author('Junjie Wen'), arxiv.Result.Author('Xianxin Chen'), arxiv.Result.Author('Yujun Chen'), arxiv.Result.Author('Zhilin Yang')]",2022-12-28 10:12:13+00:00,"Label noise is ubiquitous in various machine learning scenarios such as
self-labeling with model predictions and erroneous data annotation. Many
existing approaches are based on heuristics such as sample losses, which might
not be flexible enough to achieve optimal solutions. Meta learning based
methods address this issue by learning a data selection function, but can be
hard to optimize. In light of these pros and cons, we propose
Selection-Enhanced Noisy label Training (SENT) that does not rely on meta
learning while having the flexibility of being data-driven. SENT transfers the
noise distribution to a clean set and trains a model to distinguish noisy
labels from clean ones using model-based features. Empirically, on a wide range
of tasks including text classification and speech recognition, SENT improves
performance over strong baselines under the settings of self-training and label
corruption.",,
Singing Voice Synthesis Based on a Musical Note Position-Aware Attention Mechanism,"[arxiv.Result.Author('Yukiya Hono'), arxiv.Result.Author('Kei Hashimoto'), arxiv.Result.Author('Yoshihiko Nankaku'), arxiv.Result.Author('Keiichi Tokuda')]",2022-12-28 05:24:23+00:00,"This paper proposes a novel sequence-to-sequence (seq2seq) model with a
musical note position-aware attention mechanism for singing voice synthesis
(SVS). A seq2seq modeling approach that can simultaneously perform acoustic and
temporal modeling is attractive. However, due to the difficulty of the temporal
modeling of singing voices, many recent SVS systems with an
encoder-decoder-based model still rely on explicitly on duration information
generated by additional modules. Although some studies perform simultaneous
modeling using seq2seq models with an attention mechanism, they have
insufficient robustness against temporal modeling. The proposed attention
mechanism is designed to estimate the attention weights by considering the
rhythm given by the musical score. Furthermore, several techniques are also
introduced to improve the modeling performance of the singing voice.
Experimental results indicated that the proposed model is effective in terms of
both naturalness and robustness of timing.","5 pages, 4 figures, 2 tables, submitted to ICASSP 2023",
Annealing Double-Head: An Architecture for Online Calibration of Deep Neural Networks,"[arxiv.Result.Author('Erdong Guo'), arxiv.Result.Author('David Draper'), arxiv.Result.Author('Maria De Iorio')]",2022-12-27 21:21:58+00:00,"Model calibration, which is concerned with how frequently the model predicts
correctly, not only plays a vital part in statistical model design, but also
has substantial practical applications, such as optimal decision-making in the
real world. However, it has been discovered that modern deep neural networks
are generally poorly calibrated due to the overestimation (or underestimation)
of predictive confidence, which is closely related to overfitting. In this
paper, we propose Annealing Double-Head, a simple-to-implement but highly
effective architecture for calibrating the DNN during training. To be precise,
we construct an additional calibration head-a shallow neural network that
typically has one latent layer-on top of the last latent layer in the normal
model to map the logits to the aligned confidence. Furthermore, a simple
Annealing technique that dynamically scales the logits by calibration head in
training procedure is developed to improve its performance. Under both the
in-distribution and distributional shift circumstances, we exhaustively
evaluate our Annealing Double-Head architecture on multiple pairs of
contemporary DNN architectures and vision and speech datasets. We demonstrate
that our method achieves state-of-the-art model calibration performance without
post-processing while simultaneously providing comparable predictive accuracy
in comparison to other recently proposed calibration methods on a range of
learning tasks.","Revised Preprint. 19 pages, 10 figures, 4 tables. Typos fixed, and
  references added",
Voice conversion with limited data and limitless data augmentations,"[arxiv.Result.Author('Olga Slizovskaia'), arxiv.Result.Author('Jordi Janer'), arxiv.Result.Author('Pritish Chandna'), arxiv.Result.Author('Oscar Mayor')]",2022-12-27 18:37:21+00:00,"Applying changes to an input speech signal to change the perceived speaker of
speech to a target while maintaining the content of the input is a challenging
but interesting task known as Voice conversion (VC). Over the last few years,
this task has gained significant interest where most systems use data-driven
machine learning models. Doing the conversion in a low-latency real-world
scenario is even more challenging constrained by the availability of
high-quality data. Data augmentations such as pitch shifting and noise addition
are often used to increase the amount of data used for training machine
learning based models for this task. In this paper we explore the efficacy of
common data augmentation techniques for real-time voice conversion and
introduce novel techniques for data augmentation based on audio and voice
transformation effects as well. We evaluate the conversions for both male and
female target speakers using objective and subjective evaluation methodologies.",,
Countering Malicious Content Moderation Evasion in Online Social Networks: Simulation and Detection of Word Camouflage,"[arxiv.Result.Author('Álvaro Huertas-García'), arxiv.Result.Author('Alejandro Martín'), arxiv.Result.Author('Javier Huertas Tato'), arxiv.Result.Author('David Camacho')]",2022-12-27 16:08:49+00:00,"Content moderation is the process of screening and monitoring user-generated
content online. It plays a crucial role in stopping content resulting from
unacceptable behaviors such as hate speech, harassment, violence against
specific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to
mention some few, in Online Social Platforms. These platforms make use of a
plethora of tools to detect and manage malicious information; however,
malicious actors also improve their skills, developing strategies to surpass
these barriers and continuing to spread misleading information. Twisting and
camouflaging keywords are among the most used techniques to evade platform
content moderation systems. In response to this recent ongoing issue, this
paper presents an innovative approach to address this linguistic trend in
social networks through the simulation of different content evasion techniques
and a multilingual Transformer model for content evasion detection. In this
way, we share with the rest of the scientific community a multilingual public
tool, named ""pyleetspeak"" to generate/simulate in a customizable way the
phenomenon of content evasion through automatic word camouflage and a
multilingual Named-Entity Recognition (NER) Transformer-based model tuned for
its recognition and detection. The multilingual NER model is evaluated in
different textual scenarios, detecting different types and mixtures of
camouflage techniques, achieving an overall weighted F1 score of 0.8795. This
article contributes significantly to countering malicious information by
developing multilingual tools to simulate and detect new methods of evasion of
content on social networks, making the fight against information disorders more
effective.","21 pages, 4 figures, 6 tables, Fast Track from IDEAL 21 Conference",
Personality Detection of Applicants And Employees Using K-mode Algorithm And Ocean Model,"[arxiv.Result.Author('Binisha Mohan'), arxiv.Result.Author('Dinju Vattavayalil Joseph'), arxiv.Result.Author('Bharat Plavelil Subhash')]",2022-12-27 11:00:58+00:00,"The combination of conduct, emotion, motivation, and thinking is referred to
as personality. To shortlist candidates more effectively, many organizations
rely on personality predictions. The firm can hire or pick the best candidate
for the desired job description by grouping applicants based on the necessary
personality preferences. A model is created to identify applicants' personality
types so that employers may find qualified candidates by examining a person's
facial expression, speech intonation, and resume. Additionally, the paper
emphasises detecting the changes in employee behaviour. Employee attitudes and
behaviour towards each set of questions are being examined and analysed. Here,
the K-Modes clustering method is used to predict employee well-being, including
job pressure, the working environment, and relationships with peers, utilizing
the OCEAN Model and the CNN algorithm in the AVI-AI administrative system.
Findings imply that AVIs can be used for efficient candidate screening with an
AI decision agent. The study of the specific field is beyond the current
explorations and needed to be expanded with deeper models and new
configurations that can patch extremely complex operations.",,
"Audiovisual Database with 360 Video and Higher-Order Ambisonics Audio for Perception, Cognition, Behavior, and QoE Evaluation Research","[arxiv.Result.Author('Thomas Robotham'), arxiv.Result.Author('Ashutosh Singla'), arxiv.Result.Author('Olli S. Rummukainen'), arxiv.Result.Author('Alexander Raake'), arxiv.Result.Author('Emanuël A. P. Habets')]",2022-12-27 10:47:08+00:00,"Research into multi-modal perception, human cognition, behavior, and
attention can benefit from high-fidelity content that may recreate
real-life-like scenes when rendered on head-mounted displays. Moreover, aspects
of audiovisual perception, cognitive processes, and behavior may complement
questionnaire-based Quality of Experience (QoE) evaluation of interactive
virtual environments. Currently, there is a lack of high-quality open-source
audiovisual databases that can be used to evaluate such aspects or systems
capable of reproducing high-quality content. With this paper, we provide a
publicly available audiovisual database consisting of twelve scenes capturing
real-life nature and urban environments with a video resolution of 7680x3840 at
60 frames-per-second and with 4th-order Ambisonics audio. These 360 video
sequences, with an average duration of 60 seconds, represent real-life settings
for systematically evaluating various dimensions of uni-/multi-modal
perception, cognition, behavior, and QoE. The paper provides details of the
scene requirements, recording approach, and scene descriptions. The database
provides high-quality reference material with a balanced focus on auditory and
visual sensory information. The database will be continuously updated with
additional scenes and further metadata such as human ratings and saliency
information.","6 pages, 2 figures, accepted and presented at the 2022 14th
  International Conference on Quality of Multimedia Experience (QoMEX).
  Database is publicly accessible at https://qoevave.github.io/database/",
MixupE: Understanding and Improving Mixup from Directional Derivative Perspective,"[arxiv.Result.Author('Vikas Verma'), arxiv.Result.Author('Sarthak Mittal'), arxiv.Result.Author('Wai Hoh Tang'), arxiv.Result.Author('Hieu Pham'), arxiv.Result.Author('Juho Kannala'), arxiv.Result.Author('Yoshua Bengio'), arxiv.Result.Author('Arno Solin'), arxiv.Result.Author('Kenji Kawaguchi')]",2022-12-27 07:03:52+00:00,"Mixup is a popular data augmentation technique for training deep neural
networks where additional samples are generated by linearly interpolating pairs
of inputs and their labels. This technique is known to improve the
generalization performance in many learning paradigms and applications. In this
work, we first analyze Mixup and show that it implicitly regularizes infinitely
many directional derivatives of all orders. We then propose a new method to
improve Mixup based on the novel insight. To demonstrate the effectiveness of
the proposed method, we conduct experiments across various domains such as
images, tabular data, speech, and graphs. Our results show that the proposed
method improves Mixup across various datasets using a variety of architectures,
for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1
accuracy.",,
Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation,"[arxiv.Result.Author('Tomer Wullach'), arxiv.Result.Author('Shlomo E. Chazan')]",2022-12-27 06:42:26+00:00,"Automatic Speech Recognition (ASR) systems frequently use a search-based
decoding strategy aiming to find the best attainable transcript by considering
multiple candidates. One prominent speech recognition decoding heuristic is
beam search, which seeks the transcript with the greatest likelihood computed
using the predicted distribution. While showing substantial performance gains
in various tasks, beam search loses some of its effectiveness when the
predicted probabilities are highly confident, i.e., the predicted distribution
is massed for a single or very few classes. We show that recently proposed
Self-Supervised Learning (SSL)-based ASR models tend to yield exceptionally
confident predictions that may hamper beam search from truly considering a
diverse set of candidates. We perform a layer analysis to reveal and visualize
how predictions evolve, and propose a decoding procedure that improves the
performance of fine-tuned ASR models. Our proposed approach does not require
further training beyond the original fine-tuning, nor additional model
parameters. In fact, we find that our proposed method requires significantly
less inference computation than current approaches. We propose aggregating the
top M layers, potentially leveraging useful information encoded in intermediate
layers, and relaxing model confidence. We demonstrate the effectiveness of our
approach by conducting an empirical study on varying amounts of labeled
resources and different model sizes, showing consistent improvements in
particular when applied to low-resource scenarios.",Accepted to AAAI 2023,
Feature Selection Approaches for Optimising Music Emotion Recognition Methods,"[arxiv.Result.Author('Le Cai'), arxiv.Result.Author('Sam Ferguson'), arxiv.Result.Author('Haiyan Lu'), arxiv.Result.Author('Gengfa Fang')]",2022-12-27 05:55:34+00:00,"The high feature dimensionality is a challenge in music emotion recognition.
There is no common consensus on a relation between audio features and emotion.
The MER system uses all available features to recognize emotion; however, this
is not an optimal solution since it contains irrelevant data acting as noise.
In this paper, we introduce a feature selection approach to eliminate redundant
features for MER. We created a Selected Feature Set (SFS) based on the feature
selection algorithm (FSA) and benchmarked it by training with two models,
Support Vector Regression (SVR) and Random Forest (RF) and comparing them
against with using the Complete Feature Set (CFS). The result indicates that
the performance of MER has improved for both Random Forest (RF) and Support
Vector Regression (SVR) models by using SFS. We found using FSA can improve
performance in all scenarios, and it has potential benefits for model
efficiency and stability for MER task.",,
Two Modes of Nonmonotonic Consequence,[arxiv.Result.Author('Alexei Muravitsky')],2022-12-27 04:07:29+00:00,"This is the text of my speech at the Logica Universalis webinar, which took
place on May 11, 2022.
  I discuss two ways to implement a semantic approach to nonmonotonic
consequence relations in an arbitrary propositional language. For one
particular language, we also discuss the proof-theoretic framework that we
connect with this semantic approach.",20 pages,
Skit-S2I: An Indian Accented Speech to Intent dataset,"[arxiv.Result.Author('Shangeth Rajaa'), arxiv.Result.Author('Swaraj Dalmia'), arxiv.Result.Author('Kumarmanas Nethil')]",2022-12-26 05:10:43+00:00,"Conventional conversation assistants extract text transcripts from the speech
signal using automatic speech recognition (ASR) and then predict intent from
the transcriptions. Using end-to-end spoken language understanding (SLU), the
intents of the speaker are predicted directly from the speech signal without
requiring intermediate text transcripts. As a result, the model can optimize
directly for intent classification and avoid cascading errors from ASR. The
end-to-end SLU system also helps in reducing the latency of the intent
prediction model. Although many datasets are available publicly for
text-to-intent tasks, the availability of labeled speech-to-intent datasets is
limited, and there are no datasets available in the Indian accent. In this
paper, we release the Skit-S2I dataset, the first publicly available
Indian-accented SLU dataset in the banking domain in a conversational tonality.
We experiment with multiple baselines, compare different pretrained speech
encoder's representations, and find that SSL pretrained representations perform
slightly better than ASR pretrained representations lacking prosodic features
for speech-to-intent classification. The dataset and baseline code is available
at \url{https://github.com/skit-ai/speech-to-intent-dataset}",,
Blind estimation of room acoustic parameters from speech signals based on extended model of room impulse response,"[arxiv.Result.Author('Lijun Wang'), arxiv.Result.Author('Suradej Duangpummet'), arxiv.Result.Author('Masashi Unoki')]",2022-12-26 04:28:02+00:00,"The speech transmission index (STI) and room acoustic parameters (RAPs),
which are derived from a room impulse response (RIR), such as reverberation
time and early decay time, are essential to assess speech transmission and to
predict the listening difficulty in a sound field. Since it is difficult to
measure RIR in daily occupied spaces, simultaneous blind estimation of STI and
RAPs must be resolved as it is an imperative and challenging issue. This paper
proposes a deterministic method for blindly estimating STI and five RAPs on the
basis of an RIR stochastic model that approximates an unknown RIR. The proposed
method formulates a temporal power envelope of a reverberant speech signal to
obtain the optimal parameters for the RIR model. Simulations were conducted to
evaluate STI and RAPs from observed reverberant speech signals. The
root-mean-square errors between the estimated and ground-truth results were
used to comparatively evaluate the proposed method with the previous method.
The results showed that the proposed method can estimate STI and RAPs
effectively without any training.","5-pages, 3 figures, 2 tables",
Technical Evaluation of HoloLens for Multimedia: A First Look,"[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Haiwei Dong'), arxiv.Result.Author('Longyu Zhang'), arxiv.Result.Author('Abdulmotaleb El Saddik')]",2022-12-25 14:16:49+00:00,"A recently released cutting-edge AR device, Microsoft HoloLens, has attracted
considerable attention with its advanced capabilities. In this article, we
report the design and execution of a series of experiments to quantitatively
evaluate HoloLens' performance in head localization, real environment
reconstruction, spatial mapping, hologram visualization, and speech
recognition.",,"IEEE Multimedia, vol. 25, no. 4, pp. 8-18, 2018"
MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning,"[arxiv.Result.Author('Luke Gessler'), arxiv.Result.Author('Amir Zeldes')]",2022-12-23 18:18:20+00:00,"Transformer language models (TLMs) are critical for most NLP tasks, but they
are difficult to create for low-resource languages because of how much
pretraining data they require. In this work, we investigate two techniques for
training monolingual TLMs in a low-resource setting: greatly reducing TLM size,
and complementing the masked language modeling objective with two
linguistically rich supervised tasks (part-of-speech tagging and dependency
parsing). Results from 7 diverse languages indicate that our model, MicroBERT,
is able to produce marked improvements in downstream task evaluations relative
to a typical monolingual TLM pretraining approach. Specifically, we find that
monolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%
for NER F1 compared to a multilingual baseline, mBERT, while having less than
1% of its parameter count. We conclude reducing TLM parameter count and using
labeled data for pretraining low-resource TLMs can yield large quality benefits
and in some cases produce models that outperform multilingual approaches.","Presented at MRL at EMNLP 2022 in Abu Dhabi. Code at
  https://github.com/lgessler/microbert and models at
  https://huggingface.co/lgessler",
Fractal Patterns in Music,"[arxiv.Result.Author('John McDonough'), arxiv.Result.Author('Andrzej Herczyński')]",2022-12-23 17:41:25+00:00,"If our aesthetic preferences are affected by fractal geometry of nature,
scaling regularities would be expected to appear in all art forms, including
music. While a variety of statistical tools have been proposed to analyze time
series in sound, no consensus has as yet emerged regarding the most meaningful
measure of complexity in music, or how to discern fractal patterns in
compositions in the first place. Here we offer a new approach based on
self-similarity of the melodic lines recurring at various temporal scales. In
contrast to the statistical analyses advanced in recent literature, the
proposed method does not depend on averaging within time-windows and is
distinctively local. The corresponding definition of the fractal dimension is
based on the temporal scaling hierarchy and depends on the tonal contours of
the musical motifs. The new concepts are tested on musical 'renditions' of the
Cantor Set and Koch Curve, and then applied to a number of carefully selected
masterful compositions spanning five centuries of music making.","27 pages, 12 figures",
Large Raw Emotional Dataset with Aggregation Mechanism,"[arxiv.Result.Author('Vladimir Kondratenko'), arxiv.Result.Author('Artem Sokolov'), arxiv.Result.Author('Nikolay Karpov'), arxiv.Result.Author('Oleg Kutuzov'), arxiv.Result.Author('Nikita Savushkin'), arxiv.Result.Author('Fyodor Minkin')]",2022-12-23 11:31:02+00:00,"We present a new data set for speech emotion recognition (SER) tasks called
Dusha. The corpus contains approximately 350 hours of data, more than 300 000
audio recordings with Russian speech and their transcripts. Therefore it is the
biggest open bi-modal data collection for SER task nowadays. It is annotated
using a crowd-sourcing platform and includes two subsets: acted and real-life.
Acted subset has a more balanced class distribution than the unbalanced
real-life part consisting of audio podcasts. So the first one is suitable for
model pre-training, and the second is elaborated for fine-tuning purposes,
model approbation, and validation. This paper describes pre-processing routine,
annotation, and experiment with a baseline model to demonstrate some actual
metrics which could be obtained with the Dusha data set.","6 pages, 1 figures, submitted to ICASSP 2023",
EarSpy: Spying Caller Speech and Identity through Tiny Vibrations of Smartphone Ear Speakers,"[arxiv.Result.Author('Ahmed Tanvir Mahdad'), arxiv.Result.Author('Cong Shi'), arxiv.Result.Author('Zhengkun Ye'), arxiv.Result.Author('Tianming Zhao'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Yingying Chen'), arxiv.Result.Author('Nitesh Saxena')]",2022-12-23 05:05:09+00:00,"Eavesdropping from the user's smartphone is a well-known threat to the user's
safety and privacy. Existing studies show that loudspeaker reverberation can
inject speech into motion sensor readings, leading to speech eavesdropping.
While more devastating attacks on ear speakers, which produce much smaller
scale vibrations, were believed impossible to eavesdrop with zero-permission
motion sensors. In this work, we revisit this important line of reach. We
explore recent trends in smartphone manufacturers that include extra/powerful
speakers in place of small ear speakers, and demonstrate the feasibility of
using motion sensors to capture such tiny speech vibrations. We investigate the
impacts of these new ear speakers on built-in motion sensors and examine the
potential to elicit private speech information from the minute vibrations. Our
designed system EarSpy can successfully detect word regions, time, and
frequency domain features and generate a spectrogram for each word region. We
train and test the extracted data using classical machine learning algorithms
and convolutional neural networks. We found up to 98.66% accuracy in gender
detection, 92.6% detection in speaker detection, and 56.42% detection in digit
detection (which is 5X more significant than the random selection (10%)). Our
result unveils the potential threat of eavesdropping on phone conversations
from ear speakers using motion sensors.",,
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,"[arxiv.Result.Author('William Brannon'), arxiv.Result.Author('Yogesh Virkar'), arxiv.Result.Author('Brian Thompson')]",2022-12-23 04:12:52+00:00,"We investigate how humans perform the task of dubbing video content from one
language into another, leveraging a novel corpus of 319.57 hours of video from
54 professionally produced titles. This is the first such large-scale study we
are aware of. The results challenge a number of assumptions commonly made in
both qualitative literature on human dubbing and machine-learning literature on
automatic dubbing, arguing for the importance of vocal naturalness and
translation quality over commonly emphasized isometric (character length) and
lip-sync constraints, and for a more qualified view of the importance of
isochronic (timing) constraints. We also find substantial influence of the
source-side audio on human dubs through channels other than the words of the
translation, pointing to the need for research on ways to preserve speech
characteristics, as well as semantic transfer such as emphasis/emotion, in
automatic dubbing systems.",Accepted at TACL. pre-MIT Press publication version,
Pushing the performances of ASR models on English and Spanish accents,"[arxiv.Result.Author('Pooja Chitkara'), arxiv.Result.Author('Morgane Riviere'), arxiv.Result.Author('Jade Copet'), arxiv.Result.Author('Frank Zhang'), arxiv.Result.Author('Yatharth Saraf')]",2022-12-22 21:48:29+00:00,"Speech to text models tend to be trained and evaluated against a single
target accent. This is especially true for English for which native speakers
from the United States became the main benchmark. In this work, we are going to
show how two simple methods: pre-trained embeddings and auxiliary
classification losses can improve the performance of ASR systems. We are
looking for upgrades as universal as possible and therefore we will explore
their impact on several models architectures and several languages.",,
HMM-based data augmentation for E2E systems for building conversational speech synthesis systems,"[arxiv.Result.Author('Ishika Gupta'), arxiv.Result.Author('Anusha Prakash'), arxiv.Result.Author('Jom Kuriakose'), arxiv.Result.Author('Hema A. Murthy')]",2022-12-22 18:59:36+00:00,"This paper proposes an approach to build a high-quality text-to-speech (TTS)
system for technical domains using data augmentation. An end-to-end (E2E)
system is trained on hidden Markov model (HMM) based synthesized speech and
further fine-tuned with studio-recorded TTS data to improve the timbre of the
synthesized voice. The motivation behind the work is that issues of word skips
and repetitions are usually absent in HMM systems due to their ability to model
the duration distribution of phonemes accurately. Context-dependent pentaphone
modeling, along with tree-based clustering and state-tying, takes care of
unseen context and out-of-vocabulary words. A language model is also employed
to reduce synthesis errors further. Subjective evaluations indicate that speech
produced using the proposed system is superior to the baseline E2E synthesis
approach in terms of intelligibility when combining complementing attributes
from HMM and E2E frameworks. The further analysis highlights the proposed
approach's efficacy in low-resource scenarios.","6 pages, 7 figures, 33 references",
Alignment Entropy Regularization,"[arxiv.Result.Author('Ehsan Variani'), arxiv.Result.Author('Ke Wu'), arxiv.Result.Author('David Rybach'), arxiv.Result.Author('Cyril Allauzen'), arxiv.Result.Author('Michael Riley')]",2022-12-22 18:51:02+00:00,"Existing training criteria in automatic speech recognition(ASR) permit the
model to freely explore more than one time alignments between the feature and
label sequences. In this paper, we use entropy to measure a model's
uncertainty, i.e. how it chooses to distribute the probability mass over the
set of allowed alignments. Furthermore, we evaluate the effect of entropy
regularization in encouraging the model to distribute the probability mass only
on a smaller subset of allowed alignments. Experiments show that entropy
regularization enables a much simpler decoding method without sacrificing word
error rate, and provides better time alignment quality.",,
StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation,"[arxiv.Result.Author('Jean-Marie Lemercier'), arxiv.Result.Author('Julius Richter'), arxiv.Result.Author('Simon Welker'), arxiv.Result.Author('Timo Gerkmann')]",2022-12-22 16:35:42+00:00,"Diffusion models have shown a great ability at bridging the performance gap
between predictive and generative approaches for speech enhancement. We have
shown that they may even outperform their predictive counterparts for
non-additive corruption types or when they are evaluated on mismatched
conditions. However, diffusion models suffer from a high computational burden,
mainly as they require to run a neural network for each reverse diffusion step,
whereas predictive approaches only require one pass. As diffusion models are
generative approaches they may also produce vocalizing and breathing artifacts
in adverse conditions. In comparison, in such difficult scenarios, predictive
models typically do not produce such artifacts but tend to distort the target
speech instead, thereby degrading the speech quality. In this work, we present
a stochastic regeneration approach where an estimate given by a predictive
model is provided as a guide for further diffusion. We show that the proposed
approach uses the predictive model to remove the vocalizing and breathing
artifacts while producing very high quality samples thanks to the diffusion
model, even in adverse conditions. We further show that this approach enables
to use lighter sampling schemes with fewer diffusion steps without sacrificing
quality, thus lifting the computational burden by an order of magnitude. Source
code and audio examples are available online (https://uhh.de/inf-sp-storm).","This work has been submitted to the IEEE for publication. Copyright
  may be transferred without notice",
Training Integer-Only Deep Recurrent Neural Networks,"[arxiv.Result.Author('Vahid Partovi Nia'), arxiv.Result.Author('Eyyüb Sari'), arxiv.Result.Author('Vanessa Courville'), arxiv.Result.Author('Masoud Asgharian')]",2022-12-22 15:22:36+00:00,"Recurrent neural networks (RNN) are the backbone of many text and speech
applications. These architectures are typically made up of several
computationally complex components such as; non-linear activation functions,
normalization, bi-directional dependence and attention. In order to maintain
good accuracy, these components are frequently run using full-precision
floating-point computation, making them slow, inefficient and difficult to
deploy on edge devices. In addition, the complex nature of these operations
makes them challenging to quantize using standard quantization methods without
a significant performance drop. We present a quantization-aware training method
for obtaining a highly accurate integer-only recurrent neural network (iRNN).
Our approach supports layer normalization, attention, and an adaptive piecewise
linear (PWL) approximation of activation functions, to serve a wide range of
state-of-the-art RNNs. The proposed method enables RNN-based language models to
run on edge devices with $2\times$ improvement in runtime, and $4\times$
reduction in model size while maintaining similar accuracy as its
full-precision counterpart.",arXiv admin note: substantial text overlap with arXiv:2109.09828,
Understanding Postpartum Parents' Experiences via Two Digital Platforms,"[arxiv.Result.Author('Xuewen Yao'), arxiv.Result.Author('Miriam Mikhelson'), arxiv.Result.Author('Megan Micheletti'), arxiv.Result.Author('Eunsol Choi'), arxiv.Result.Author('S Craig Watkins'), arxiv.Result.Author('Edison Thomaz'), arxiv.Result.Author('Kaya De Barbaro')]",2022-12-22 02:13:30+00:00,"Digital platforms, including online forums and helplines, have emerged as
avenues of support for caregivers suffering from postpartum mental health
distress. Understanding support seekers' experiences as shared on these
platforms could provide crucial insight into caregivers' needs during this
vulnerable time. In the current work, we provide a descriptive analysis of the
concerns, psychological states, and motivations shared by healthy and
distressed postpartum support seekers on two digital platforms, a one-on-one
digital helpline and a publicly available online forum. Using a combination of
human annotations, dictionary models and unsupervised techniques, we find stark
differences between the experiences of distressed and healthy mothers.
Distressed mothers described interpersonal problems and a lack of support, with
8.60% - 14.56% reporting severe symptoms including suicidal ideation. In
contrast, the majority of healthy mothers described childcare issues, such as
questions about breastfeeding or sleeping, and reported no severe mental health
concerns. Across the two digital platforms, we found that distressed mothers
shared similar content. However, the patterns of speech and affect shared by
distressed mothers differed between the helpline vs. the online forum,
suggesting the design of these platforms may shape meaningful measures of their
support-seeking experiences. Our results provide new insight into the
experiences of caregivers suffering from postpartum mental health distress. We
conclude by discussing methodological considerations for understanding content
shared by support seekers and design considerations for the next generation of
support tools for postpartum parents.","Will be published in PACM HCI, CSCW1, April 2023 issue",
ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement,"[arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Tal Remez'), arxiv.Result.Author('Bowen Shi'), arxiv.Result.Author('Jacob Donley'), arxiv.Result.Author('Yossi Adi')]",2022-12-21 21:36:52+00:00,"Prior works on improving speech quality with visual input typically study
each type of auditory distortion separately (e.g., separation, inpainting,
video-to-speech) and present tailored algorithms. This paper proposes to unify
these subjects and study Generalized Speech Enhancement, where the goal is not
to reconstruct the exact reference clean signal, but to focus on improving
certain aspects of speech. In particular, this paper concerns intelligibility,
quality, and video synchronization. We cast the problem as audio-visual speech
resynthesis, which is composed of two steps: pseudo audio-visual speech
recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and
P-TTS are connected by discrete units derived from a self-supervised speech
model. Moreover, we utilize self-supervised audio-visual speech model to
initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first
high-quality model for in-the-wild video-to-speech synthesis and achieves
superior performance on all LRS3 audio-visual enhancement tasks with a single
model. To demonstrates its applicability in the real world, ReVISE is also
evaluated on EasyCom, an audio-visual benchmark collected under challenging
acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE
greatly suppresses noise and improves quality. Project page:
https://wnhsu.github.io/ReVISE.",,
Universal versus system-specific features of punctuation usage patterns in~major Western~languages,"[arxiv.Result.Author('Tomasz Stanisz'), arxiv.Result.Author('Stanislaw Drozdz'), arxiv.Result.Author('Jaroslaw Kwapien')]",2022-12-21 16:52:10+00:00,"The celebrated proverb that ""speech is silver, silence is golden"" has a long
multinational history and multiple specific meanings. In written texts
punctuation can in fact be considered one of its manifestations. Indeed, the
virtue of effectively speaking and writing involves - often decisively - the
capacity to apply the properly placed breaks. In the present study, based on a
large corpus of world-famous and representative literary texts in seven major
Western languages, it is shown that the distribution of intervals between
consecutive punctuation marks in almost all texts can universally be
characterised by only two parameters of the discrete Weibull distribution which
can be given an intuitive interpretation in terms of the so-called hazard
function. The values of these two parameters tend to be language-specific,
however, and even appear to navigate translations. The properties of the
computed hazard functions indicate that among the studied languages, English
turns out to be the least constrained by the necessity to place a consecutive
punctuation mark to partition a sequence of words. This may suggest that when
compared to other studied languages, English is more flexible, in the sense of
allowing longer uninterrupted sequences of words. Spanish reveals similar
tendency to only a bit lesser extent.",,
Generating music with sentiment using Transformer-GANs,"[arxiv.Result.Author('Pedro Neves'), arxiv.Result.Author('Jose Fornari'), arxiv.Result.Author('João Florindo')]",2022-12-21 15:59:35+00:00,"The field of Automatic Music Generation has seen significant progress thanks
to the advent of Deep Learning. However, most of these results have been
produced by unconditional models, which lack the ability to interact with their
users, not allowing them to guide the generative process in meaningful and
practical ways. Moreover, synthesizing music that remains coherent across
longer timescales while still capturing the local aspects that make it sound
``realistic'' or ``human-like'' is still challenging. This is due to the large
computational requirements needed to work with long sequences of data, and also
to limitations imposed by the training schemes that are often employed. In this
paper, we propose a generative model of symbolic music conditioned by data
retrieved from human sentiment. The model is a Transformer-GAN trained with
labels that correspond to different configurations of the valence and arousal
dimensions that quantitatively represent human affective states. We try to
tackle both of the problems above by employing an efficient linear version of
Attention and using a Discriminator both as a tool to improve the overall
quality of the generated music and its ability to follow the conditioning
signals.",,
Polytopic Analysis of Music,"[arxiv.Result.Author('Axel Marmoret'), arxiv.Result.Author('Jérémy E. Cohen'), arxiv.Result.Author('Frédéric Bimbot')]",2022-12-21 14:58:20+00:00,"Structural segmentation of music refers to the task of finding a symbolic
representation of the organisation of a song, reducing the musical flow to a
partition of non-overlapping segments. Under this definition, the musical
structure may not be unique, and may even be ambiguous. One way to resolve that
ambiguity is to see this task as a compression process, and to consider the
musical structure as the optimization of a given compression criteria. In that
viewpoint, C. Guichaoua developed a compression-driven model for retrieving the
musical structure, based on the ""System and Contrast"" model, and on polytopes,
which are extension of nhypercubes. We present this model, which we call
""polytopic analysis of music"", along with a new opensource dedicated toolbox
called MusicOnPolytopes (in Python). This model is also extended to the use of
the Tonnetz as a relation system. Structural segmentation experiments are
conducted on the RWC Pop dataset. Results show improvements compared to the
previous ones, presented by C. Guichaoua.",Work document,
RECAP: Retrieval Augmented Music Captioner,"[arxiv.Result.Author('Zihao He'), arxiv.Result.Author('Weituo Hao'), arxiv.Result.Author('Xuchen Song')]",2022-12-21 10:20:54+00:00,"With the prevalence of stream media platforms serving music search and
recommendation, interpreting music by understanding audio and lyrics
interactively has become an important and challenging task. However, many
previous works focus on refining individual components of encoder-decoder
architecture mapping music to caption tokens, ignoring the potential usage of
audio and lyrics correspondence. In this paper, we propose to explicitly learn
the multi-modal alignment with retrieval augmentation by contrastive learning.
By learning audio-lyrics correspondence, the model is guided to learn better
cross-modal attention weights, thus generating high-quality caption words. We
provide both theoretical and empirical results that demonstrate the advantage
of the proposed method.",,
End-to-End Automatic Speech Recognition model for the Sudanese Dialect,"[arxiv.Result.Author('Ayman Mansour'), arxiv.Result.Author('Wafaa F. Mukhtar')]",2022-12-21 07:35:33+00:00,"Designing a natural voice interface rely mostly on Speech recognition for
interaction between human and their modern digital life equipment. In addition,
speech recognition narrows the gap between monolingual individuals to better
exchange communication. However, the field lacks wide support for several
universal languages and their dialects, while most of the daily conversations
are carried out using them. This paper comes to inspect the viability of
designing an Automatic Speech Recognition model for the Sudanese dialect, which
is one of the Arabic Language dialects, and its complexity is a product of
historical and social conditions unique to its speakers. This condition is
reflected in both the form and content of the dialect, so this paper gives an
overview of the Sudanese dialect and the tasks of collecting represented
resources and pre-processing performed to construct a modest dataset to
overcome the lack of annotated data. Also proposed end- to-end speech
recognition model, the design of the model was formed using Convolution Neural
Networks. The Sudanese dialect dataset would be a stepping stone to enable
future Natural Language Processing research targeting the dialect. The designed
model provided some insights into the current recognition task and reached an
average Label Error Rate of 73.67%.",,
"4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders","[arxiv.Result.Author('Yui Sudo'), arxiv.Result.Author('Muhammad Shakeel'), arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Shinji Watanabe')]",2022-12-21 07:15:59+00:00,"The network architecture of end-to-end (E2E) automatic speech recognition
(ASR) can be classified into several models, including connectionist temporal
classification (CTC), recurrent neural network transducer (RNN-T), attention
mechanism, and non-autoregressive mask-predict models. Since each of these
network architectures has pros and cons, a typical use case is to switch these
separate models depending on the application requirement, resulting in the
increased overhead of maintaining all models. Several methods for integrating
two of these complementary models to mitigate the overhead issue have been
proposed; however, if we integrate more models, we will further benefit from
these complementary models and realize broader applications with a single
system. This paper proposes four-decoder joint modeling (4D) of CTC, attention,
RNN-T, and mask-predict, which has the following three advantages: 1) The four
decoders are jointly trained so that they can be easily switched depending on
the application scenarios. 2) Joint training may bring model regularization and
improve the model robustness thanks to their complementary properties. 3) Novel
one-pass joint decoding methods using CTC, attention, and RNN-T further
improves the performance. The experimental results showed that the proposed
model consistently reduced the WER.",Submitted to ICASSP 2023,
KL Regularized Normalization Framework for Low Resource Tasks,"[arxiv.Result.Author('Neeraj Kumar'), arxiv.Result.Author('Ankur Narang'), arxiv.Result.Author('Brejesh Lall')]",2022-12-21 05:59:25+00:00,"Large pre-trained models, such as Bert, GPT, and Wav2Vec, have demonstrated
great potential for learning representations that are transferable to a wide
variety of downstream tasks . It is difficult to obtain a large quantity of
supervised data due to the limited availability of resources and time. In light
of this, a significant amount of research has been conducted in the area of
adopting large pre-trained datasets for diverse downstream tasks via fine
tuning, linear probing, or prompt tuning in low resource settings.
Normalization techniques are essential for accelerating training and improving
the generalization of deep neural networks and have been successfully used in a
wide variety of applications. A lot of normalization techniques have been
proposed but the success of normalization in low resource downstream NLP and
speech tasks is limited. One of the reasons is the inability to capture
expressiveness by rescaling parameters of normalization. We propose
KullbackLeibler(KL) Regularized normalization (KL-Norm) which make the
normalized data well behaved and helps in better generalization as it reduces
over-fitting, generalises well on out of domain distributions and removes
irrelevant biases and features with negligible increase in model parameters and
memory overheads. Detailed experimental evaluation on multiple low resource NLP
and speech tasks, demonstrates the superior performance of KL-Norm as compared
to other popular normalization and regularization techniques.",arXiv admin note: text overlap with arXiv:2106.05469 by other authors,
An Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits,"[arxiv.Result.Author('Kai Li'), arxiv.Result.Author('Fenghua Xie'), arxiv.Result.Author('Hang Chen'), arxiv.Result.Author('Kexin Yuan'), arxiv.Result.Author('Xiaolin Hu')]",2022-12-21 03:28:30+00:00,"Audio-visual approaches involving visual inputs have laid the foundation for
recent progress in speech separation. However, the optimization of the
concurrent usage of auditory and visual inputs is still an active research
area. Inspired by the cortico-thalamo-cortical circuit, in which the sensory
processing mechanisms of different modalities modulate one another via the
non-lemniscal sensory thalamus, we propose a novel cortico-thalamo-cortical
neural network (CTCNet) for audio-visual speech separation (AVSS). First, the
CTCNet learns hierarchical auditory and visual representations in a bottom-up
manner in separate auditory and visual subnetworks, mimicking the functions of
the auditory and visual cortical areas. Then, inspired by the large number of
connections between cortical regions and the thalamus, the model fuses the
auditory and visual information in a thalamic subnetwork through top-down
connections. Finally, the model transmits this fused information back to the
auditory and visual subnetworks, and the above process is repeated several
times. The results of experiments on three speech separation benchmark datasets
show that CTCNet remarkably outperforms existing AVSS methods with
considerablely fewer parameters. These results suggest that mimicking the
anatomical connectome of the mammalian brain has great potential for advancing
the development of deep neural networks. Project repo is
https://github.com/JusperLee/CTCNet.","13 pages, 5 figures",
Addressing the Selection Bias in Voice Assistance: Training Voice Assistance Model in Python with Equal Data Selection,"[arxiv.Result.Author('Kashav Piya'), arxiv.Result.Author('Srijal Shrestha'), arxiv.Result.Author('Cameran Frank'), arxiv.Result.Author('Estephanos Jebessa'), arxiv.Result.Author('Tauheed Khan Mohd')]",2022-12-20 21:26:05+00:00,"In recent times, voice assistants have become a part of our day-to-day lives,
allowing information retrieval by voice synthesis, voice recognition, and
natural language processing. These voice assistants can be found in many
modern-day devices such as Apple, Amazon, Google, and Samsung. This project is
primarily focused on Virtual Assistance in Natural Language Processing. Natural
Language Processing is a form of AI that helps machines understand people and
create feedback loops. This project will use deep learning to create a Voice
Recognizer and use Commonvoice and data collected from the local community for
model training using Google Colaboratory. After recognizing a command, the AI
assistant will be able to perform the most suitable actions and then give a
response.
  The motivation for this project comes from the race and gender bias that
exists in many virtual assistants. The computer industry is primarily dominated
by the male gender, and because of this, many of the products produced do not
regard women. This bias has an impact on natural language processing. This
project will be utilizing various open-source projects to implement machine
learning algorithms and train the assistant algorithm to recognize different
types of voices, accents, and dialects. Through this project, the goal to use
voice data from underrepresented groups to build a voice assistant that can
recognize voices regardless of gender, race, or accent. Increasing the
representation of women in the computer industry is important for the future of
the industry. By representing women in the initial study of voice assistants,
it can be shown that females play a vital role in the development of this
technology. In line with related work, this project will use first-hand data
from the college population and middle-aged adults to train voice assistant to
combat gender bias.",,
Did the Musk Takeover Boost Contentious Actors on Twitter?,[arxiv.Result.Author('Christopher Barrie')],2022-12-20 20:48:30+00:00,"Twitter has been accused of a liberal bias in its account verification and
content moderation policies. Elon Musk pledged, after his acquisition of the
company, to promote free speech on the platform by overhauling verification and
moderation policies. These events sparked fears of a rise in influence of
contentious actors -- notably from the political right. In this article, I use
a publicly released list of 138k Twitter accounts that purchased blue check
verification during the open window of November 9-November 11, 2022. I retrieve
4.9m tweets from a sample of politically contentious accounts. I then compare
engagement on contentious user posts before and after the Musk acquisition. I
find that the period following the Musk acquisition saw a substantive increase
in post engagement. There is no additional increase following blue tick
verification. I explain the findings with reference to an increase in activity
by a newly sympathetic user base.",,
SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks,"[arxiv.Result.Author('Suwon Shon'), arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Chyi-Jiunn Lin'), arxiv.Result.Author('Ankita Pasad'), arxiv.Result.Author('Felix Wu'), arxiv.Result.Author('Roshan Sharma'), arxiv.Result.Author('Wei-Lun Wu'), arxiv.Result.Author('Hung-Yi Lee'), arxiv.Result.Author('Karen Livescu'), arxiv.Result.Author('Shinji Watanabe')]",2022-12-20 18:39:59+00:00,"Spoken language understanding (SLU) tasks have been studied for many decades
in the speech research community, but have not received as much attention as
lower-level tasks like speech and speaker recognition. In particular, there are
not nearly as many SLU task benchmarks, and many of the existing ones use data
that is not freely available to all researchers. Recent work has begun to
introduce such benchmark datasets for several tasks. In this work, we introduce
several new annotated SLU benchmark tasks based on freely available speech
data, which complement existing benchmarks and address gaps in the SLU
evaluation landscape. We contribute four tasks: question answering and
summarization involve inference over longer speech sequences; named entity
localization addresses the speech-specific task of locating the targeted
content in the signal; dialog act classification identifies the function of a
given speech utterance. We follow the blueprint of the Spoken Language
Understanding Evaluation (SLUE) benchmark suite. In order to facilitate the
development of SLU models that leverage the success of pre-trained speech
representations, we will be publishing for each task (i) annotations for a
relatively small fine-tuning set, (ii) annotated development and test sets, and
(iii) baseline models for easy reproducibility and comparisons. In this work,
we present the details of data collection and annotation and the performance of
the baseline models. We also perform sensitivity analysis of pipeline models'
performance (speech recognizer + text model) to the speech recognition
accuracy, using more than 20 state-of-the-art speech recognition models.",,
AnnoBERT: Effectively Representing Multiple Annotators' Label Choices to Improve Hate Speech Detection,"[arxiv.Result.Author('Wenjie Yin'), arxiv.Result.Author('Vibhor Agarwal'), arxiv.Result.Author('Aiqi Jiang'), arxiv.Result.Author('Arkaitz Zubiaga'), arxiv.Result.Author('Nishanth Sastry')]",2022-12-20 16:30:11+00:00,"Supervised approaches generally rely on majority-based labels. However, it is
hard to achieve high agreement among annotators in subjective tasks such as
hate speech detection. Existing neural network models principally regard labels
as categorical variables, while ignoring the semantic information in diverse
label texts. In this paper, we propose AnnoBERT, a first-of-its-kind
architecture integrating annotator characteristics and label text with a
transformer-based model to detect hate speech, with unique representations
based on each annotator's characteristics via Collaborative Topic Regression
(CTR) and integrate label text to enrich textual representations. During
training, the model associates annotators with their label choices given a
piece of text; during evaluation, when label information is not available, the
model predicts the aggregated label given by the participating annotators by
utilising the learnt association. The proposed approach displayed an advantage
in detecting hate speech, especially in the minority class and edge cases with
annotator disagreement. Improvement in the overall performance is the largest
when the dataset is more label-imbalanced, suggesting its practical value in
identifying real-world hate speech, as the volume of hate speech in-the-wild is
extremely small on social media, when compared with normal (non-hate) speech.
Through ablation studies, we show the relative contributions of annotator
embeddings and label text to the model performance, and tested a range of
alternative annotator embeddings and label text combinations.",accepted at ICWSM 2023,"17th International AAAI Conference on Web and Social Media (ICWSM
  2023). Please cite accordingly"
Hopf Physical Reservoir Computer for Reconfigurable Sound Recognition,"[arxiv.Result.Author('Md Raf E Ul Shougat'), arxiv.Result.Author('XiaoFu Li'), arxiv.Result.Author('Siyao Shao'), arxiv.Result.Author('Kathleen Walden McGarvey'), arxiv.Result.Author('Edmon Perkins')]",2022-12-20 15:51:20+00:00,"The Hopf oscillator is a nonlinear oscillator that exhibits limit cycle
motion. This reservoir computer utilizes the vibratory nature of the
oscillator, which makes it an ideal candidate for reconfigurable sound
recognition tasks. In this paper, the capabilities of the Hopf reservoir
computer performing sound recognition are systematically demonstrated. This
work shows that the Hopf reservoir computer can offer superior sound
recognition accuracy compared to legacy approaches (e.g., a Mel spectrum +
machine learning approach). More importantly, the Hopf reservoir computer
operating as a sound recognition system does not require audio preprocessing
and has a very simple setup while still offering a high degree of
reconfigurability. These features pave the way of applying physical reservoir
computing for sound recognition in low power edge devices.","21 pages, 11 figures",
TTS-Guided Training for Accent Conversion Without Parallel Data,"[arxiv.Result.Author('Yi Zhou'), arxiv.Result.Author('Zhizheng Wu'), arxiv.Result.Author('Mingyang Zhang'), arxiv.Result.Author('Xiaohai Tian'), arxiv.Result.Author('Haizhou Li')]",2022-12-20 12:33:45+00:00,"Accent Conversion (AC) seeks to change the accent of speech from one (source)
to another (target) while preserving the speech content and speaker identity.
However, many AC approaches rely on source-target parallel speech data. We
propose a novel accent conversion framework without the need of parallel data.
Specifically, a text-to-speech (TTS) system is first pretrained with
target-accented speech data. This TTS model and its hidden representations are
expected to be associated only with the target accent. Then, a speech encoder
is trained to convert the accent of the speech under the supervision of the
pretrained TTS model. In doing so, the source-accented speech and its
corresponding transcription are forwarded to the speech encoder and the
pretrained TTS, respectively. The output of the speech encoder is optimized to
be the same as the text embedding in the TTS system. At run-time, the speech
encoder is combined with the pretrained TTS decoder to convert the
source-accented speech toward the target. In the experiments, we converted
English with two source accents (Chinese and Indian) to the target accent
(American/British/Canadian). Both objective metrics and subjective listening
tests successfully validate that, without any parallel data, the proposed
approach generates speech samples that are close to the target accent with high
speech quality.","5 pages, 4 figures, submitted to signal processing letter",
A Simple Feature Method for Prosody Rhythm Comparison,"[arxiv.Result.Author('Mariana Julião'), arxiv.Result.Author('Alberto Abad'), arxiv.Result.Author('Helena Moniz')]",2022-12-20 12:26:44+00:00,"Of all components of Prosody, Rhythm has been regarded as the hardest to
address, as it is utterly linked to Pitch and Intensity. Nevertheless, Rhythm
is a very good indicator of a speaker's fluency in a foreign language or even
of some diseases. Canonical ways to measure Rhythm, such as $\Delta C$ or
$\%V$, involve a cumbersome process of segment alignment, often leading to
modest and questionable results. Perceptively, however, rhythm does not sound
as difficult, as humans can grasp it even when the text is not fully
intelligible. In this work, we develop an empirical and unsupervised method of
rhythm assessment, which does not rely on the content. We have created a
fixed-length representation of each utterance, Peak Embedding (PE), which
codifies the proportional distance between peaks of the chosen Low-Level
Descriptors. Clustering pairs of small sentence-like units, we have attained
averages of 0.444 for Silhouette Coefficient using PE with Loudness, and 0.979
for Global Separability Index with a combination of PE with Pitch and Loudness.
Clustering same-structure words, we have attained averages of 0.196 for
Silhouette Coefficient and 0.864 for Global Separability Index for PE with
Loudness.",,
Emotion Selectable End-to-End Text-based Speech Editing,"[arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Ruibo Fu'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Zhengqi Wen'), arxiv.Result.Author('Chu Yuan Zhang')]",2022-12-20 12:02:40+00:00,"Text-based speech editing allows users to edit speech by intuitively cutting,
copying, and pasting text to speed up the process of editing speech. In the
previous work, CampNet (context-aware mask prediction network) is proposed to
realize text-based speech editing, significantly improving the quality of
edited speech. This paper aims at a new task: adding emotional effect to the
editing speech during the text-based speech editing to make the generated
speech more expressive. To achieve this task, we propose Emo-CampNet (emotion
CampNet), which can provide the option of emotional attributes for the
generated speech in text-based speech editing and has the one-shot ability to
edit unseen speakers' speech. Firstly, we propose an end-to-end
emotion-selectable text-based speech editing model. The key idea of the model
is to control the emotion of generated speech by introducing additional emotion
attributes based on the context-aware mask prediction network. Secondly, to
prevent the emotion of the generated speech from being interfered by the
emotional components in the original speech, a neutral content generator is
proposed to remove the emotion from the original speech, which is optimized by
the generative adversarial framework. Thirdly, two data augmentation methods
are proposed to enrich the emotional and pronunciation information in the
training set, which can enable the model to edit the unseen speaker's speech.
The experimental results that 1) Emo-CampNet can effectively control the
emotion of the generated speech in the process of text-based speech editing;
And can edit unseen speakers' speech. 2) Detailed ablation experiments further
prove the effectiveness of emotional selectivity and data augmentation methods.
The demo page is available at https://hairuo55.github.io/Emo-CampNet/","Under review, 12 pages, 11 figures, demo page is available at
  https://hairuo55.github.io/Emo-CampNet/",
VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion,"[arxiv.Result.Author('Hanbo Cai'), arxiv.Result.Author('Pengcheng Zhang'), arxiv.Result.Author('Hai Dong'), arxiv.Result.Author('Yan Xiao'), arxiv.Result.Author('Shunhui Ji')]",2022-12-20 09:24:25+00:00,"Keyword spotting (KWS) based on deep neural networks (DNNs) has achieved
massive success in voice control scenarios. However, training of such DNN-based
KWS systems often requires significant data and hardware resources.
Manufacturers often entrust this process to a third-party platform. This makes
the training process uncontrollable, where attackers can implant backdoors in
the model by manipulating third-party training data. An effective backdoor
attack can force the model to make specified judgments under certain
conditions, i.e., triggers. In this paper, we design a backdoor attack scheme
based on Voiceprint Selection and Voice Conversion, abbreviated as VSVC.
Experimental results demonstrated that VSVC is feasible to achieve an average
attack success rate close to 97% in four victim models when poisoning less than
1% of the training data.","7 pages,5 figures",
Visual Transformers for Primates Classification and Covid Detection,"[arxiv.Result.Author('Steffen Illium'), arxiv.Result.Author('Robert Müller'), arxiv.Result.Author('Andreas Sedlmeier'), arxiv.Result.Author('Claudia-Linnhoff Popien')]",2022-12-20 09:10:25+00:00,"We apply the vision transformer, a deep machine learning model build around
the attention mechanism, on mel-spectrogram representations of raw audio
recordings. When adding mel-based data augmentation techniques and
sample-weighting, we achieve comparable performance on both (PRS and CCS
challenge) tasks of ComParE21, outperforming most single model baselines. We
further introduce overlapping vertical patching and evaluate the influence of
parameter configurations. Index Terms: audio classification, attention,
mel-spectrogram, unbalanced data-sets, computational paralinguistics",,
Exploring Effective Fusion Algorithms for Speech Based Self-Supervised Learning Models,"[arxiv.Result.Author('Changli Tang'), arxiv.Result.Author('Yujin Wang'), arxiv.Result.Author('Xie Chen'), arxiv.Result.Author('Wei-Qiang Zhang')]",2022-12-20 09:09:02+00:00,"Self-supervised learning (SSL) has achieved great success in various areas
including speech processing. Recently, it is proven that speech based SSL
models are able to extract superior universal representations on a range of
downstream tasks compared to traditional hand-craft feature (e.g. FBank, MFCC)
in the SUPERB benchmark. However, different types of SSL models might exhibit
distinct strengths on different downstream tasks. In order to better utilize
the potential power of SSL models, in this work, we explore the effective
fusion on multiple SSL models. A series of model fusion algorithms are
investigated and compared by combining two types of SSL models, Hubert and
Data2vec, on two representative tasks from SUPERB benchmark, which are speaker
identification (SID) and automatic speech recognition (ASR) tasks. The
experimental results demonstrate that our proposed fusion algorithms can
further boost the individual model significantly.",Accepted by NCMMSC2022,
Improving the quality of neural TTS using long-form content and multi-speaker multi-style modeling,"[arxiv.Result.Author('Tuomo Raitio'), arxiv.Result.Author('Javier Latorre'), arxiv.Result.Author('Andrea Davis'), arxiv.Result.Author('Ladan Golipour')]",2022-12-20 08:28:34+00:00,"Neural text-to-speech (TTS) can provide quality close to natural speech if an
adequate amount of high-quality speech material is available for training.
However, acquiring speech data for TTS training is costly and time-consuming,
especially if the goal is to generate different speaking styles. In this work,
we show that we can transfer speaking style across speakers and improve the
quality of synthetic speech by training a multi-speaker multi-style (MSMS)
model with long-form recordings, in addition to regular TTS recordings. In
particular, we show that 1) multi-speaker modeling improves the overall TTS
quality, 2) the proposed MSMS approach outperforms pre-training and fine-tuning
approach when utilizing additional multi-speaker data, and 3) long-form
speaking style is highly rated regardless of the target text domain.",Submitted to ICASSP 2023,
A Twitter BERT Approach for Offensive Language Detection in Marathi,"[arxiv.Result.Author('Tanmay Chavan'), arxiv.Result.Author('Shantanu Patankar'), arxiv.Result.Author('Aditya Kane'), arxiv.Result.Author('Omkar Gokhale'), arxiv.Result.Author('Raviraj Joshi')]",2022-12-20 07:22:45+00:00,"Automated offensive language detection is essential in combating the spread
of hate speech, particularly in social media. This paper describes our work on
Offensive Language Identification in low resource Indic language Marathi. The
problem is formulated as a text classification task to identify a tweet as
offensive or non-offensive. We evaluate different mono-lingual and
multi-lingual BERT models on this classification task, focusing on BERT models
pre-trained with social media datasets. We compare the performance of MuRIL,
MahaTweetBERT, MahaTweetBERT-Hateful, and MahaBERT on the HASOC 2022 test set.
We also explore external data augmentation from other existing Marathi hate
speech corpus HASOC 2021 and L3Cube-MahaHate. The MahaTweetBERT, a BERT model,
pre-trained on Marathi tweets when fine-tuned on the combined dataset (HASOC
2021 + HASOC 2022 + MahaHate), outperforms all models with an F1 score of 98.43
on the HASOC 2022 test set. With this, we also provide a new state-of-the-art
result on HASOC 2022 / MOLD v2 test set.",,
InterMulti:Multi-view Multimodal Interactions with Text-dominated Hierarchical High-order Fusion for Emotion Analysis,"[arxiv.Result.Author('Feng Qiu'), arxiv.Result.Author('Wanzeng Kong'), arxiv.Result.Author('Yu Ding')]",2022-12-20 07:02:32+00:00,"Humans are sophisticated at reading interlocutors' emotions from multimodal
signals, such as speech contents, voice tones and facial expressions. However,
machines might struggle to understand various emotions due to the difficulty of
effectively decoding emotions from the complex interactions between multimodal
signals. In this paper, we propose a multimodal emotion analysis framework,
InterMulti, to capture complex multimodal interactions from different views and
identify emotions from multimodal signals. Our proposed framework decomposes
signals of different modalities into three kinds of multimodal interaction
representations, including a modality-full interaction representation, a
modality-shared interaction representation, and three modality-specific
interaction representations. Additionally, to balance the contribution of
different modalities and learn a more informative latent interaction
representation, we developed a novel Text-dominated Hierarchical High-order
Fusion(THHF) module. THHF module reasonably integrates the above three kinds of
representations into a comprehensive multimodal interaction representation.
Extensive experimental results on widely used datasets, (i.e.) MOSEI, MOSI and
IEMOCAP, demonstrate that our method outperforms the state-of-the-art.","9 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2212.08661",
Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data,"[arxiv.Result.Author('Mozhdeh Gheini'), arxiv.Result.Author('Tatiana Likhomanenko'), arxiv.Result.Author('Matthias Sperber'), arxiv.Result.Author('Hendra Setiawan')]",2022-12-20 03:54:44+00:00,"Self-training has been shown to be helpful in addressing data scarcity for
many domains, including vision, speech, and language. Specifically,
self-training, or pseudo-labeling, labels unsupervised data and adds that to
the training pool. In this work, we investigate and use pseudo-labeling for a
recently proposed novel setup: joint transcription and translation of speech,
which suffers from an absence of sufficient data resources. We show that under
such data-deficient circumstances, the unlabeled data can significantly vary in
domain from the supervised data, which results in pseudo-label quality
degradation. We investigate two categories of remedies that require no
additional supervision and target the domain mismatch: pseudo-label filtering
and data augmentation. We show that pseudo-label analysis and processing as
such results in additional gains on top of the vanilla pseudo-labeling setup
resulting in total improvements of up to 0.6% absolute WER and 2.2 BLEU points.",,
Speaking Style Conversion With Discrete Self-Supervised Units,"[arxiv.Result.Author('Gallil Maimon'), arxiv.Result.Author('Yossi Adi')]",2022-12-19 18:53:04+00:00,"Voice Conversion (VC) is the task of making a spoken utterance by one speaker
sound as if uttered by a different speaker, while keeping other aspects like
content unchanged. Current VC methods, focus primarily on spectral features
like timbre, while ignoring the unique speaking style of people which often
impacts prosody. In this study, we introduce a method for converting not only
the timbre, but also prosodic information (i.e., rhythm and pitch changes) to
those of the target speaker. The proposed approach is based on a pretrained,
self-supervised, model for encoding speech to discrete units, which make it
simple, effective, and easy to optimise. We consider the many-to-many setting
with no paired data. We introduce a suite of quantitative and qualitative
evaluation metrics for this setup, and empirically demonstrate the proposed
approach is significantly superior to the evaluated baselines. Code and samples
can be found under https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ .",,
SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations,"[arxiv.Result.Author('Ioannis Tsiamas'), arxiv.Result.Author('José A. R. Fonollosa'), arxiv.Result.Author('Marta R. Costa-jussà')]",2022-12-19 18:29:31+00:00,"Data scarcity is one of the main issues with the end-to-end approach for
Speech Translation, as compared to the cascaded one. Although most data
resources for Speech Translation are originally document-level, they offer a
sentence-level view, which can be directly used during training. But this
sentence-level view is single and static, potentially limiting the utility of
the data. Our proposed data augmentation method SegAugment challenges this idea
and aims to increase data availability by providing multiple alternative
sentence-level views of a dataset. Our method heavily relies on an Audio
Segmentation system to re-segment the speech of each document, after which we
obtain the target text with alignment methods. The Audio Segmentation system
can be parameterized with different length constraints, thus giving us access
to multiple and diverse sentence-level views for each document. Experiments in
MuST-C show consistent gains across 8 language pairs, with an average increase
of 2.2 BLEU points, and up to 4.7 BLEU for lower-resource scenarios in mTEDx.
Additionally, we find that SegAugment is also applicable to purely
sentence-level data, as in CoVoST, and that it enables Speech Translation
models to completely close the gap between the gold and automatic segmentation
at inference time.","Work in progress, 10 pages + appendix",
Norm of word embedding encodes information gain,"[arxiv.Result.Author('Momose Oyama'), arxiv.Result.Author('Sho Yokoi'), arxiv.Result.Author('Hidetoshi Shimodaira')]",2022-12-19 17:45:07+00:00,"Distributed representations of words encode lexical semantic information, but
how is that information encoded in word embeddings? Focusing on the skip-gram
with negative-sampling method, we show theoretically and experimentally that
the squared norm of word embedding encodes the information gain defined by the
Kullback-Leibler divergence of the co-occurrence distribution of a word to the
unigram distribution of the corpus. Furthermore, through experiments on tasks
of keyword extraction, hypernym prediction, and part-of-speech discrimination,
we confirmed that the KL divergence and the squared norm of embedding work as a
measure of the informativeness of a word provided that the bias caused by word
frequency is adequately corrected.",,
NusaCrowd: Open Source Initiative for Indonesian NLP Resources,"[arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Holy Lovenia'), arxiv.Result.Author('Alham Fikri Aji'), arxiv.Result.Author('Genta Indra Winata'), arxiv.Result.Author('Bryan Wilie'), arxiv.Result.Author('Rahmad Mahendra'), arxiv.Result.Author('Christian Wibisono'), arxiv.Result.Author('Ade Romadhony'), arxiv.Result.Author('Karissa Vincentio'), arxiv.Result.Author('Fajri Koto'), arxiv.Result.Author('Jennifer Santoso'), arxiv.Result.Author('David Moeljadi'), arxiv.Result.Author('Cahya Wirawan'), arxiv.Result.Author('Frederikus Hudi'), arxiv.Result.Author('Ivan Halim Parmonangan'), arxiv.Result.Author('Ika Alfina'), arxiv.Result.Author('Muhammad Satrio Wicaksono'), arxiv.Result.Author('Ilham Firdausi Putra'), arxiv.Result.Author('Samsul Rahmadani'), arxiv.Result.Author('Yulianti Oenang'), arxiv.Result.Author('Ali Akbar Septiandri'), arxiv.Result.Author('James Jaya'), arxiv.Result.Author('Kaustubh D. Dhole'), arxiv.Result.Author('Arie Ardiyanti Suryani'), arxiv.Result.Author('Rifki Afina Putri'), arxiv.Result.Author('Dan Su'), arxiv.Result.Author('Keith Stevens'), arxiv.Result.Author('Made Nindyatama Nityasya'), arxiv.Result.Author('Muhammad Farid Adilazuarda'), arxiv.Result.Author('Ryan Ignatius'), arxiv.Result.Author('Ryandito Diandaru'), arxiv.Result.Author('Tiezheng Yu'), arxiv.Result.Author('Vito Ghifari'), arxiv.Result.Author('Wenliang Dai'), arxiv.Result.Author('Yan Xu'), arxiv.Result.Author('Dyah Damapuspita'), arxiv.Result.Author('Cuk Tho'), arxiv.Result.Author('Ichwanul Muslim Karo Karo'), arxiv.Result.Author('Tirana Noor Fatyanosa'), arxiv.Result.Author('Ziwei Ji'), arxiv.Result.Author('Pascale Fung'), arxiv.Result.Author('Graham Neubig'), arxiv.Result.Author('Timothy Baldwin'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Herry Sujaini'), arxiv.Result.Author('Sakriani Sakti'), arxiv.Result.Author('Ayu Purwarianti')]",2022-12-19 17:28:22+00:00,"We present NusaCrowd, a collaborative initiative to collect and unite
existing resources for Indonesian languages, including opening access to
previously non-public resources. Through this initiative, we have has brought
together 137 datasets and 117 standardized data loaders. The quality of the
datasets has been assessed manually and automatically, and their effectiveness
has been demonstrated in multiple experiments. NusaCrowd's data collection
enables the creation of the first zero-shot benchmarks for natural language
understanding and generation in Indonesian and its local languages.
Furthermore, NusaCrowd brings the creation of the first multilingual automatic
speech recognition benchmark in Indonesian and its local languages. Our work is
intended to help advance natural language processing research in
under-represented languages.",,
"Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models","[arxiv.Result.Author('Yong Cheng'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Melvin Johnson'), arxiv.Result.Author('Wolfgang Macherey'), arxiv.Result.Author('Ankur Bapna')]",2022-12-19 15:45:36+00:00,"We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model
pre-trained jointly on unlabeled speech, unlabeled text and supervised data
spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST)
and Machine Translation (MT), in over 100 languages. By leveraging a quantized
representation of speech as a target, Mu$^{2}$SLAM trains the speech-text
models with a sequence-to-sequence masked denoising objective similar to T5 on
the decoder and a masked language modeling (MLM) objective on the encoder, for
both unlabeled speech and text, while utilizing the supervised tasks to improve
cross-lingual and cross-modal representation alignment within the model. On
CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained
on public datasets, improving on xx-en translation over the previous best by
1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR,
our model matches the performance of an mSLAM model fine-tuned with an RNN-T
decoder, despite using a relatively weaker sequence-to-sequence architecture.
On text understanding tasks, our model improves by more than 6\% over mSLAM on
XNLI, getting closer to the performance of mT5 models of comparable capacity on
XNLI and TydiQA, paving the way towards a single model for all speech and text
understanding tasks.",,
WACO: Word-Aligned Contrastive Learning for Speech Translation,"[arxiv.Result.Author('Siqi Ouyang'), arxiv.Result.Author('Rong Ye'), arxiv.Result.Author('Lei Li')]",2022-12-19 10:49:35+00:00,"End-to-end Speech Translation (E2E ST) aims to translate source speech into
target translation without generating the intermediate transcript. However,
existing approaches for E2E ST degrade considerably when only limited ST data
are available. We observe that an ST model's performance strongly correlates
with its embedding similarity from speech and transcript. In this paper, we
propose Word-Aligned COntrastive learning (WACO), a novel method for few-shot
speech-to-text translation. Our key idea is bridging word-level representations
for both modalities via contrastive learning. We evaluate WACO and other
methods on the MuST-C dataset, a widely used ST benchmark. Our experiments
demonstrate that WACO outperforms the best baseline methods by 0.7-8.5 BLEU
points with only 1-hour parallel data. Code is available at
https://anonymous.4open.science/r/WACO .",,
SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation,"[arxiv.Result.Author('Wenda Xu'), arxiv.Result.Author('Xian Qian'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('William Yang Wang')]",2022-12-19 09:02:16+00:00,"Is it possible to leverage large scale raw and raw parallel corpora to build
a general learned metric? Existing learned metrics have gaps to human
judgements, are model-dependent or are limited to the domains or tasks where
human ratings are available. In this paper, we propose SEScore2, a model-based
metric pretrained over million-scale synthetic dataset constructed by our novel
retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation
to human judgements without any human rating supervisions. Importantly, our
unsupervised SEScore2 can outperform supervised metrics, which are trained on
the News human ratings, at the TED domain. We evaluate SEScore2 over four text
generation tasks across three languages. SEScore2 outperforms all prior
unsupervised evaluation metrics in machine translation, speech translation,
data-to-text and dialogue generation, with average Kendall improvements 0.158.
SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue
generation and overall correlation.",,
An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations,"[arxiv.Result.Author('Shelly Jain'), arxiv.Result.Author('Priyanshi Pal'), arxiv.Result.Author('Anil Vuppala'), arxiv.Result.Author('Prasanta Ghosh'), arxiv.Result.Author('Chiranjeevi Yarra')]",2022-12-19 07:41:39+00:00,"Speech systems are sensitive to accent variations. This is especially
challenging in the Indian context, with an abundance of languages but a dearth
of linguistic studies characterising pronunciation variations. The growing
number of L2 English speakers in India reinforces the need to study accents and
L1-L2 interactions. We investigate the accents of Indian English (IE) speakers
and report in detail our observations, both specific and common to all regions.
In particular, we observe the phonemic variations and phonotactics occurring in
the speakers' native languages and apply this to their English pronunciations.
We demonstrate the influence of 18 Indian languages on IE by comparing the
native language pronunciations with IE pronunciations obtained jointly from
existing literature studies and phonetically annotated speech of 80 speakers.
Consequently, we are able to validate the intuitions of Indian language
influences on IE pronunciations by justifying pronunciation rules from the
perspective of Indian language phonology. We obtain a comprehensive description
in terms of universal and region-specific characteristics of IE, which
facilitates accent conversion and adaptation of existing ASR and TTS systems to
different Indian accents.","9 pages, 1 figure",
APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,"[arxiv.Result.Author('Soumya Sanyal'), arxiv.Result.Author('Yichong Xu'), arxiv.Result.Author('Shuohang Wang'), arxiv.Result.Author('Ziyi Yang'), arxiv.Result.Author('Reid Pryzant'), arxiv.Result.Author('Wenhao Yu'), arxiv.Result.Author('Chenguang Zhu'), arxiv.Result.Author('Xiang Ren')]",2022-12-19 07:40:02+00:00,"Logical reasoning of text is an important ability that requires understanding
the information present in the text, their interconnections, and then reasoning
through them to infer new conclusions. Prior works on improving the logical
reasoning ability of language models require complex processing of training
data (e.g., aligning symbolic knowledge to text), yielding task-specific data
augmentation solutions that restrict the learning of general logical reasoning
skills. In this work, we propose APOLLO, an adaptively pretrained language
model that has improved logical reasoning abilities. We select a subset of
Wikipedia, based on a set of logical inference keywords, for continued
pretraining of a language model. We use two self-supervised loss functions: a
modified masked language modeling loss where only specific parts-of-speech
words, that would likely require more reasoning than basic language
understanding, are masked, and a sentence-level classification loss that
teaches the model to distinguish between entailment and contradiction types of
sentences. The proposed training paradigm is both simple and independent of
task formats. We demonstrate the effectiveness of APOLLO by comparing it with
prior baselines on two logical reasoning datasets. APOLLO performs comparably
on ReClor and outperforms baselines on LogiQA.","11 pages, 5 figures",
Exploring Workplace Behaviors through Speaking Patterns using Large-scale Multimodal Wearable Recordings: A Study of Healthcare Providers,"[arxiv.Result.Author('Tiantian Feng'), arxiv.Result.Author('Shrikanth Narayanan')]",2022-12-18 14:01:35+00:00,"Interpersonal spoken communication is central to human interaction and the
exchange of information. Such interactive processes involve not only speech and
spoken language but also non-verbal cues such as hand gestures, facial
expressions, and nonverbal vocalization, that are used to express feelings and
provide feedback. These multimodal communication signals carry a variety of
information about the people: traits like gender and age as well as about
physical and psychological states and behavior. This work uses wearable
multimodal sensors to investigate interpersonal communication behaviors
focusing on speaking patterns among healthcare providers with a focus on
nurses. We analyze longitudinal data collected from $99$ nurses in a large
hospital setting over ten weeks. The results indicate that speaking pattern
differences across shift schedules and working units. Moreover, results show
that speaking patterns combined with physiological measures can be used to
predict affect measures and life satisfaction scores. The implementation of
this work can be accessed at https://github.com/usc-sail/tiles-audio-arousal.",,
BEATs: Audio Pre-Training with Acoustic Tokenizers,"[arxiv.Result.Author('Sanyuan Chen'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Chengyi Wang'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Daniel Tompkins'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Furu Wei')]",2022-12-18 10:41:55+00:00,"The massive growth of self-supervised learning (SSL) has been witnessed in
language, vision, speech, and audio domains over the past few years. While
discrete label prediction is widely adopted for other modalities, the
state-of-the-art audio SSL models still employ reconstruction loss for
pre-training. Compared with reconstruction loss, semantic-rich discrete label
prediction encourages the SSL model to abstract the high-level audio semantics
and discard the redundant details as in human perception. However, a
semantic-rich acoustic tokenizer for general audio pre-training is usually not
straightforward to obtain, due to the continuous property of audio and
unavailable phoneme sequences like speech. To tackle this challenge, we propose
BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder
representation from Audio Transformers, where an acoustic tokenizer and an
audio SSL model are optimized by iterations. In the first iteration, we use
random projection as the acoustic tokenizer to train an audio SSL model in a
mask and label prediction manner. Then, we train an acoustic tokenizer for the
next iteration by distilling the semantic knowledge from the pre-trained or
fine-tuned audio SSL model. The iteration is repeated with the hope of mutual
promotion of the acoustic tokenizer and audio SSL model. The experimental
results demonstrate our acoustic tokenizers can generate discrete labels with
rich audio semantics and our audio SSL models achieve state-of-the-art results
across various audio classification benchmarks, even outperforming previous
models that use more training data and model parameters significantly.
Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for
audio-only models without using any external data, and 98.1% accuracy on
ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",,
Fast FullSubNet: Accelerate Full-band and Sub-band Fusion Model for Single-channel Speech Enhancement,"[arxiv.Result.Author('Xiang Hao'), arxiv.Result.Author('Xiaofei Li')]",2022-12-18 05:41:33+00:00,"FullSubNet is our recently proposed real-time single-channel speech
enhancement network that achieves outstanding performance on the Deep Noise
Suppression (DNS) Challenge dataset. A number of variants of FullSubNet have
been proposed recently, but they all focus on the structure design towards
better performance and are rarely concerned with computational efficiency. This
work proposes a new architecture named Fast FullSubNet dedicated to
accelerating the computation of FullSubNet. Specifically, Fast FullSubNet
processes sub-band speech spectra in the mel-frequency domain by using cascaded
linear-to-mel full-band, sub-band, and mel-to-linear full-band models such that
frequencies involved in the sub-band computation are vastly reduced. After
that, a down-sampling operation is proposed for the sub-band input sequence to
further reduce the computational complexity along the time axis. Experimental
results show that, compared to FullSubNet, Fast FullSubNet has only 13%
computational complexity and 16% processing time, and achieves comparable or
even better performance.","submitted to 2023 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)",
"A Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness","[arxiv.Result.Author('Tiantian Feng'), arxiv.Result.Author('Rajat Hebbar'), arxiv.Result.Author('Nicholas Mehlman'), arxiv.Result.Author('Xuan Shi'), arxiv.Result.Author('Aditya Kommineni'), arxiv.Result.Author('and Shrikanth Narayanan')]",2022-12-18 04:21:35+00:00,"Speech-centric machine learning systems have revolutionized many leading
domains ranging from transportation and healthcare to education and defense,
profoundly changing how people live, work, and interact with each other.
However, recent studies have demonstrated that many speech-centric ML systems
may need to be considered more trustworthy for broader deployment.
Specifically, concerns over privacy breaches, discriminating performance, and
vulnerability to adversarial attacks have all been discovered in ML research
fields. In order to address the above challenges and risks, a significant
number of efforts have been made to ensure these ML systems are trustworthy,
especially private, safe, and fair. In this paper, we conduct the first
comprehensive survey on speech-centric trustworthy ML topics related to
privacy, safety, and fairness. In addition to serving as a summary report for
the research community, we point out several promising future research
directions to inspire the researchers who wish to explore further in this area.",,
Low-Resource Authorship Style Transfer with In-Context Learning,"[arxiv.Result.Author('Ajay Patel'), arxiv.Result.Author('Nicholas Andrews'), arxiv.Result.Author('Chris Callison-Burch')]",2022-12-18 01:57:30+00:00,"Authorship style transfer involves altering the style of text to match the
style of some target author whilst preserving the semantic meaning of the
original text. Existing approaches to unsupervised authorship style transfer
like STRAP have largely focused on style transfer for target authors with many
examples of their writing style through books, speeches, or other published
works (Krishna et al., 2020). Due to this high-resource training data
requirement (often greater than 100,000 words), these approaches are often only
useful for style transfer to the style of published authors, politicians, or
other well-known figures and authorship styles. In this paper, we attempt to
perform low-resource authorship style transfer, a more challenging class of
authorship style transfer where only a limited amount of text in the target
author's style may exist. In our experiments, we specifically choose source and
target authors from Reddit to perform style transfer over their Reddit posts,
limiting ourselves to just 16 posts (on average $\approx$ 500 words) of the
target author's style. We then propose a method for automatic evaluation on the
low-resource authorship style transfer task utilizing authorship and style
representation embeddings (Rivera-Soto et al., 2021; Wegmann et al., 2022). We
evaluate our style transferred outputs with the proposed automatic evaluation
method and find that our method, STYLL, is able to outperform STRAP and a
comprehensive set of baselines.",,
Learning from Taxonomy: Multi-label Few-Shot Classification for Everyday Sound Recognition,"[arxiv.Result.Author('Jinhua Liang'), arxiv.Result.Author('Huy Phan'), arxiv.Result.Author('Emmanouil Benetos')]",2022-12-17 20:56:55+00:00,"Everyday sound recognition aims to infer types of sound events in audio
streams. While many works succeeded in training models with high performance in
a fully-supervised manner, they are still restricted to the demand of large
quantities of labelled data and the range of predefined classes. To overcome
these drawbacks, this work firstly curates a new database named FSD-FS for
multi-label few-shot audio classification. It then explores how to incorporate
audio taxonomy in few-shot learning. Specifically, this work proposes
label-dependent prototypical networks (LaD-protonet) to exploit parent-children
relationships between labels. Plus, it applies taxonomy-aware label smoothing
techniques to boost model performance. Experiments demonstrate that
LaD-protonet outperforms original prototypical networks as well as other
state-of-the-art methods. Moreover, its performance can be further boosted when
combined with taxonomy-aware label smoothing.",submitted to ICASSP2023,
AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation,"[arxiv.Result.Author('Xingshan Zeng'), arxiv.Result.Author('Liangyou Li'), arxiv.Result.Author('Qun Liu')]",2022-12-17 16:14:30+00:00,"To alleviate the data scarcity problem in End-to-end speech translation (ST),
pre-training on data for speech recognition and machine translation is
considered as an important technique. However, the modality gap between speech
and text prevents the ST model from efficiently inheriting knowledge from the
pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It
adapts the speech features with a new shrinking mechanism to mitigate the
length mismatch between speech and text features by predicting word boundaries.
Experiments on the MUST-C dataset demonstrate that AdaTranS achieves better
performance than the other shrinking-based methods, with higher inference speed
and lower memory usage. Further experiments also show that AdaTranS can be
equipped with additional alignment losses to further improve performance.",,
Interaction design for socially assistive robots for people with developmental disabilities,[arxiv.Result.Author('Xiaodong Wu')],2022-12-17 06:35:17+00:00,"Social robots, also known as service or assistant robots, have been developed
to improve the quality of human life in recent years. Socially assistive robots
(SAR) are a special type of social robots that focus on providing support
through social interaction. The design of socially capable and intelligent
robots can vary, depending on the target user groups. In this work, I assess
the effect of socially assistive robots' roles, functions, and communication
approaches in the context of a social agent providing service or companionship
to users with developmental disabilities. In this thesis, I describe an
exploratory study of interaction design for a socially assistive robot that
supports people suffering from developmental disabilities. While exploring the
impacts of visual elements to robot's visual interface and different aspects of
robot's social dimension, I developed a series of prototypes and tested them
through three user studies that included three residents with various function
levels at a local group home for people with developmental disabilities. All
user studies had been recorded for the following qualitative data analysis.
Results show that each design factor played a different role in delivering
information and in increasing engagement, and there are more aspects of HRI to
consider besides robot's graphical user interface and speech, such as proxemics
and robot's physical appearance and dimensions. I also note that some
fundamental design principles that would work for ordinary users did not apply
to our target user group. I conclude that socially assistive robots could
benefit our target users and acknowledge that these robots were not suitable
for certain scenarios based on the feedback from our users.",Master of Science Thesis,
SkillFence: A Systems Approach to Practically Mitigating Voice-Based Confusion Attacks,"[arxiv.Result.Author('Ashish Hooda'), arxiv.Result.Author('Matthew Wallace'), arxiv.Result.Author('Kushal Jhunjhunwalla'), arxiv.Result.Author('Earlence Fernandes'), arxiv.Result.Author('Kassem Fawaz')]",2022-12-16 22:22:04+00:00,"Voice assistants are deployed widely and provide useful functionality.
However, recent work has shown that commercial systems like Amazon Alexa and
Google Home are vulnerable to voice-based confusion attacks that exploit design
issues. We propose a systems-oriented defense against this class of attacks and
demonstrate its functionality for Amazon Alexa. We ensure that only the skills
a user intends execute in response to voice commands. Our key insight is that
we can interpret a user's intentions by analyzing their activity on counterpart
systems of the web and smartphones. For example, the Lyft ride-sharing Alexa
skill has an Android app and a website. Our work shows how information from
counterpart apps can help reduce dis-ambiguities in the skill invocation
process. We build SkilIFence, a browser extension that existing voice assistant
users can install to ensure that only legitimate skills run in response to
their commands. Using real user data from MTurk (N = 116) and experimental
trials involving synthetic and organic speech, we show that SkillFence provides
a balance between usability and security by securing 90.83% of skills that a
user will need with a False acceptance rate of 19.83%.",,
Speech Aware Dialog System Technology Challenge (DSTC11),"[arxiv.Result.Author('Hagen Soltau'), arxiv.Result.Author('Izhak Shafran'), arxiv.Result.Author('Mingqiu Wang'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Jeffrey Zhao'), arxiv.Result.Author('Ye Jia'), arxiv.Result.Author('Wei Han'), arxiv.Result.Author('Yuan Cao'), arxiv.Result.Author('Aramys Miranda')]",2022-12-16 20:30:33+00:00,"Most research on task oriented dialog modeling is based on written text
input. However, users interact with practical dialog systems often using speech
as input. Typically, systems convert speech into text using an Automatic Speech
Recognition (ASR) system, introducing errors. Furthermore, these systems do not
address the differences in written and spoken language. The research on this
topic is stymied by the lack of a public corpus. Motivated by these
considerations, our goal in hosting the speech-aware dialog state tracking
challenge was to create a public corpus or task which can be used to
investigate the performance gap between the written and spoken forms of input,
develop models that could alleviate this gap, and establish whether
Text-to-Speech-based (TTS) systems is a reasonable surrogate to the more-labor
intensive human data collection. We created three spoken versions of the
popular written-domain MultiWoz task -- (a) TTS-Verbatim: written user inputs
were converted into speech waveforms using a TTS system, (b) Human-Verbatim:
humans spoke the user inputs verbatim, and (c) Human-paraphrased: humans
paraphrased the user inputs. Additionally, we provided different forms of ASR
output to encourage wider participation from teams that may not have access to
state-of-the-art ASR systems. These included ASR transcripts, word time stamps,
and latent representations of the audio (audio encoder outputs). In this paper,
we describe the corpus, report results from participating teams, provide
preliminary analyses of their results, and summarize the current
state-of-the-art in this domain.",,
Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recognition,"[arxiv.Result.Author('Aleksandr Laptev'), arxiv.Result.Author('Boris Ginsburg')]",2022-12-16 20:27:40+00:00,"This paper presents a class of new fast non-trainable entropy-based
confidence estimation methods for automatic speech recognition. We show how
per-frame entropy values can be normalized and aggregated to obtain a
confidence measure per unit and per word for Connectionist Temporal
Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models.
Proposed methods have similar computational complexity to the traditional
method based on the maximum per-frame probability, but they are more
adjustable, have a wider effective threshold range, and better push apart the
confidence distributions of correct and incorrect words. We evaluate the
proposed confidence measures on LibriSpeech test sets, and show that they are
up to 2 and 4 times better than confidence estimation based on the maximum
per-frame probability at detecting incorrect words for Conformer-CTC and
Conformer-RNN-T models, respectively.","To appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar. 8 pages, 4
  figures, 4 tables",
Source Tracing: Detecting Voice Spoofing,"[arxiv.Result.Author('Tinglong Zhu'), arxiv.Result.Author('Xingming Wang'), arxiv.Result.Author('Xiaoyi Qin'), arxiv.Result.Author('Ming Li')]",2022-12-16 17:29:15+00:00,"Recent anti-spoofing systems focus on spoofing detection, where the task is
only to determine whether the test audio is fake. However, there are few
studies putting attention to identifying the methods of generating fake speech.
Common spoofing attack algorithms in the logical access (LA) scenario, such as
voice conversion and speech synthesis, can be divided into several stages:
input processing, conversion, waveform generation, etc. In this work, we
propose a system for classifying different spoofing attributes, representing
characteristics of different modules in the whole pipeline. Classifying
attributes for the spoofing attack other than determining the whole spoofing
pipeline can make the system more robust when encountering complex combinations
of different modules at different stages. In addition, our system can also be
used as an auxiliary system for anti-spoofing against unseen spoofing methods.
The experiments are conducted on ASVspoof 2019 LA data set and the proposed
method achieved a 20\% relative improvement against conventional binary spoof
detection methods.",Accepted by APSIPA ASC,
Fine-grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis,"[arxiv.Result.Author('Matyáš Boháček'), arxiv.Result.Author('Michal Bravanský'), arxiv.Result.Author('Filip Trhlík'), arxiv.Result.Author('Václav Moravec')]",2022-12-16 16:00:19+00:00,"We present the Verifee Dataset: a novel dataset of news articles with
fine-grained trustworthiness annotations. We develop a detailed methodology
that assesses the texts based on their parameters encompassing editorial
transparency, journalist conventions, and objective reporting while penalizing
manipulative techniques. We bring aboard a diverse set of researchers from
social, media, and computer sciences to overcome barriers and limited framing
of this interdisciplinary problem. We collect over $10,000$ unique articles
from almost $60$ Czech online news sources. These are categorized into one of
the $4$ classes across the credibility spectrum we propose, raging from
entirely trustworthy articles all the way to the manipulative ones. We produce
detailed statistics and study trends emerging throughout the set. Lastly, we
fine-tune multiple popular sequence-to-sequence language models using our
dataset on the trustworthiness classification task and report the best testing
F-1 score of $0.52$. We open-source the dataset, annotation methodology, and
annotators' instructions in full length at https://verifee.ai/research to
enable easy build-up work. We believe similar methods can help prevent
disinformation and educate in the realm of media literacy.","13 pages, 3 figures; to be published at the Second Workshop on
  Multimodal Fact-Checking and Hate Speech Detection (DEFACTIFY 2023) at the
  AAAI 2023 Conference, February 14, 2023, Washington, D.C",
Context-aware Fine-tuning of Self-supervised Speech Models,"[arxiv.Result.Author('Suwon Shon'), arxiv.Result.Author('Felix Wu'), arxiv.Result.Author('Kwangyoun Kim'), arxiv.Result.Author('Prashant Sridhar'), arxiv.Result.Author('Karen Livescu'), arxiv.Result.Author('Shinji Watanabe')]",2022-12-16 15:46:15+00:00,"Self-supervised pre-trained transformers have improved the state of the art
on a variety of speech tasks. Due to the quadratic time and space complexity of
self-attention, they usually operate at the level of relatively short (e.g.,
utterance) segments. In this paper, we study the use of context, i.e.,
surrounding segments, during fine-tuning and propose a new approach called
context-aware fine-tuning. We attach a context module on top of the last layer
of a pre-trained model to encode the whole segment into a context embedding
vector which is then used as an additional feature for the final prediction.
During the fine-tuning stage, we introduce an auxiliary loss that encourages
this context embedding vector to be similar to context vectors of surrounding
segments. This allows the model to make predictions without access to these
surrounding segments at inference time and requires only a tiny overhead
compared to standard fine-tuned models. We evaluate the proposed approach using
the SLUE and Librilight benchmarks for several downstream tasks: Automatic
speech recognition (ASR), named entity recognition (NER), and sentiment
analysis (SA). The results show that context-aware fine-tuning not only
outperforms a standard fine-tuning baseline but also rivals a strong context
injection baseline that uses neighboring speech segments during inference.",,
"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks","[arxiv.Result.Author('Esaú Villatoro-Tello'), arxiv.Result.Author('Srikanth Madikeri'), arxiv.Result.Author('Juan Zuluaga-Gomez'), arxiv.Result.Author('Bidisha Sharma'), arxiv.Result.Author('Seyyed Saeed Sarfjoo'), arxiv.Result.Author('Iuliia Nigmatulina'), arxiv.Result.Author('Petr Motlicek'), arxiv.Result.Author('Alexei V. Ivanov'), arxiv.Result.Author('Aravind Ganapathiraju')]",2022-12-16 14:01:42+00:00,"In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs allows SLU
systems to improve in comparison to the 1-best setup (4% relative improvement).
However, crossmodal approaches, i.e., learning from acoustic and text
embeddings, obtains performance similar to the oracle setup, and a relative
improvement of 18% over the 1-best configuration. Thus, crossmodal
architectures represent a good alternative to overcome the limitations of
working purely automatically generated textual data.",Submitted to ICASSP 2023 (Under review),
BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric,"[arxiv.Result.Author('Mingda Chen'), arxiv.Result.Author('Paul-Ambroise Duquenne'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Justine Kao'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Marta R. Costa-jussà')]",2022-12-16 14:00:26+00:00,"End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.",,
Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning,"[arxiv.Result.Author('Alex Tamkin'), arxiv.Result.Author('Margalit Glasgow'), arxiv.Result.Author('Xiluo He'), arxiv.Result.Author('Noah Goodman')]",2022-12-16 10:08:38+00:00,"What role do augmentations play in contrastive learning? Recent work suggests
that good augmentations are label-preserving with respect to a specific
downstream task. We complicate this picture by showing that label-destroying
augmentations can be useful in the foundation model setting, where the goal is
to learn diverse, general-purpose representations for multiple downstream
tasks. We perform contrastive learning experiments on a range of image and
audio datasets with multiple downstream tasks (e.g. for digits superimposed on
photographs, predicting the class of one vs. the other). We find that Viewmaker
Networks, a recently proposed model for learning augmentations for contrastive
learning, produce label-destroying augmentations that stochastically destroy
features needed for different downstream tasks. These augmentations are
interpretable (e.g. altering shapes, digits, or letters added to images) and
surprisingly often result in better performance compared to expert-designed
augmentations, despite not preserving label information. To support our
empirical results, we theoretically analyze a simple contrastive learning
setting with a linear model. In this setting, label-destroying augmentations
are crucial for preventing one set of features from suppressing the learning of
features useful for another downstream task. Our results highlight the need for
analyzing the interaction between multiple downstream tasks when trying to
explain the success of foundation models.",,
Towards Unified All-Neural Beamforming for Time and Frequency Domain Speech Separation,"[arxiv.Result.Author('Rongzhi Gu'), arxiv.Result.Author('Shi-Xiong Zhang'), arxiv.Result.Author('Yuexian Zou'), arxiv.Result.Author('Dong Yu')]",2022-12-16 08:48:19+00:00,"Recently, frequency domain all-neural beamforming methods have achieved
remarkable progress for multichannel speech separation. In parallel, the
integration of time domain network structure and beamforming also gains
significant attention. This study proposes a novel all-neural beamforming
method in time domain and makes an attempt to unify the all-neural beamforming
pipelines for time domain and frequency domain multichannel speech separation.
The proposed model consists of two modules: separation and beamforming. Both
modules perform temporal-spectral-spatial modeling and are trained from
end-to-end using a joint loss function. The novelty of this study lies in two
folds. Firstly, a time domain directional feature conditioned on the direction
of the target speaker is proposed, which can be jointly optimized within the
time domain architecture to enhance target signal estimation. Secondly, an
all-neural beamforming network in time domain is designed to refine the
pre-separated results. This module features with parametric time-variant
beamforming coefficient estimation, without explicitly following the derivation
of optimal filters that may lead to an upper bound. The proposed method is
evaluated on simulated reverberant overlapped speech data derived from the
AISHELL-1 corpus. Experimental results demonstrate significant performance
improvements over frequency domain state-of-the-arts, ideal magnitude masks and
existing time domain neural beamforming methods.",,
Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder,"[arxiv.Result.Author('Yusuke Yasuda'), arxiv.Result.Author('Tomoki Toda')]",2022-12-16 08:14:04+00:00,"Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of
the factors that have been driving TTS are the advancements of probabilistic
models and latent representation learning. We propose a TTS method based on
latent variable conversion using a diffusion probabilistic model and the
variational autoencoder (VAE). In our TTS method, we use a waveform model based
on VAE, a diffusion model that predicts the distribution of latent variables in
the waveform model from texts, and an alignment model that learns alignments
between the text and speech latent sequences. Our method integrates diffusion
with VAE by modeling both mean and variance parameters with diffusion, where
the target distribution is determined by approximation from VAE. This latent
variable conversion framework potentially enables us to flexibly incorporate
various latent feature extractors. Our experiments show that our method is
robust to linguistic labels with poor orthography and alignment errors.",Submitted to ICASSP 2023,
Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language,"[arxiv.Result.Author('Yusuke Yasuda'), arxiv.Result.Author('Tomoki Toda')]",2022-12-16 07:47:03+00:00,"End-to-end text-to-speech synthesis (TTS) can generate highly natural
synthetic speech from raw text. However, rendering the correct pitch accents is
still a challenging problem for end-to-end TTS. To tackle the challenge of
rendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a
self-supervised pretrained model in the character and phoneme domain for TTS.
We investigate the effects of features captured by PnG~BERT on Japanese TTS by
modifying the fine-tuning condition to determine the conditions helpful
inferring pitch accents. We manipulate content of PnG~BERT features from being
text-oriented to speech-oriented by changing the number of fine-tuned layers
during TTS. In addition, we teach PnG~BERT pitch accent information by
fine-tuning with tone prediction as an additional downstream task. Our
experimental results show that the features of PnG~BERT captured by pretraining
contain information helpful inferring pitch accent, and PnG~BERT outperforms
baseline Tacotron on accent correctness in a listening test.",,"IEEE Journal of Selected Topics in Signal Processing (Volume: 16,
  Issue: 6, October 2022)"
EffMulti: Efficiently Modeling Complex Multimodal Interactions for Emotion Analysis,"[arxiv.Result.Author('Feng Qiu'), arxiv.Result.Author('Chengyang Xie'), arxiv.Result.Author('Yu Ding'), arxiv.Result.Author('Wanzeng Kong')]",2022-12-16 03:05:55+00:00,"Humans are skilled in reading the interlocutor's emotion from multimodal
signals, including spoken words, simultaneous speech, and facial expressions.
It is still a challenge to effectively decode emotions from the complex
interactions of multimodal signals. In this paper, we design three kinds of
multimodal latent representations to refine the emotion analysis process and
capture complex multimodal interactions from different views, including a
intact three-modal integrating representation, a modality-shared
representation, and three modality-individual representations. Then, a
modality-semantic hierarchical fusion is proposed to reasonably incorporate
these representations into a comprehensive interaction representation. The
experimental results demonstrate that our EffMulti outperforms the
state-of-the-art methods. The compelling performance benefits from its
well-designed framework with ease of implementation, lower computing
complexity, and less trainable parameters.","6 pages,1 figure",
MAViL: Masked Audio-Video Learners,"[arxiv.Result.Author('Po-Yao Huang'), arxiv.Result.Author('Vasu Sharma'), arxiv.Result.Author('Hu Xu'), arxiv.Result.Author('Chaitanya Ryali'), arxiv.Result.Author('Haoqi Fan'), arxiv.Result.Author('Yanghao Li'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Gargi Ghosh'), arxiv.Result.Author('Jitendra Malik'), arxiv.Result.Author('Christoph Feichtenhofer')]",2022-12-15 18:59:59+00:00,"We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks. Code will be available
soon.",Technical report,
UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units,"[arxiv.Result.Author('Hirofumi Inaguma'), arxiv.Result.Author('Sravya Popuri'), arxiv.Result.Author('Ilia Kulikov'), arxiv.Result.Author('Peng-Jen Chen'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Yu-An Chung'), arxiv.Result.Author('Yun Tang'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Juan Pino')]",2022-12-15 18:58:28+00:00,"Direct speech-to-speech translation (S2ST), in which all components can be
optimized jointly, is advantageous over cascaded approaches to achieve fast
inference with a simplified pipeline. We present a novel two-pass direct S2ST
architecture, {\textit UnitY}, which first generates textual representations
and predicts discrete acoustic units subsequently. We enhance the model
performance by subword prediction in the first-pass decoder, advanced two-pass
decoder architecture design and search strategy, and better training
regularization. To leverage large amounts of unlabeled text data, we pre-train
the first-pass text decoder based on the self-supervised denoising
auto-encoding task. Experimental evaluations on benchmark datasets at various
data scales demonstrate that UnitY outperforms a single-pass speech-to-unit
translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show
that the proposed methods boost the performance even when predicting
spectrogram in the second pass. However, predicting discrete units achieves
2.51x decoding speed-up compared to that case.",Early draft. Work in progress,
Vision Transformers are Parameter-Efficient Audio-Visual Learners,"[arxiv.Result.Author('Yan-Bo Lin'), arxiv.Result.Author('Yi-Lin Sung'), arxiv.Result.Author('Jie Lei'), arxiv.Result.Author('Mohit Bansal'), arxiv.Result.Author('Gedas Bertasius')]",2022-12-15 17:31:54+00:00,"Vision transformers (ViTs) have achieved impressive results on various
computer vision tasks in the last several years. In this work, we study the
capability of frozen ViTs, pretrained only on visual data, to generalize to
audio-visual data without finetuning any of its original parameters. To do so,
we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained
ViTs to audio-visual tasks by injecting a small number of trainable parameters
into every layer of a frozen ViT. To efficiently fuse visual and audio cues,
our LAVISH adapter uses a small set of latent tokens, which form an attention
bottleneck, thus, eliminating the quadratic cost of standard cross-attention.
Compared to the existing modality-specific audio-visual methods, our approach
achieves competitive or even better performance on various audio-visual tasks
while using fewer tunable parameters and without relying on costly audio
pretraining or external audio encoders. Our code is available at
https://genjib.github.io/project_page/LAVISH/",project page: https://genjib.github.io/project_page/LAVISH/,
RWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis,"[arxiv.Result.Author('Shinhyeok Oh'), arxiv.Result.Author('HyeongRae Noh'), arxiv.Result.Author('Yoonseok Hong'), arxiv.Result.Author('Insoo Oh')]",2022-12-15 16:17:03+00:00,"With the advent of deep learning, a huge number of text-to-speech (TTS)
models which produce human-like speech have emerged. Recently, by introducing
syntactic and semantic information w.r.t the input text, various approaches
have been proposed to enrich the naturalness and expressiveness of TTS models.
Although these strategies showed impressive results, they still have some
limitations in utilizing language information. First, most approaches only use
graph networks to utilize syntactic and semantic information without
considering linguistic features. Second, most previous works do not explicitly
consider adjacent words when encoding syntactic and semantic information, even
though it is obvious that adjacent words are usually meaningful when encoding
the current word. To address these issues, we propose Relation-aware Word
Encoding Network (RWEN), which effectively allows syntactic and semantic
information based on two modules (i.e., Semantic-level Relation Encoding and
Adjacent Word Relation Encoding). Experimental results show substantial
improvements compared to previous works.",Accepted to AAAI 2023,
Audio-based AI classifiers show no evidence of improved COVID-19 screening over simple symptoms checkers,"[arxiv.Result.Author('Harry Coppock'), arxiv.Result.Author('George Nicholson'), arxiv.Result.Author('Ivan Kiskin'), arxiv.Result.Author('Vasiliki Koutra'), arxiv.Result.Author('Kieran Baker'), arxiv.Result.Author('Jobie Budd'), arxiv.Result.Author('Richard Payne'), arxiv.Result.Author('Emma Karoune'), arxiv.Result.Author('David Hurley'), arxiv.Result.Author('Alexander Titcomb'), arxiv.Result.Author('Sabrina Egglestone'), arxiv.Result.Author('Ana Tendero Cañadas'), arxiv.Result.Author('Lorraine Butler'), arxiv.Result.Author('Radka Jersakova'), arxiv.Result.Author('Jonathon Mellor'), arxiv.Result.Author('Selina Patel'), arxiv.Result.Author('Tracey Thornley'), arxiv.Result.Author('Peter Diggle'), arxiv.Result.Author('Sylvia Richardson'), arxiv.Result.Author('Josef Packham'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Davide Pigoli'), arxiv.Result.Author('Steven Gilmour'), arxiv.Result.Author('Stephen Roberts'), arxiv.Result.Author('Chris Holmes')]",2022-12-15 15:44:02+00:00,"Recent work has reported that AI classifiers trained on audio recordings can
accurately predict severe acute respiratory syndrome coronavirus 2 (SARSCoV2)
infection status. Here, we undertake a large scale study of audio-based deep
learning classifiers, as part of the UK governments pandemic response. We
collect and analyse a dataset of audio recordings from 67,842 individuals with
linked metadata, including reverse transcription polymerase chain reaction
(PCR) test outcomes, of whom 23,514 tested positive for SARS CoV 2. Subjects
were recruited via the UK governments National Health Service Test-and-Trace
programme and the REal-time Assessment of Community Transmission (REACT)
randomised surveillance survey. In an unadjusted analysis of our dataset AI
classifiers predict SARS-CoV-2 infection status with high accuracy (Receiver
Operating Characteristic Area Under the Curve (ROCAUC) 0.846 [0.838, 0.854])
consistent with the findings of previous studies. However, after matching on
measured confounders, such as age, gender, and self reported symptoms, our
classifiers performance is much weaker (ROC-AUC 0.619 [0.594, 0.644]). Upon
quantifying the utility of audio based classifiers in practical settings, we
find them to be outperformed by simple predictive scores based on user reported
symptoms.",,
Attention as a guide for Simultaneous Speech Translation,"[arxiv.Result.Author('Sara Papi'), arxiv.Result.Author('Matteo Negri'), arxiv.Result.Author('Marco Turchi')]",2022-12-15 14:18:53+00:00,"The study of the attention mechanism has sparked interest in many fields,
such as language modeling and machine translation. Although its patterns have
been exploited to perform different tasks, from neural network understanding to
textual alignment, no previous work has analysed the encoder-decoder attention
behavior in speech translation (ST) nor used it to improve ST on a specific
task. In this paper, we fill this gap by proposing an attention-based policy
(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the
existing attention relations between audio input and textual output. Its goal
is to leverage the encoder-decoder attention scores to guide inference in real
time. Results on en->{de, es} show that the EDAtt policy achieves overall
better results compared to the SimulST state of the art, especially in terms of
computational-aware latency.",,
Statistical Design and Analysis for Robust Machine Learning: A Case Study from COVID-19,"[arxiv.Result.Author('Davide Pigoli'), arxiv.Result.Author('Kieran Baker'), arxiv.Result.Author('Jobie Budd'), arxiv.Result.Author('Lorraine Butler'), arxiv.Result.Author('Harry Coppock'), arxiv.Result.Author('Sabrina Egglestone'), arxiv.Result.Author('Steven G. Gilmour'), arxiv.Result.Author('Chris Holmes'), arxiv.Result.Author('David Hurley'), arxiv.Result.Author('Radka Jersakova'), arxiv.Result.Author('Ivan Kiskin'), arxiv.Result.Author('Vasiliki Koutra'), arxiv.Result.Author('Jonathon Mellor'), arxiv.Result.Author('George Nicholson'), arxiv.Result.Author('Joe Packham'), arxiv.Result.Author('Selina Patel'), arxiv.Result.Author('Richard Payne'), arxiv.Result.Author('Stephen J. Roberts'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Ana Tendero-Cañadas'), arxiv.Result.Author('Tracey Thornley'), arxiv.Result.Author('Alexander Titcomb')]",2022-12-15 13:50:13+00:00,"Since early in the coronavirus disease 2019 (COVID-19) pandemic, there has
been interest in using artificial intelligence methods to predict COVID-19
infection status based on vocal audio signals, for example cough recordings.
However, existing studies have limitations in terms of data collection and of
the assessment of the performances of the proposed predictive models. This
paper rigorously assesses state-of-the-art machine learning techniques used to
predict COVID-19 infection status based on vocal audio signals, using a dataset
collected by the UK Health Security Agency. This dataset includes acoustic
recordings and extensive study participant meta-data. We provide guidelines on
testing the performance of methods to classify COVID-19 infection status based
on acoustic features and we discuss how these can be extended more generally to
the development and assessment of predictive methods based on public health
datasets.",,
A large-scale and PCR-referenced vocal audio dataset for COVID-19,"[arxiv.Result.Author('Jobie Budd'), arxiv.Result.Author('Kieran Baker'), arxiv.Result.Author('Emma Karoune'), arxiv.Result.Author('Harry Coppock'), arxiv.Result.Author('Selina Patel'), arxiv.Result.Author('Ana Tendero Cañadas'), arxiv.Result.Author('Alexander Titcomb'), arxiv.Result.Author('Richard Payne'), arxiv.Result.Author('David Hurley'), arxiv.Result.Author('Sabrina Egglestone'), arxiv.Result.Author('Lorraine Butler'), arxiv.Result.Author('Jonathon Mellor'), arxiv.Result.Author('George Nicholson'), arxiv.Result.Author('Ivan Kiskin'), arxiv.Result.Author('Vasiliki Koutra'), arxiv.Result.Author('Radka Jersakova'), arxiv.Result.Author('Rachel A. McKendry'), arxiv.Result.Author('Peter Diggle'), arxiv.Result.Author('Sylvia Richardson'), arxiv.Result.Author('Björn W. Schuller'), arxiv.Result.Author('Steven Gilmour'), arxiv.Result.Author('Davide Pigoli'), arxiv.Result.Author('Stephen Roberts'), arxiv.Result.Author('Josef Packham'), arxiv.Result.Author('Tracey Thornley'), arxiv.Result.Author('Chris Holmes')]",2022-12-15 11:40:40+00:00,"The UK COVID-19 Vocal Audio Dataset is designed for the training and
evaluation of machine learning models that classify SARS-CoV-2 infection status
or associated respiratory symptoms using vocal audio. The UK Health Security
Agency recruited voluntary participants through the national Test and Trace
programme and the REACT-1 survey in England from March 2021 to March 2022,
during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and
some Omicron variant sublineages. Audio recordings of volitional coughs,
exhalations, and speech were collected in the 'Speak up to help beat
coronavirus' digital survey alongside demographic, self-reported symptom and
respiratory condition data, and linked to SARS-CoV-2 test results. The UK
COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2
PCR-referenced audio recordings to date. PCR results were linked to 70,794 of
72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms
were reported by 45.62% of participants. This dataset has additional potential
uses for bioacoustics research, with 11.30% participants reporting asthma, and
27.20% with linked influenza PCR test results.","36 pages, 4 figures",
Improving Fast-slow Encoder based Transducer with Streaming Deliberation,"[arxiv.Result.Author('Ke Li'), arxiv.Result.Author('Jay Mahadeokar'), arxiv.Result.Author('Jinxi Guo'), arxiv.Result.Author('Yangyang Shi'), arxiv.Result.Author('Gil Keren'), arxiv.Result.Author('Ozlem Kalinli'), arxiv.Result.Author('Michael L. Seltzer'), arxiv.Result.Author('Duc Le')]",2022-12-15 08:16:46+00:00,"This paper introduces a fast-slow encoder based transducer with streaming
deliberation for end-to-end automatic speech recognition. We aim to improve the
recognition accuracy of the fast-slow encoder based transducer while keeping
its latency low by integrating a streaming deliberation model. Specifically,
the deliberation model leverages partial hypotheses from the streaming fast
encoder and implicitly learns to correct recognition errors. We modify the
parallel beam search algorithm for fast-slow encoder based transducer to be
efficient and compatible with the deliberation model. In addition, the
deliberation model is designed to process streaming data. To further improve
the deliberation performance, a simple text augmentation approach is explored.
We also compare LSTM and Conformer models for encoding partial hypotheses.
Experiments on Librispeech and in-house data show relative WER reductions
(WERRs) from 3% to 5% with a slight increase in model size and negligible extra
token emission latency compared with fast-slow encoder based transducer.
Compared with vanilla neural transducers, the proposed deliberation model
together with fast-slow encoder based transducer obtains relative 10-11% WERRs
on Librispeech and around relative 6% WERR on in-house data with smaller
emission delays.",Submitted to ICASSP 2023,
DeFT-AN: Dense Frequency-Time Attentive Network for Multichannel Speech Enhancement,"[arxiv.Result.Author('Dongheon Lee'), arxiv.Result.Author('Jung-Woo Choi')]",2022-12-15 01:03:18+00:00,"In this study, we propose a dense frequency-time attentive network (DeFT-AN)
for multichannel speech enhancement. DeFT-AN is a mask estimation network that
predicts a complex spectral masking pattern for suppressing the noise and
reverberation embedded in the short-time Fourier transform (STFT) of an input
signal. The proposed mask estimation network incorporates three different types
of blocks for aggregating information in the spatial, spectral, and temporal
dimensions. It utilizes a spectral transformer with a modified feed-forward
network and a temporal conformer with sequential dilated convolutions. The use
of dense blocks and transformers dedicated to the three different
characteristics of audio signals enables more comprehensive enhancement in
noisy and reverberant environments. The remarkable performance of DeFT-AN over
state-of-the-art multichannel models is demonstrated based on two popular noisy
and reverberant datasets in terms of various metrics for speech quality and
intelligibility.","5 pages, 2 figures, 3 tables",
"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language","[arxiv.Result.Author('Alexei Baevski'), arxiv.Result.Author('Arun Babu'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Michael Auli')]",2022-12-14 22:13:11+00:00,"Current self-supervised learning algorithms are often modality-specific and
require large amounts of computational resources. To address these issues, we
increase the training efficiency of data2vec, a learning objective that
generalizes across several modalities. We do not encode masked tokens, use a
fast convolutional decoder and amortize the effort to build teacher
representations. data2vec 2.0 benefits from the rich contextualized target
representations introduced in data2vec which enable a fast self-supervised
learner. Experiments on ImageNet-1K image classification show that data2vec 2.0
matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,
on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x
less time, and on GLUE natural language understanding it matches a retrained
RoBERTa model in half the time. Trading some speed for accuracy results in
ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.",,
Tackling the Cocktail Fork Problem for Separation and Transcription of Real-World Soundtracks,"[arxiv.Result.Author('Darius Petermann'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Aswin Shanmugam Subramanian'), arxiv.Result.Author('Zhong-Qiu Wang'), arxiv.Result.Author('Jonathan Le Roux')]",2022-12-14 16:47:43+00:00,"Emulating the human ability to solve the cocktail party problem, i.e., focus
on a source of interest in a complex acoustic scene, is a long standing goal of
audio source separation research. Much of this research investigates separating
speech from noise, speech from speech, musical instruments from each other, or
sound events from each other. In this paper, we focus on the cocktail fork
problem, which takes a three-pronged approach to source separation by
separating an audio mixture such as a movie soundtrack or podcast into the
three broad categories of speech, music, and sound effects (SFX - understood to
include ambient noise and natural sound events). We benchmark the performance
of several deep learning-based source separation models on this task and
evaluate them with respect to simple objective measures such as
signal-to-distortion ratio (SDR) as well as objective metrics that better
correlate with human perception. Furthermore, we thoroughly evaluate how source
separation can influence downstream transcription tasks. First, we investigate
the task of activity detection on the three sources as a way to both further
improve source separation and perform transcription. We formulate the
transcription tasks as speech recognition for speech and audio tagging for
music and SFX. We observe that, while the use of source separation estimates
improves transcription performance in comparison to the original soundtrack,
performance is still sub-optimal due to artifacts introduced by the separation
process. Therefore, we thoroughly investigate how remixing of the three
separated source stems at various relative levels can reduce artifacts and
consequently improve the transcription performance. We find that remixing music
and SFX interferences at a target SNR of 17.5 dB reduces speech recognition
word error rate, and similar impact from remixing is observed for tagging music
and SFX content.","Submitted to IEEE TASLP (In review), 13 pages, 6 figures",
"Quotations, Coreference Resolution, and Sentiment Annotations in Croatian News Articles: An Exploratory Study","[arxiv.Result.Author('Jelena Sarajlić'), arxiv.Result.Author('Gaurish Thakkar'), arxiv.Result.Author('Diego Alves'), arxiv.Result.Author('Nives Mikelic Preradović')]",2022-12-14 11:54:12+00:00,"This paper presents a corpus annotated for the task of direct-speech
extraction in Croatian. The paper focuses on the annotation of the quotation,
co-reference resolution, and sentiment annotation in SETimes news corpus in
Croatian and on the analysis of its language-specific differences compared to
English. From this, a list of the phenomena that require special attention when
performing these annotations is derived. The generated corpus with quotation
features annotations can be used for multiple tasks in the field of Natural
Language Processing.",,
Speech and Natural Language Processing Technologies for Pseudo-Pilot Simulator,"[arxiv.Result.Author('Amrutha Prasad'), arxiv.Result.Author('Juan Zuluaga-Gomez'), arxiv.Result.Author('Petr Motlicek'), arxiv.Result.Author('Saeed Sarfjoo'), arxiv.Result.Author('Iuliia Nigmatulina'), arxiv.Result.Author('Karel Vesely')]",2022-12-14 11:34:59+00:00,"This paper describes a simple yet efficient repetition-based modular system
for speeding up air-traffic controllers (ATCos) training. E.g., a human pilot
is still required in EUROCONTROL's ESCAPE lite simulator (see
https://www.eurocontrol.int/simulator/escape) during ATCo training. However,
this need can be substituted by an automatic system that could act as a pilot.
In this paper, we aim to develop and integrate a pseudo-pilot agent into the
ATCo training pipeline by merging diverse artificial intelligence (AI) powered
modules. The system understands the voice communications issued by the ATCo,
and, in turn, it generates a spoken prompt that follows the pilot's phraseology
to the initial communication. Our system mainly relies on open-source AI tools
and air traffic control (ATC) databases, thus, proving its simplicity and ease
of replicability. The overall pipeline is composed of the following: (1) a
submodule that receives and pre-processes the input stream of raw audio, (2) an
automatic speech recognition (ASR) system that transforms audio into a sequence
of words; (3) a high-level ATC-related entity parser, which extracts relevant
information from the communication, i.e., callsigns and commands, and finally,
(4) a speech synthesizer submodule that generates responses based on the
high-level ATC entities previously extracted. Overall, we show that this system
could pave the way toward developing a real proof-of-concept pseudo-pilot
system. Hence, speeding up the training of ATCos while drastically reducing its
overall cost.","Presented at Sesar Innovation Days 2022.
  https://www.sesarju.eu/sesarinnovationdays",
Multi-Scale Feature Fusion Transformer Network for End-to-End Single Channel Speech Separation,"[arxiv.Result.Author('Yinhao Xu'), arxiv.Result.Author('Jian Zhou'), arxiv.Result.Author('Liang Tao'), arxiv.Result.Author('Hon Keung Kwan')]",2022-12-14 11:32:28+00:00,"Recently studies on time-domain audio separation networks (TasNets) have made
a great stride in speech separation. One of the most representative TasNets is
a network with a dual-path segmentation approach. However, the original model
called DPRNN used a fixed feature dimension and unchanged segment size
throughout all layers of the network. In this paper, we propose a multi-scale
feature fusion transformer network (MSFFT-Net) based on the conventional
dual-path structure for single-channel speech separation. Unlike the
conventional dual-path structure where only one processing path exists,
adopting several iterative blocks with alternative intra-chunk and inter-chunk
operations to capture local and global context information, the proposed
MSFFT-Net has multiple parallel processing paths where the feature information
can be exchanged between multiple parallel processing paths. Experiments show
that our proposed networks based on multi-scale feature fusion structure have
achieved better results than the original dual-path model on the benchmark
dataset-WSJ0-2mix, where the SI-SNRi score of MSFFT-3P is 20.7dB (1.47%
improvement), and MSFFT-2P is 21.0dB (3.45% improvement), which achieves SOTA
on WSJ0-2mix without any data augmentation method.",,
Event-driven Spectrotemporal Feature Extraction and Classification using a Silicon Cochlea Model,"[arxiv.Result.Author('Ying Xu'), arxiv.Result.Author('Samalika Perera'), arxiv.Result.Author('Yeshwanth Bethi'), arxiv.Result.Author('Saeed Afshar'), arxiv.Result.Author('André van Schaik')]",2022-12-14 09:54:01+00:00,"This paper presents a reconfigurable digital implementation of an event-based
binaural cochlear system on a Field Programmable Gate Array (FPGA). It consists
of a pair of the Cascade of Asymmetric Resonators with Fast Acting Compression
(CAR FAC) cochlea models and leaky integrate and fire (LIF) neurons.
Additionally, we propose an event-driven SpectroTemporal Receptive Field (STRF)
Feature Extraction using Adaptive Selection Thresholds (FEAST). It is tested on
the TIDIGTIS benchmark and compared with current event-based auditory signal
processing approaches and neural networks.","12 pages, 8 figures",
Probing Deep Speaker Embeddings for Speaker-related Tasks,"[arxiv.Result.Author('Zifeng Zhao'), arxiv.Result.Author('Ding Pan'), arxiv.Result.Author('Junyi Peng'), arxiv.Result.Author('Rongzhi Gu')]",2022-12-14 07:33:36+00:00,"Deep speaker embeddings have shown promising results in speaker recognition,
as well as in other speaker-related tasks. However, some issues are still under
explored, for instance, the information encoded in these representations and
their influence on downstream tasks. Four deep speaker embeddings are studied
in this paper, namely, d-vector, x-vector, ResNetSE-34 and ECAPA-TDNN. Inspired
by human voice mechanisms, we explored possibly encoded information from
perspectives of identity, contents and channels; Based on this, experiments
were conducted on three categories of speaker-related tasks to further explore
impacts of different deep embeddings, including discriminative tasks (speaker
verification and diarization), guiding tasks (target speaker detection and
extraction) and regulating tasks (multi-speaker text-to-speech). Results show
that all deep embeddings encoded channel and content information in addition to
speaker identity, but the extent could vary and their performance on
speaker-related tasks can be tremendously different: ECAPA-TDNN is dominant in
discriminative tasks, and d-vector leads the guiding tasks, while regulating
task is less sensitive to the choice of speaker representations. These may
benefit future research utilizing speaker embeddings.",,
CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos,"[arxiv.Result.Author('Hao-Wen Dong'), arxiv.Result.Author('Naoya Takahashi'), arxiv.Result.Author('Yuki Mitsufuji'), arxiv.Result.Author('Julian McAuley'), arxiv.Result.Author('Taylor Berg-Kirkpatrick')]",2022-12-14 07:21:45+00:00,"Recent years have seen progress beyond domain-specific sound separation for
speech or music towards universal sound separation for arbitrary sounds. Prior
work on universal sound separation has investigated separating a target sound
out of an audio mixture given a text query. Such text-queried sound separation
systems provide a natural and scalable interface for specifying arbitrary
target sounds. However, supervised text-queried sound separation systems
require costly labeled audio-text pairs for training. Moreover, the audio
provided in existing datasets is often recorded in a controlled environment,
causing a considerable generalization gap to noisy audio in the wild. In this
work, we aim to approach text-queried universal sound separation by using only
unlabeled data. We propose to leverage the visual modality as a bridge to learn
the desired audio-textual correspondence. The proposed CLIPSep model first
encodes the input query into a query vector using the contrastive
language-image pretraining (CLIP) model, and the query vector is then used to
condition an audio separation model to separate out the target sound. While the
model is trained on image-audio pairs extracted from unlabeled videos, at test
time we can instead query the model with text inputs in a zero-shot setting,
thanks to the joint language-image embedding learned by the CLIP model.
Further, videos in the wild often contain off-screen sounds and background
noise that may hinder the model from learning the desired audio-textual
correspondence. To address this problem, we further propose an approach called
noise invariant training for training a query-based sound separation model on
noisy data. Experimental results show that the proposed models successfully
learn text-queried universal sound separation using only noisy unlabeled
videos, even achieving competitive performance against a supervised model in
some settings.",,
Efficient Speech Representation Learning with Low-Bit Quantization,"[arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Paden Tomasello'), arxiv.Result.Author('Abdelrahman Mohamed')]",2022-12-14 06:09:08+00:00,"With the development of hardware for machine learning, newer models often
come at the cost of both increased sizes and computational complexity. In
effort to improve the efficiency for these models, we apply and investigate
recent quantization techniques on speech representation learning models. The
quantization techniques were evaluated on the SUPERB benchmark. On the ASR
task, with aggressive quantization to 1 bit, we achieved 86.32% storage
reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12)
with increased word error rate (7.06 -> 15.96). In comparison with
DistillHuBERT which also aims for model compression, the 2-bit configuration
yielded slightly smaller storage (35.84 vs. 46.98), better word error rate
(12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",7 pages,
AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,"[arxiv.Result.Author('Dhrubajyoti Pathak'), arxiv.Result.Author('Sukumar Nandi'), arxiv.Result.Author('Priyankoo Sarmah')]",2022-12-14 05:36:18+00:00,"Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.",Accepted in AICCSA 2022,
DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect,"[arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Zhenhui Ye'), arxiv.Result.Author('Qian Chen'), arxiv.Result.Author('Siqi Zheng'), arxiv.Result.Author('Wen Wang'), arxiv.Result.Author('Qinglin Zhang'), arxiv.Result.Author('Zhou Zhao')]",2022-12-14 03:18:21+00:00,"Recently, binaural audio synthesis (BAS) has emerged as a promising research
field for its applications in augmented and virtual realities. Binaural audio
helps us to orient ourselves and establish immersion by providing the brain
with interaural time differences reflecting spatial information. However,
existing methods are limited in terms of phase estimation, which is crucial for
spatial hearing. In this paper, we propose the DopplerBAS method to explicitly
address the Doppler effect of the moving sound source. Specifically, we
calculate the radial relative velocity of the moving speaker in spherical
coordinates, which further guides the synthesis of binaural audio. This simple
method neither introduces any additional hyper-parameters nor modifies the loss
functions, and is plug-and-play: it scales well to different types of
backbones. DopplerBAS distinctly improves WarpNet and BinauralGrad in the phase
error metric and reaches a new state-of-the-art: 0.780 (vs. the current
state-of-the-art 0.807). Experiments and ablation studies demonstrate the
effectiveness of our method.",key words: binaural audio; stereophonic sound,
Disentangling Prosody Representations with Unsupervised Speech Reconstruction,"[arxiv.Result.Author('Leyuan Qu'), arxiv.Result.Author('Taihao Li'), arxiv.Result.Author('Cornelius Weber'), arxiv.Result.Author('Theresa Pekarek-Rosin'), arxiv.Result.Author('Fuji Ren'), arxiv.Result.Author('Stefan Wermter')]",2022-12-14 01:37:35+00:00,"Human speech can be characterized by different components, including semantic
content, speaker identity and prosodic information. Significant progress has
been made in disentangling representations for semantic content and speaker
identity in Automatic Speech Recognition (ASR) and speaker verification tasks
respectively. However, it is still an open challenging research question to
extract prosodic information because of the intrinsic association of different
attributes, such as timbre and rhythm, and because of the need for unsupervised
training schemes to achieve robust large-scale and speaker-independent ASR. The
aim of this paper is to address the disentanglement of emotional prosody from
speech based on unsupervised reconstruction. Specifically, we identify, design,
implement and integrate three crucial components in our proposed speech
reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech
signals into discrete units for semantic content, (2) a pretrained speaker
verification model to generate speaker identity embeddings, and (3) a trainable
prosody encoder to learn prosody representations. We first pretrain the
Prosody2Vec representations on unlabelled emotional speech corpora, then
fine-tune the model on specific datasets to perform Speech Emotion Recognition
(SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective
evaluations on the EVC task suggest that Prosody2Vec effectively captures
general prosodic features that can be smoothly transferred to other emotional
speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the
prosody features learned by Prosody2Vec are complementary and beneficial for
the performance of widely used speech pretraining models and surpass the
state-of-the-art methods when combining Prosody2Vec with HuBERT
representations. Some audio samples can be found on our demo website.",,
RT-1: Robotics Transformer for Real-World Control at Scale,"[arxiv.Result.Author('Anthony Brohan'), arxiv.Result.Author('Noah Brown'), arxiv.Result.Author('Justice Carbajal'), arxiv.Result.Author('Yevgen Chebotar'), arxiv.Result.Author('Joseph Dabis'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Keerthana Gopalakrishnan'), arxiv.Result.Author('Karol Hausman'), arxiv.Result.Author('Alex Herzog'), arxiv.Result.Author('Jasmine Hsu'), arxiv.Result.Author('Julian Ibarz'), arxiv.Result.Author('Brian Ichter'), arxiv.Result.Author('Alex Irpan'), arxiv.Result.Author('Tomas Jackson'), arxiv.Result.Author('Sally Jesmonth'), arxiv.Result.Author('Nikhil J Joshi'), arxiv.Result.Author('Ryan Julian'), arxiv.Result.Author('Dmitry Kalashnikov'), arxiv.Result.Author('Yuheng Kuang'), arxiv.Result.Author('Isabel Leal'), arxiv.Result.Author('Kuang-Huei Lee'), arxiv.Result.Author('Sergey Levine'), arxiv.Result.Author('Yao Lu'), arxiv.Result.Author('Utsav Malla'), arxiv.Result.Author('Deeksha Manjunath'), arxiv.Result.Author('Igor Mordatch'), arxiv.Result.Author('Ofir Nachum'), arxiv.Result.Author('Carolina Parada'), arxiv.Result.Author('Jodilyn Peralta'), arxiv.Result.Author('Emily Perez'), arxiv.Result.Author('Karl Pertsch'), arxiv.Result.Author('Jornell Quiambao'), arxiv.Result.Author('Kanishka Rao'), arxiv.Result.Author('Michael Ryoo'), arxiv.Result.Author('Grecia Salazar'), arxiv.Result.Author('Pannag Sanketi'), arxiv.Result.Author('Kevin Sayed'), arxiv.Result.Author('Jaspiar Singh'), arxiv.Result.Author('Sumedh Sontakke'), arxiv.Result.Author('Austin Stone'), arxiv.Result.Author('Clayton Tan'), arxiv.Result.Author('Huong Tran'), arxiv.Result.Author('Vincent Vanhoucke'), arxiv.Result.Author('Steve Vega'), arxiv.Result.Author('Quan Vuong'), arxiv.Result.Author('Fei Xia'), arxiv.Result.Author('Ted Xiao'), arxiv.Result.Author('Peng Xu'), arxiv.Result.Author('Sichun Xu'), arxiv.Result.Author('Tianhe Yu'), arxiv.Result.Author('Brianna Zitkovich')]",2022-12-13 18:55:15+00:00,"By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer.github.io",See website at robotics-transformer.github.io,
Predicting Knowledge Gain for MOOC Video Consumption,"[arxiv.Result.Author('Christian Otto'), arxiv.Result.Author('Markos Stamatakis'), arxiv.Result.Author('Anett Hoppe'), arxiv.Result.Author('Ralph Ewerth')]",2022-12-13 15:57:32+00:00,"Informal learning on the Web using search engines as well as more structured
learning on MOOC platforms have become very popular in recent years. As a
result of the vast amount of available learning resources, intelligent
retrieval and recommendation methods are indispensable -- this is true also for
MOOC videos. However, the automatic assessment of this content with regard to
predicting (potential) knowledge gain has not been addressed by previous work
yet. In this paper, we investigate whether we can predict learning success
after MOOC video consumption using 1) multimodal features covering slide and
speech content, and 2) a wide range of text-based features describing the
content of the video. In a comprehensive experimental setting, we test four
different classifiers and various feature subset combinations. We conduct a
detailed feature importance analysis to gain insights in which modality
benefits knowledge gain prediction the most.","13 pages, 1 figure, 3 tables","AIED 2022. Lecture Notes in Computer Science, vol 13356, pp.
  458-462"
"Lisan: Yemeni, Iraqi, Libyan, and Sudanese Arabic Dialect Copora with Morphological Annotations","[arxiv.Result.Author('Mustafa Jarrar'), arxiv.Result.Author('Fadi A Zaraket'), arxiv.Result.Author('Tymaa Hammouda'), arxiv.Result.Author('Daanish Masood Alavi'), arxiv.Result.Author('Martin Waahlisch')]",2022-12-13 10:37:10+00:00,"This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and
Libyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.
We collected the content of the corpora from several social media platforms.
The Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.
The corpora of the other three dialects (~ 50K tokens each) came manually from
Facebook and YouTube posts and comments.
  Thirty five (35) annotators who are native speakers of the target dialects
carried out the annotations. The annotators segemented all words in the four
corpora into prefixes, stems and suffixes and labeled each with different
morphological features such as part of speech, lemma, and a gloss in English.
An Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the
annation. The annotators were trained on a set of guidelines and on how to use
ADAT. We developed ADAT to assist the annotators and to ensure compatibility
with SAMA and Curras tagsets. The tool is open source, and the four corpora are
also available online.",,
Style-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and Speaker-wise Normalization in Speech Synthesis,"[arxiv.Result.Author('Chunyu Qiang'), arxiv.Result.Author('Peng Yang'), arxiv.Result.Author('Hao Che'), arxiv.Result.Author('Xiaorui Wang'), arxiv.Result.Author('Zhongyuan Wang')]",2022-12-13 06:26:25+00:00,"Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesised speech of a target speaker's timbre. Most
previous approaches rely on data with style labels, but manually-annotated
labels are expensive and not always reliable. In response to this problem, we
propose Style-Label-Free, a cross-speaker style transfer method, which can
realize the style transfer from source speaker to target speaker without style
labels. Firstly, a reference encoder structure based on quantized variational
autoencoder (Q-VAE) and style bottleneck is designed to extract discrete style
representations. Secondly, a speaker-wise batch normalization layer is proposed
to reduce the source speaker leakage. In order to improve the style extraction
ability of the reference encoder, a style invariant and contrastive data
augmentation method is proposed. Experimental results show that the method
outperforms the baseline. We provide a website with audio samples.",Published to ISCSLP 2022,
Towards trustworthy phoneme boundary detection with autoregressive model and improved evaluation metric,"[arxiv.Result.Author('Hyeongju Kim'), arxiv.Result.Author('Hyeong-Seok Choi')]",2022-12-13 05:56:57+00:00,"Phoneme boundary detection has been studied due to its central role in
various speech applications. In this work, we point out that this task needs to
be addressed not only by algorithmic way, but also by evaluation metric. To
this end, we first propose a state-of-the-art phoneme boundary detector that
operates in an autoregressive manner, dubbed SuperSeg. Experiments on the TIMIT
and Buckeye corpora demonstrates that SuperSeg identifies phoneme boundaries
with significant margin compared to existing models. Furthermore, we note that
there is a limitation on the popular evaluation metric, R-value, and propose
new evaluation metrics that prevent each boundary from contributing to
evaluation multiple times. The proposed metrics reveal the weaknesses of
non-autoregressive baselines and establishes a reliable criterion that suits
for evaluating phoneme boundary detection.","5 pages, submitted to ICASSP 2023",
Towards deep generation of guided wave representations for composite materials,"[arxiv.Result.Author('Mahindra Rautela'), arxiv.Result.Author('J. Senthilnath'), arxiv.Result.Author('Armin Huber'), arxiv.Result.Author('S. Gopalakrishnan')]",2022-12-13 04:35:36+00:00,"Laminated composite materials are widely used in most fields of engineering.
Wave propagation analysis plays an essential role in understanding the
short-duration transient response of composite structures. The forward
physics-based models are utilized to map from elastic properties space to wave
propagation behavior in a laminated composite material. Due to the
high-frequency, multi-modal, and dispersive nature of the guided waves, the
physics-based simulations are computationally demanding. It makes property
prediction, generation, and material design problems more challenging. In this
work, a forward physics-based simulator such as the stiffness matrix method is
utilized to collect group velocities of guided waves for a set of composite
materials. A variational autoencoder (VAE)-based deep generative model is
proposed for the generation of new and realistic polar group velocity
representations. It is observed that the deep generator is able to reconstruct
unseen representations with very low mean square reconstruction error. Global
Monte Carlo and directional equally-spaced samplers are used to sample the
continuous, complete and organized low-dimensional latent space of VAE. The
sampled point is fed into the trained decoder to generate new polar
representations. The network has shown exceptional generation capabilities. It
is also seen that the latent space forms a conceptual space where different
directions and regions show inherent patterns related to the generated
representations and their corresponding material properties.",,
Jointly Learning Visual and Auditory Speech Representations from Raw Data,"[arxiv.Result.Author('Alexandros Haliassos'), arxiv.Result.Author('Pingchuan Ma'), arxiv.Result.Author('Rodrigo Mira'), arxiv.Result.Author('Stavros Petridis'), arxiv.Result.Author('Maja Pantic')]",2022-12-12 21:04:06+00:00,"We present RAVEn, a self-supervised multi-modal approach to jointly learn
visual and auditory speech representations. Our pre-training objective involves
encoding masked inputs, and then predicting contextualised targets generated by
slowly-evolving momentum encoders. Driven by the inherent differences between
video and audio, our design is asymmetric w.r.t. the two modalities' pretext
tasks: Whereas the auditory stream predicts both the visual and auditory
targets, the visual one predicts only the auditory targets. We observe strong
results in low- and high-resource labelled data settings when fine-tuning the
visual and auditory encoders resulting from a single pre-training stage, in
which the encoders are jointly trained. Notably, RAVEn surpasses all
self-supervised methods on visual speech recognition (VSR) on LRS3, and
combining RAVEn with self-training using only 30 hours of labelled data even
outperforms a recent semi-supervised method trained on 90,000 hours of
non-public data. At the same time, we achieve state-of-the-art results in the
LRS3 low-resource setting for auditory speech recognition (as well as for VSR).
Our findings point to the viability of learning powerful speech representations
entirely from raw video and audio, i.e., without relying on handcrafted
features. Code and models will be made public.",22 pages,
Direct Speech-to-speech Translation without Textual Annotation using Bottleneck Features,"[arxiv.Result.Author('Junhui Zhang'), arxiv.Result.Author('Junjie Pan'), arxiv.Result.Author('Xiang Yin'), arxiv.Result.Author('Zejun Ma')]",2022-12-12 10:03:10+00:00,"Speech-to-speech translation directly translates a speech utterance to
another between different languages, and has great potential in tasks such as
simultaneous interpretation. State-of-art models usually contains an auxiliary
module for phoneme sequences prediction, and this requires textual annotation
of the training dataset. We propose a direct speech-to-speech translation model
which can be trained without any textual annotation or content information.
Instead of introducing an auxiliary phoneme prediction task in the model, we
propose to use bottleneck features as intermediate training objectives for our
model to ensure the translation performance of the system. Experiments on
Mandarin-Cantonese speech translation demonstrate the feasibility of the
proposed approach and the performance can match a cascaded system with respect
of translation and synthesis qualities.","4 pages, 3 figures",
Non-parallel Accent Conversion using Pseudo Siamese Disentanglement Network,"[arxiv.Result.Author('Dongya Jia'), arxiv.Result.Author('Qiao Tian'), arxiv.Result.Author('Jiaxin Li'), arxiv.Result.Author('Yuanzhe Chen'), arxiv.Result.Author('Kainan Peng'), arxiv.Result.Author('Mingbo Ma'), arxiv.Result.Author('Yuping Wang'), arxiv.Result.Author('Yuxuan Wang')]",2022-12-12 08:02:02+00:00,"The main goal of accent conversion (AC) is to convert the accent of speech
into the target accent while preserving the content and timbre. Previous
reference-based methods rely on reference utterances in the inference phase,
which limits their practical application. What's more, previous reference-free
methods mostly require parallel data in the training phase. In this paper, we
propose a reference-free method based on non-parallel data from the perspective
of feature disentanglement. Pseudo Siamese Disentanglement Network (PSDN) is
proposed to disentangle the accent information from the content representation
and model the target accent. Besides, a timbre augmentation method is proposed
to enhance the ability of timbre retaining for speakers without target-accent
data. Experimental results show that the proposed system can convert the accent
of native American English speech into Indian accent with higher accentedness
(3.47) than the baseline (2.75) and input (1.19). The naturalness of converted
speech is also comparable to that of the input.",,
TriNet: stabilizing self-supervised learning from complete or slow collapse,"[arxiv.Result.Author('Lixin Cao'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Ben Yang'), arxiv.Result.Author('Dan Su'), arxiv.Result.Author('Dong Yu')]",2022-12-12 05:55:07+00:00,"Self-supervised learning (SSL) models confront challenges of abrupt
informational collapse or slow dimensional collapse. We propose TriNet, which
introduces a novel triple-branch architecture for preventing collapse and
stabilizing the pre-training. Our experimental results show that the proposed
method notably stabilizes and accelerates pre-training and achieves a relative
word error rate reduction (WERR) of 5.32% compared to the state-of-the-art
(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code
at https://github.com/tencent-ailab/.",,
A Study of Slang Representation Methods,"[arxiv.Result.Author('Aravinda Kolla'), arxiv.Result.Author('Filip Ilievski'), arxiv.Result.Author('Hông-Ân Sandlin'), arxiv.Result.Author('Alain Mermoud')]",2022-12-11 21:56:44+00:00,"Considering the large amount of content created online by the minute,
slang-aware automatic tools are critically needed to promote social good, and
assist policymakers and moderators in restricting the spread of offensive
language, abuse, and hate speech. Despite the success of large language models
and the spontaneous emergence of slang dictionaries, it is unclear how far
their combination goes in terms of slang understanding for downstream social
good tasks. In this paper, we provide a framework to study different
combinations of representation learning models and knowledge resources for a
variety of downstream tasks that rely on slang understanding. Our experiments
show the superiority of models that have been pre-trained on social media data,
while the impact of dictionaries is positive only for static word embeddings.
Our error analysis identifies core challenges for slang representation
learning, including out-of-vocabulary words, polysemy, variance, and annotation
disagreements, which can be traced to characteristics of slang as a quickly
evolving and highly subjective language.",,
Multimodal and Explainable Internet Meme Classification,"[arxiv.Result.Author('Abhinav Kumar Thakur'), arxiv.Result.Author('Filip Ilievski'), arxiv.Result.Author('Hông-Ân Sandlin'), arxiv.Result.Author('Alain Mermoud'), arxiv.Result.Author('Zhivar Sourati'), arxiv.Result.Author('Luca Luceri'), arxiv.Result.Author('Riccardo Tommasini')]",2022-12-11 21:52:21+00:00,"Warning: this paper contains content that may be offensive or upsetting. In
the current context where online platforms have been effectively weaponized in
a variety of geo-political events and social issues, Internet memes make fair
content moderation at scale even more difficult. Existing work on meme
classification and tracking has focused on black-box methods that do not
explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.",,
MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset,"[arxiv.Result.Author('Kailin Liang'), arxiv.Result.Author('Bin Liu'), arxiv.Result.Author('Yifan Hu'), arxiv.Result.Author('Rui Liu'), arxiv.Result.Author('Feilong Bao'), arxiv.Result.Author('Guanglai Gao')]",2022-12-11 14:55:02+00:00,"Text-to-Speech (TTS) synthesis for low-resource languages is an attractive
research issue in academia and industry nowadays. Mongolian is the official
language of the Inner Mongolia Autonomous Region and a representative
low-resource language spoken by over 10 million people worldwide. However,
there is a relative lack of open-source datasets for Mongolian TTS. Therefore,
we make public an open-source multi-speaker Mongolian TTS dataset, named
MnTTS2, for the benefit of related researchers. In this work, we prepare the
transcription from various topics and invite three professional Mongolian
announcers to form a three-speaker TTS dataset, in which each announcer records
10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we
build the baseline system based on the state-of-the-art FastSpeech2 model and
HiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2
dataset is sufficient to build robust multi-speaker TTS models for real-world
applications. The MnTTS2 dataset, training recipe, and pretrained models are
released at: \url{https://github.com/ssmlkl/MnTTS2}",Accepted by NCMMSC'2022 (https://ncmmsc2022.ustc.edu.cn/main.htm),
AliCHI: A Large-scale Multi-modal Dataset and Automated Evaluation Tool for Human-like Dialogue Systems,"[arxiv.Result.Author('Zhiling Luo'), arxiv.Result.Author('Qiankun Shi'), arxiv.Result.Author('Sha Zhao'), arxiv.Result.Author('Wei Zhou'), arxiv.Result.Author('Haiqing Chen'), arxiv.Result.Author('Yuankai Ma'), arxiv.Result.Author('Haitao Leng')]",2022-12-11 12:33:53+00:00,"A well-designed interactive human-like dialogue system is expected to take
actions (e.g. smiling) and respond in a pattern similar to humans. However, due
to the limitation of single-modality (only speech) or small volume of currently
public datasets, most dialogue systems can only respond in speech and cannot
take human-like actions. In this work, we build a large-scale multi-modal
dataset of human-to-human conversation in a face-to-face fashion, with
fine-grained annotations. The raw data in video format contains 635 dialogue
sessions, being collected from 200 participants on designed topics and lasting
52 hours in total. Moreover, we manually annotated the verbal and non-verbal
behaviors in each dialogue session on their start/end timestamp. Furthermore,
we developed a corresponding evaluation tool for human-like dialogue systems to
automatically evaluates the accuracy of two basic tasks, turn-taking
prediction, and backchannel prediction, on both time and content. We have
opened the data, the tools will be released at the conference.",,
End-to-End Speech Translation of Arabic to English Broadcast News,"[arxiv.Result.Author('Fethi Bougares'), arxiv.Result.Author('Salim Jouili')]",2022-12-11 11:35:46+00:00,"Speech translation (ST) is the task of directly translating acoustic speech
signals in a source language into text in a foreign language. ST task has been
addressed, for a long time, using a pipeline approach with two modules : first
an Automatic Speech Recognition (ASR) in the source language followed by a
text-to-text Machine translation (MT). In the past few years, we have seen a
paradigm shift towards the end-to-end approaches using sequence-to-sequence
deep neural network models. This paper presents our efforts towards the
development of the first Broadcast News end-to-end Arabic to English speech
translation system. Starting from independent ASR and MT LDC releases, we were
able to identify about 92 hours of Arabic audio recordings for which the manual
transcription was also translated into English at the segment level. These data
was used to train and compare pipeline and end-to-end speech translation
systems under multiple scenarios including transfer learning and data
augmentation techniques.",Arabic Natural Language Processing Workshop 2022,
BASPRO: a balanced script producer for speech corpus collection based on the genetic algorithm,"[arxiv.Result.Author('Yu-Wen Chen'), arxiv.Result.Author('Hsin-Min Wang'), arxiv.Result.Author('Yu Tsao')]",2022-12-11 02:05:30+00:00,"The performance of speech-processing models is heavily influenced by the
speech corpus that is used for training and evaluation. In this study, we
propose BAlanced Script PROducer (BASPRO) system, which can automatically
construct a phonetically balanced and rich set of Chinese sentences for
collecting Mandarin Chinese speech data. First, we used pretrained natural
language processing systems to extract ten-character candidate sentences from a
large corpus of Chinese news texts. Then, we applied a genetic algorithm-based
method to select 20 phonetically balanced sentence sets, each containing 20
sentences, from the candidate sentences. Using BASPRO, we obtained a recording
script called TMNews, which contains 400 ten-character sentences. TMNews covers
84% of the syllables used in the real world. Moreover, the syllable
distribution has 0.96 cosine similarity to the real-world syllable
distribution. We converted the script into a speech corpus using two
text-to-speech systems. Using the designed speech corpus, we tested the
performances of speech enhancement (SE) and automatic speech recognition (ASR),
which are one of the most important regression- and classification-based speech
processing tasks, respectively. The experimental results show that the SE and
ASR models trained on the designed speech corpus outperform their counterparts
trained on a randomly composed speech corpus.",accepted by APSIPA Transactions on Signal and Information Processing,
Synthetic Wave-Geometric Impulse Responses for Improved Speech Dereverberation,"[arxiv.Result.Author('Rohith Aralikatti'), arxiv.Result.Author('Zhenyu Tang'), arxiv.Result.Author('Dinesh Manocha')]",2022-12-10 20:15:23+00:00,"We present a novel approach to improve the performance of learning-based
speech dereverberation using accurate synthetic datasets. Our approach is
designed to recover the reverb-free signal from a reverberant speech signal. We
show that accurately simulating the low-frequency components of Room Impulse
Responses (RIRs) is important to achieving good dereverberation. We use the GWA
dataset that consists of synthetic RIRs generated in a hybrid fashion: an
accurate wave-based solver is used to simulate the lower frequencies and
geometric ray tracing methods simulate the higher frequencies. We demonstrate
that speech dereverberation models trained on hybrid synthetic RIRs outperform
models trained on RIRs generated by prior geometric ray tracing methods on four
real-world RIR datasets.",Submitted to ICASSP 2023,
A Comparison of Audio Preprocessing Techniques and Deep Learning Algorithms for Raga Recognition,"[arxiv.Result.Author('Devayani Hebbar'), arxiv.Result.Author('Vandana Jagtap')]",2022-12-10 16:51:12+00:00,"Ragas form the foundation for Indian Classical Music. The task of Raga
Recognition has gained traction in the Music Information Retrieval community in
the recent past, which can be attributed to the nuances of Indian Classical
Music that have resulted in a plethora of research problems in Computing. In
this work, we used two different digital audio signal processing techniques to
preprocess audio samples of Carnatic classical ragas that were then processed
by various Deep Learning models. Their results were compared in order to infer
which DASP technique is better suited to the task of raga recognition. We
obtained state of the art results, with our best model reaching a testing
accuracy of 98.1%. We also compared each model ability to distinguish between
similar ragas.","7 pages, 6 figures, 7 tables",
Leveraging Modality-specific Representations for Audio-visual Speech Recognition via Reinforcement Learning,"[arxiv.Result.Author('Chen Chen'), arxiv.Result.Author('Yuchen Hu'), arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Heqing Zou'), arxiv.Result.Author('Beier Zhu'), arxiv.Result.Author('Eng Siong Chng')]",2022-12-10 14:01:54+00:00,"Audio-visual speech recognition (AVSR) has gained remarkable success for
ameliorating the noise-robustness of speech recognition. Mainstream methods
focus on fusing audio and visual inputs to obtain modality-invariant
representations. However, such representations are prone to over-reliance on
audio modality as it is much easier to recognize than video modality in clean
conditions. As a result, the AVSR model underestimates the importance of visual
stream in face of noise corruption. To this end, we leverage visual
modality-specific representations to provide stable complementary information
for the AVSR task. Specifically, we propose a reinforcement learning (RL) based
framework called MSRL, where the agent dynamically harmonizes
modality-invariant and modality-specific representations in the auto-regressive
decoding process. We customize a reward function directly related to
task-specific metrics (i.e., word error rate), which encourages the MSRL to
effectively explore the optimal integration strategy. Experimental results on
the LRS3 dataset show that the proposed method achieves state-of-the-art in
both clean and various noisy conditions. Furthermore, we demonstrate the better
generality of MSRL system than other baselines when test set contains unseen
noises.",Accepted by AAAI2023,
Variational Speech Waveform Compression to Catalyze Semantic Communications,"[arxiv.Result.Author('Shengshi Yao'), arxiv.Result.Author('Zixuan Xiao'), arxiv.Result.Author('Sixian Wang'), arxiv.Result.Author('Jincheng Dai'), arxiv.Result.Author('Kai Niu'), arxiv.Result.Author('Ping Zhang')]",2022-12-10 12:52:59+00:00,"We propose a novel neural waveform compression method to catalyze emerging
speech semantic communications. By introducing nonlinear transform and
variational modeling, we effectively capture the dependencies within speech
frames and estimate the probabilistic distribution of the speech feature more
accurately, giving rise to better compression performance. In particular, the
speech signals are analyzed and synthesized by a pair of nonlinear transforms,
yielding latent features. An entropy model with hyperprior is built to capture
the probabilistic distribution of latent features, followed with quantization
and entropy coding. The proposed waveform codec can be optimized flexibly
towards arbitrary rate, and the other appealing feature is that it can be
easily optimized for any differentiable loss function, including perceptual
loss used in semantic communications. To further improve the fidelity, we
incorporate residual coding to mitigate the degradation arising from
quantization distortion at the latent space. Results indicate that achieving
the same performance, the proposed method saves up to 27% coding rate than
widely used adaptive multi-rate wideband (AMR-WB) codec as well as emerging
neural waveform coding methods.",,
GPU-accelerated Guided Source Separation for Meeting Transcription,"[arxiv.Result.Author('Desh Raj'), arxiv.Result.Author('Daniel Povey'), arxiv.Result.Author('Sanjeev Khudanpur')]",2022-12-10 11:20:17+00:00,"Guided source separation (GSS) is a type of target-speaker extraction method
that relies on pre-computed speaker activities and blind source separation to
perform front-end enhancement of overlapped speech signals. It was first
proposed during the CHiME-5 challenge and provided significant improvements
over the delay-and-sum beamforming baseline. Despite its strengths, however,
the method has seen limited adoption for meeting transcription benchmarks
primarily due to its high computation time. In this paper, we describe our
improved implementation of GSS that leverages the power of modern GPU-based
pipelines, including batched processing of frequencies and segments, to provide
300x speed-up over CPU-based inference. The improved inference time allows us
to perform detailed ablation studies over several parameters of the GSS
algorithm -- such as context duration, number of channels, and noise class, to
name a few. We provide end-to-end reproducible pipelines for speaker-attributed
transcription of popular meeting benchmarks: LibriCSS, AMI, and AliMeeting. Our
code and recipes are publicly available: https://github.com/desh2608/gss.","7 pages, 4 figures. Code available at https://github.com/desh2608/gss",
Hyperbolic Audio Source Separation,"[arxiv.Result.Author('Darius Petermann'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Aswin Subramanian'), arxiv.Result.Author('Jonathan Le Roux')]",2022-12-09 17:47:48+00:00,"We introduce a framework for audio source separation using embeddings on a
hyperbolic manifold that compactly represent the hierarchical relationship
between sound sources and time-frequency features. Inspired by recent successes
modeling hierarchical relationships in text and images with hyperbolic
embeddings, our algorithm obtains a hyperbolic embedding for each
time-frequency bin of a mixture signal and estimates masks using hyperbolic
softmax layers. On a synthetic dataset containing mixtures of multiple people
talking and musical instruments playing, our hyperbolic model performed
comparably to a Euclidean baseline in terms of source to distortion ratio, with
stronger performance at low embedding dimensions. Furthermore, we find that
time-frequency regions containing multiple overlapping sources are embedded
towards the center (i.e., the most uncertain region) of the hyperbolic space,
and we can use this certainty estimate to efficiently trade-off between
artifact introduction and interference reduction when isolating individual
sounds.","Submitted to ICASSP 2023, Demo page:
  https://darius522.github.io/hyperbolic-audio-sep/",
Memories are One-to-Many Mapping Alleviators in Talking Face Generation,"[arxiv.Result.Author('Anni Tang'), arxiv.Result.Author('Tianyu He'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Jun Ling'), arxiv.Result.Author('Runnan Li'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Li Song'), arxiv.Result.Author('Jiang Bian')]",2022-12-09 17:45:36+00:00,"Talking face generation aims at generating photo-realistic video portraits of
a target person driven by input audio. Due to its nature of one-to-many mapping
from the input audio to the output video (e.g., one speech content may have
multiple feasible visual appearances), learning a deterministic mapping like
previous works brings ambiguity during training, and thus causes inferior
visual results. Although this one-to-many mapping could be alleviated in part
by a two-stage framework (i.e., an audio-to-expression model followed by a
neural-rendering model), it is still insufficient since the prediction is
produced without enough information (e.g., emotions, wrinkles, etc.). In this
paper, we propose MemFace to complement the missing information with an
implicit memory and an explicit memory that follow the sense of the two stages
respectively. More specifically, the implicit memory is employed in the
audio-to-expression model to capture high-level semantics in the
audio-expression shared space, while the explicit memory is employed in the
neural-rendering model to help synthesize pixel-level details. Our experimental
results show that our proposed MemFace surpasses all the state-of-the-art
results across multiple scenarios consistently and significantly.",Project page: see https://memoryface.github.io,
Uncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models,"[arxiv.Result.Author('Huajian Fang'), arxiv.Result.Author('Timo Gerkmann')]",2022-12-09 13:03:09+00:00,"Single-channel deep speech enhancement approaches often estimate a single
multiplicative mask to extract clean speech without a measure of its accuracy.
Instead, in this work, we propose to quantify the uncertainty associated with
clean speech estimates in neural network-based speech enhancement. Predictive
uncertainty is typically categorized into aleatoric uncertainty and epistemic
uncertainty. The former accounts for the inherent uncertainty in data and the
latter corresponds to the model uncertainty. Aiming for robust clean speech
estimation and efficient predictive uncertainty quantification, we propose to
integrate statistical complex Gaussian mixture models (CGMMs) into a deep
speech enhancement framework. More specifically, we model the dependency
between input and output stochastically by means of a conditional probability
density and train a neural network to map the noisy input to the full posterior
distribution of clean speech, modeled as a mixture of multiple complex Gaussian
components. Experimental results on different datasets show that the proposed
algorithm effectively captures predictive uncertainty and that combining
powerful statistical models and deep learning also delivers a superior speech
enhancement performance.","5 pages, 4 figures",
Geometry-aware DoA Estimation using a Deep Neural Network with mixed-data input features,"[arxiv.Result.Author('Ulrik Kowalk'), arxiv.Result.Author('Simon Doclo'), arxiv.Result.Author('Joerg Bitzer')]",2022-12-09 11:41:16+00:00,"Unlike model-based direction of arrival (DoA) estimation algorithms,
supervised learning-based DoA estimation algorithms based on deep neural
networks (DNNs) are usually trained for one specific microphone array geometry,
resulting in poor performance when applied to a different array geometry. In
this paper we illustrate the fundamental difference between supervised
learning-based and model-based algorithms leading to this sensitivity. Aiming
at designing a supervised learning-based DoA estimation algorithm that
generalizes well to different array geometries, in this paper we propose a
geometry-aware DoA estimation algorithm. The algorithm uses a fully connected
DNN and takes mixed data as input features, namely the time lags maximizing the
generalized cross-correlation with phase transform and the microphone
coordinates, which are assumed to be known. Experimental results for a
reverberant scenario demonstrate the flexibility of the proposed algorithm
towards different array geometries and show that the proposed algorithm
outperforms model-based algorithms such as steered response power with phase
transform.",Submitted to ICASSP 2023,
Machine Learning-based Classification of Birds through Birdsong,"[arxiv.Result.Author('Yueying Chang'), arxiv.Result.Author('Richard O. Sinnott')]",2022-12-09 06:20:50+00:00,"Audio sound recognition and classification is used for many tasks and
applications including human voice recognition, music recognition and audio
tagging. In this paper we apply Mel Frequency Cepstral Coefficients (MFCC) in
combination with a range of machine learning models to identify (Australian)
birds from publicly available audio files of their birdsong. We present
approaches used for data processing and augmentation and compare the results of
various state of the art machine learning models. We achieve an overall
accuracy of 91% for the top-5 birds from the 30 selected as the case study.
Applying the models to more challenging and diverse audio files comprising 152
bird species, we achieve an accuracy of 58%",,
High Quality Audio Coding with MDCTNet,"[arxiv.Result.Author('Grant Davidson'), arxiv.Result.Author('Mark Vinton'), arxiv.Result.Author('Per Ekstrand'), arxiv.Result.Author('Cong Zhou'), arxiv.Result.Author('Lars Villemoes'), arxiv.Result.Author('Lie Lu')]",2022-12-08 22:18:20+00:00,"We propose a neural audio generative model, MDCTNet, operating in the
perceptually weighted domain of an adaptive modified discrete cosine transform
(MDCT). The architecture of the model captures correlations in both time and
frequency directions with recurrent layers (RNNs). An audio coding system is
obtained by training MDCTNet on a diverse set of fullband monophonic audio
signals at 48 kHz sampling, conditioned by a perceptual audio encoder. In a
subjective listening test with ten excerpts chosen to be balanced across
content types, yet stressful for both codecs, the mean performance of the
proposed system for 24 kb/s variable bitrate (VBR) is similar to that of Opus
at twice the bitrate.","Five pages, five figures",
A Data-driven Cognitive Salience Model for Objective Perceptual Audio Quality Assessment,"[arxiv.Result.Author('Pablo M. Delgado'), arxiv.Result.Author('Jürgen Herre')]",2022-12-08 21:40:01+00:00,"Objective audio quality measurement systems often use perceptual models to
predict the subjective quality scores of processed signals, as reported in
listening tests. Most systems map different metrics of perceived degradation
into a single quality score predicting subjective quality. This requires a
quality mapping stage that is informed by real listening test data using
statistical learning (i.e., a data-driven approach) with distortion metrics as
input features. However, the amount of reliable training data is limited in
practice, and usually not sufficient for a comprehensive training of large
learning models. Models of cognitive effects in objective systems can, however,
improve the learning model. Specifically, considering the salience of certain
distortion types, they provide additional features to the mapping stage that
improve the learning process, especially for limited amounts of training data.
We propose a novel data-driven salience model that informs the quality mapping
stage by explicitly estimating the cognitive/degradation metric interactions
using a salience measure. Systems incorporating the novel salience model are
shown to outperform equivalent systems that only use statistical learning to
combine cognitive and degradation metrics, as well as other well-known
measurement systems, for a representative validation dataset.",Accepted version of the paper submitted to ICASSP 2020,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), 2022, pp. 986-990"
SpeechLMScore: Evaluating speech generation using speech language model,"[arxiv.Result.Author('Soumi Maiti'), arxiv.Result.Author('Yifan Peng'), arxiv.Result.Author('Takaaki Saeki'), arxiv.Result.Author('Shinji Watanabe')]",2022-12-08 21:00:15+00:00,"While human evaluation is the most reliable metric for evaluating speech
generation systems, it is generally costly and time-consuming. Previous studies
on automatic speech quality assessment address the problem by predicting human
evaluation scores with machine learning models. However, they rely on
supervised learning and thus suffer from high annotation costs and domain-shift
problems. We propose SpeechLMScore, an unsupervised metric to evaluate
generated speech using a speech-language model. SpeechLMScore computes the
average log-probability of a speech signal by mapping it into discrete tokens
and measures the average probability of generating the sequence of tokens.
Therefore, it does not require human annotation and is a highly scalable
framework. Evaluation results demonstrate that the proposed metric shows a
promising correlation with human evaluation scores on different speech
generation tasks including voice conversion, text-to-speech, and speech
enhancement.",,
Framewise WaveGAN: High Speed Adversarial Vocoder in Time Domain with Very Low Computational Complexity,"[arxiv.Result.Author('Ahmed Mustafa'), arxiv.Result.Author('Jean-Marc Valin'), arxiv.Result.Author('Jan Büthe'), arxiv.Result.Author('Paris Smaragdis'), arxiv.Result.Author('Mike Goodwin')]",2022-12-08 19:38:34+00:00,"GAN vocoders are currently one of the state-of-the-art methods for building
high-quality neural waveform generative models. However, most of their
architectures require dozens of billion floating-point operations per second
(GFLOPS) to generate speech waveforms in samplewise manner. This makes GAN
vocoders still challenging to run on normal CPUs without accelerators or
parallel computers. In this work, we propose a new architecture for GAN
vocoders that mainly depends on recurrent and fully-connected networks to
directly generate the time domain signal in framewise manner. This results in
considerable reduction of the computational cost and enables very fast
generation on both GPUs and low-complexity CPUs. Experimental results show that
our Framewise WaveGAN vocoder achieves significantly higher quality than
auto-regressive maximum-likelihood vocoders such as LPCNet at a very low
complexity of 1.2 GFLOPS. This makes GAN vocoders more practical on edge and
low-power devices.","Submitted to ICASSP 2023, demo:
  https://ahmed-fau.github.io/fwgan_demo/",
Low-Bitrate Redundancy Coding of Speech Using a Rate-Distortion-Optimized Variational Autoencoder,"[arxiv.Result.Author('Jean-Marc Valin'), arxiv.Result.Author('Jan Büthe'), arxiv.Result.Author('Ahmed Mustafa')]",2022-12-08 18:23:52+00:00,"Robustness to packet loss is one of the main ongoing challenges in real-time
speech communication. Deep packet loss concealment (PLC) techniques have
recently demonstrated improved quality compared to traditional PLC. Despite
that, all PLC techniques hit fundamental limitations when too much acoustic
information is lost. To reduce losses in the first place, data is commonly sent
multiple times using various redundancy mechanisms. We propose a neural speech
coder specifically optimized to transmit a large amount of overlapping
redundancy at a very low bitrate, up to 50x redundancy using less than 32~kb/s.
Results show that the proposed redundancy is more effective than the existing
Opus codec redundancy, and that the two can be combined for even greater
robustness.","Submitted to ICASSP 2023, 5 pages",
Generating Holistic 3D Human Motion from Speech,"[arxiv.Result.Author('Hongwei Yi'), arxiv.Result.Author('Hualin Liang'), arxiv.Result.Author('Yifei Liu'), arxiv.Result.Author('Qiong Cao'), arxiv.Result.Author('Yandong Wen'), arxiv.Result.Author('Timo Bolkart'), arxiv.Result.Author('Dacheng Tao'), arxiv.Result.Author('Michael J. Black')]",2022-12-08 17:25:19+00:00,"This work addresses the problem of generating 3D holistic body motions from
human speech. Given a speech recording, we synthesize sequences of 3D body
poses, hand gestures, and facial expressions that are realistic and diverse. To
achieve this, we first build a high-quality dataset of 3D holistic body meshes
with synchronous speech. We then define a novel speech-to-motion generation
framework in which the face, body, and hands are modeled separately. The
separated modeling stems from the fact that face articulation strongly
correlates with human speech, while body poses and hand gestures are less
correlated. Specifically, we employ an autoencoder for face motions, and a
compositional vector-quantized variational autoencoder (VQ-VAE) for the body
and hand motions. The compositional VQ-VAE is key to generating diverse
results. Additionally, we propose a cross-conditional autoregressive model that
generates body poses and hand gestures, leading to coherent and realistic
motions. Extensive experiments and user studies demonstrate that our proposed
approach achieves state-of-the-art performance both qualitatively and
quantitatively. Our novel dataset and code will be released for research
purposes at https://talkshow.is.tue.mpg.de.",Project Webpage: https://talkshow.is.tue.mpg.de,
OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models,"[arxiv.Result.Author('Jinze Bai'), arxiv.Result.Author('Rui Men'), arxiv.Result.Author('Hao Yang'), arxiv.Result.Author('Xuancheng Ren'), arxiv.Result.Author('Kai Dang'), arxiv.Result.Author('Yichang Zhang'), arxiv.Result.Author('Xiaohuan Zhou'), arxiv.Result.Author('Peng Wang'), arxiv.Result.Author('Sinan Tan'), arxiv.Result.Author('An Yang'), arxiv.Result.Author('Zeyu Cui'), arxiv.Result.Author('Yu Han'), arxiv.Result.Author('Shuai Bai'), arxiv.Result.Author('Wenbin Ge'), arxiv.Result.Author('Jianxin Ma'), arxiv.Result.Author('Junyang Lin'), arxiv.Result.Author('Jingren Zhou'), arxiv.Result.Author('Chang Zhou')]",2022-12-08 17:07:09+00:00,"Generalist models, which are capable of performing diverse multi-modal tasks
in a task-agnostic way within a single model, have been explored recently.
Being, hopefully, an alternative to approaching general-purpose AI, existing
generalist models are still at an early stage, where modality and task coverage
is limited. To empower multi-modal task-scaling and speed up this line of
research, we release a generalist model learning system, OFASys, built on top
of a declarative task interface named multi-modal instruction. At the core of
OFASys is the idea of decoupling multi-modal task representations from the
underlying model implementations. In OFASys, a task involving multiple
modalities can be defined declaratively even with just a single line of code.
The system automatically generates task plans from such instructions for
training and inference. It also facilitates multi-task training for diverse
multi-modal workloads. As a starting point, we provide presets of 7 different
modalities and 23 highly-diverse example tasks in OFASys, with which we also
develop a first-in-kind, single model, OFA+, that can handle text, image,
speech, video, and motion data. The single OFA+ model achieves 95% performance
in average with only 16% parameters of 15 task-finetuned models, showcasing the
performance reliability of multi-modal task-scaling provided by OFASys.
Available at https://github.com/OFA-Sys/OFASys",,
On The Relevance Of The Differences Between HRTF Measurement Setups For Machine Learning,"[arxiv.Result.Author('Johan Pauwels'), arxiv.Result.Author('Lorenzo Picinali')]",2022-12-08 14:19:46+00:00,"As spatial audio is enjoying a surge in popularity, data-driven machine
learning techniques that have been proven successful in other domains are
increasingly used to process head-related transfer function measurements.
However, these techniques require much data, whereas the existing datasets are
ranging from tens to the low hundreds of datapoints. It therefore becomes
attractive to combine multiple of these datasets, although they are measured
under different conditions. In this paper, we first establish the common ground
between a number of datasets, then we investigate potential pitfalls of mixing
datasets. We perform a simple experiment to test the relevance of the remaining
differences between datasets when applying machine learning techniques.
Finally, we pinpoint the most relevant differences.",,
DDSupport: Language Learning Support System that Displays Differences and Distances from Model Speech,"[arxiv.Result.Author('Kazuki Kawamura'), arxiv.Result.Author('Jun Rekimoto')]",2022-12-08 05:49:15+00:00,"When beginners learn to speak a non-native language, it is difficult for them
to judge for themselves whether they are speaking well. Therefore,
computer-assisted pronunciation training systems are used to detect learner
mispronunciations. These systems typically compare the user's speech with that
of a specific native speaker as a model in units of rhythm, phonemes, or words
and calculate the differences. However, they require extensive speech data with
detailed annotations or can only compare with one specific native speaker. To
overcome these problems, we propose a new language learning support system that
calculates speech scores and detects mispronunciations by beginners based on a
small amount of unannotated speech data without comparison to a specific
person. The proposed system uses deep learning--based speech processing to
display the pronunciation score of the learner's speech and the
difference/distance between the learner's and a group of models' pronunciation
in an intuitively visual manner. Learners can gradually improve their
pronunciation by eliminating differences and shortening the distance from the
model until they become sufficiently proficient. Furthermore, since the
pronunciation score and difference/distance are not calculated compared to
specific sentences of a particular model, users are free to study the sentences
they wish to study. We also built an application to help non-native speakers
learn English and confirmed that it can improve users' speech intelligibility.",,"2022 21st IEEE International Conference on Machine Learning and
  Applications (ICMLA)"
Learning to Dub Movies via Hierarchical Prosody Models,"[arxiv.Result.Author('Gaoxiang Cong'), arxiv.Result.Author('Liang Li'), arxiv.Result.Author('Yuankai Qi'), arxiv.Result.Author('Zhengjun Zha'), arxiv.Result.Author('Qi Wu'), arxiv.Result.Author('Wenyu Wang'), arxiv.Result.Author('Bin Jiang'), arxiv.Result.Author('Ming-Hsuan Yang'), arxiv.Result.Author('Qingming Huang')]",2022-12-08 03:29:04+00:00,"Given a piece of text, a video clip and a reference audio, the movie dubbing
(also known as visual voice clone V2C) task aims to generate speeches that
match the speaker's emotion presented in the video using the desired speaker
voice as reference. V2C is more challenging than conventional text-to-speech
tasks as it additionally requires the generated speech to exactly match the
varying emotions and speaking speed presented in the video. Unlike previous
works, we propose a novel movie dubbing architecture to tackle these problems
via hierarchical prosody modelling, which bridges the visual information to
corresponding speech prosody from three aspects: lip, face, and scene.
Specifically, we align lip movement to the speech duration, and convey facial
expression to speech energy and pitch via attention mechanism based on valence
and arousal representations inspired by recent psychology findings. Moreover,
we design an emotion booster to capture the atmosphere from global video
scenes. All these embeddings together are used to generate mel-spectrogram and
then convert to speech waves via existing vocoder. Extensive experimental
results on the Chem and V2C benchmark datasets demonstrate the favorable
performance of the proposed method. The source code and trained models will be
released to the public.",,
Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors,"[arxiv.Result.Author('Zhentao Yu'), arxiv.Result.Author('Zixin Yin'), arxiv.Result.Author('Deyu Zhou'), arxiv.Result.Author('Duomin Wang'), arxiv.Result.Author('Finn Wong'), arxiv.Result.Author('Baoyuan Wang')]",2022-12-07 17:55:41+00:00,"In this paper, we introduce a simple and novel framework for one-shot
audio-driven talking head generation. Unlike prior works that require
additional driving sources for controlled synthesis in a deterministic manner,
we instead probabilistically sample all the holistic lip-irrelevant facial
motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the
input audio while still maintaining both the photo-realism of audio-lip
synchronization and the overall naturalness. This is achieved by our newly
proposed audio-to-visual diffusion prior trained on top of the mapping between
audio and disentangled non-lip facial representations. Thanks to the
probabilistic nature of the diffusion prior, one big advantage of our framework
is it can synthesize diverse facial motion sequences given the same audio clip,
which is quite user-friendly for many real applications. Through comprehensive
evaluations on public benchmarks, we conclude that (1) our diffusion prior
outperforms auto-regressive prior significantly on almost all the concerned
metrics; (2) our overall system is competitive with prior works in terms of
audio-lip synchronization but can effectively sample rich and natural-looking
lip-irrelevant facial motions while still semantically harmonized with the
audio input.",16 pages,
iQuery: Instruments as Queries for Audio-Visual Sound Separation,"[arxiv.Result.Author('Jiaben Chen'), arxiv.Result.Author('Renrui Zhang'), arxiv.Result.Author('Dongze Lian'), arxiv.Result.Author('Jiaqi Yang'), arxiv.Result.Author('Ziyao Zeng'), arxiv.Result.Author('Jianbo Shi')]",2022-12-07 17:55:06+00:00,"Current audio-visual separation methods share a standard architecture design
where an audio encoder-decoder network is fused with visual encoding features
at the encoder bottleneck. This design confounds the learning of multi-modal
feature encoding with robust sound decoding for audio separation. To generalize
to a new instrument: one must finetune the entire visual and audio network for
all musical instruments. We re-formulate visual-sound separation task and
propose Instrument as Query (iQuery) with a flexible query expansion mechanism.
Our approach ensures cross-modal consistency and cross-instrument
disentanglement. We utilize ""visually named"" queries to initiate the learning
of audio queries and use cross-modal attention to remove potential sound source
interference at the estimated waveforms. To generalize to a new instrument or
event class, drawing inspiration from the text-prompt design, we insert an
additional query as an audio prompt while freezing the attention mechanism.
Experimental results on three benchmarks demonstrate that our iQuery improves
audio-visual sound source separation performance.",,
Magic: Multi Art Genre Intelligent Choreography Dataset and Network for 3D Dance Generation,"[arxiv.Result.Author('Ronghui Li'), arxiv.Result.Author('Junfan Zhao'), arxiv.Result.Author('Yachao Zhang'), arxiv.Result.Author('Mingyang Su'), arxiv.Result.Author('Zeping Ren'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Xiu Li')]",2022-12-07 16:10:08+00:00,"Achieving multiple genres and long-term choreography sequences from given
music is a challenging task, due to the lack of a multi-genre dataset. To
tackle this problem,we propose a Multi Art Genre Intelligent Choreography
Dataset (MagicDance). The data of MagicDance is captured from professional
dancers assisted by motion capture technicians. It has a total of 8 hours 3D
motioncapture human dances with paired music, and 16 different dance genres. To
the best of our knowledge, MagicDance is the 3D dance dataset with the most
genres. In addition, we find that the existing two types of methods
(generation-based method and synthesis-based method) can only satisfy one of
the diversity and duration, but they can complement to some extent. Based on
this observation, we also propose a generation-synthesis choreography network
(MagicNet), which cascades a Diffusion-based 3D Diverse Dance fragments
Generation Network (3DGNet) and a Genre&Coherent aware Retrieval Module (GCRM).
The former can generate various dance fragments from only one music clip. The
latter is utilized to select the best dance fragment generated by 3DGNet and
switch them into a complete dance according to the genre and coherent matching
score. Quantitative and qualitative experiments demonstrate the quality of
MagicDance, and the state-of-the-art performance of MagicNet.",,
Learning Double-Compression Video Fingerprints Left from Social-Media Platforms,"[arxiv.Result.Author('Irene Amerini'), arxiv.Result.Author('Aris Anagnostopoulos'), arxiv.Result.Author('Luca Maiano'), arxiv.Result.Author('Lorenzo Ricciardi Celsi')]",2022-12-07 14:22:58+00:00,"Social media and messaging apps have become major communication platforms.
Multimedia contents promote improved user engagement and have thus become a
very important communication tool. However, fake news and manipulated content
can easily go viral, so, being able to verify the source of videos and images
as well as to distinguish between native and downloaded content becomes
essential. Most of the work performed so far on social media provenance has
concentrated on images; in this paper, we propose a CNN architecture that
analyzes video content to trace videos back to their social network of origin.
The experiments demonstrate that stating platform provenance is possible for
videos as well as images with very good accuracy.",,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)"
M3ST: Mix at Three Levels for Speech Translation,"[arxiv.Result.Author('Xuxin Cheng'), arxiv.Result.Author('Qianqian Dong'), arxiv.Result.Author('Fengpeng Yue'), arxiv.Result.Author('Tom Ko'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Yuexian Zou')]",2022-12-07 14:22:00+00:00,"How to solve the data scarcity problem for end-to-end speech-to-text
translation (ST)? It's well known that data augmentation is an efficient method
to improve performance for many tasks by enlarging the dataset. In this paper,
we propose Mix at three levels for Speech Translation (M^3ST) method to
increase the diversity of the augmented training corpus. Specifically, we
conduct two phases of fine-tuning based on a pre-trained model using external
machine translation (MT) data. In the first stage of fine-tuning, we mix the
training corpus at three levels, including word level, sentence level and frame
level, and fine-tune the entire model with mixed data. At the second stage of
fine-tuning, we take both original speech sequences and original text sequences
in parallel into the model to fine-tune the network, and use Jensen-Shannon
divergence to regularize their outputs. Experiments on MuST-C speech
translation benchmark and analysis show that M^3ST outperforms current strong
baselines and achieves state-of-the-art results on eight directions with an
average BLEU of 29.9.",Submitted to ICASSP 2023,
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers,"[arxiv.Result.Author('Zijian Yang'), arxiv.Result.Author('Wei Zhou'), arxiv.Result.Author('Ralf Schlüter'), arxiv.Result.Author('Hermann Ney')]",2022-12-07 12:49:10+00:00,"Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
modes, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.",submitted to ICASSP 2023,
"Low-Resource End-to-end Sanskrit TTS using Tacotron2, WaveGlow and Transfer Learning","[arxiv.Result.Author('Ankur Debnath'), arxiv.Result.Author('Shridevi S Patil'), arxiv.Result.Author('Gangotri Nadiger'), arxiv.Result.Author('Ramakrishnan Angarai Ganesan')]",2022-12-07 10:15:34+00:00,"End-to-end text-to-speech (TTS) systems have been developed for European
languages like English and Spanish with state-of-the-art speech quality,
prosody, and naturalness. However, development of end-to-end TTS for Indian
languages is lagging behind in terms of quality. The challenges involved in
such a task are: 1) scarcity of quality training data; 2) low efficiency during
training and inference; 3) slow convergence in the case of large vocabulary
size. In our work reported in this paper, we have investigated the use of
fine-tuning the English-pretrained Tacotron2 model with limited Sanskrit data
to synthesize natural sounding speech in Sanskrit in low resource settings. Our
experiments show encouraging results, achieving an overall MOS of 3.38 from 37
evaluators with good Sanskrit spoken knowledge. This is really a very good
result, considering the fact that the speech data we have used is of duration
2.5 hours only.",,
Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit,"[arxiv.Result.Author('Pengcheng Li'), arxiv.Result.Author('Genshun Wan'), arxiv.Result.Author('Fenglin Ding'), arxiv.Result.Author('Hang Chen'), arxiv.Result.Author('Jianqing Gao'), arxiv.Result.Author('Jia Pan'), arxiv.Result.Author('Cong Liu')]",2022-12-07 06:31:31+00:00,"Speech pre-training has shown great success in learning useful and general
latent representations from large-scale unlabeled data. Based on a
well-designed self-supervised learning pattern, pre-trained models can be used
to serve lots of downstream speech tasks such as automatic speech recognition.
In order to take full advantage of the labed data in low resource task, we
present an improved pre-training method by introducing a supervision-enhanced
acoustic unit (SEAU) pattern to intensify the expression of comtext information
and ruduce the training cost. Encoder representations extracted from the SEAU
pattern are used to generate more representative target units for HuBERT
pre-training process. The proposed method, named SeHuBERT, achieves a relative
word error rate reductions of 10.5% and 4.9% comared with the standard HuBERT
on Turkmen speech recognition task with 500 hours and 100 hours fine-tuning
data respectively. Extended to more languages and more data, SeHuBERT can aslo
achieve a relative word error rate reductions of approximately 10% at half of
the training cost compared with HuBERT.",Submitted to ICASSP 2023,
Progressive Multi-Scale Self-Supervised Learning for Speech Recognition,"[arxiv.Result.Author('Genshun Wan'), arxiv.Result.Author('Tan Liu'), arxiv.Result.Author('Hang Chen'), arxiv.Result.Author('Jia Pan'), arxiv.Result.Author('Cong Liu'), arxiv.Result.Author('Zhongfu Ye')]",2022-12-07 06:29:00+00:00,"Self-supervised learning (SSL) models have achieved considerable improvements
in automatic speech recognition (ASR). In addition, ASR performance could be
further improved if the model is dedicated to audio content information
learning theoretically. To this end, we propose a progressive multi-scale
self-supervised learning (PMS-SSL) method, which uses fine-grained target sets
to compute SSL loss at top layer while uses coarse-grained target sets at
intermediate layers. Furthermore, PMS-SSL introduces multi-scale structure into
multi-head self-attention for better speech representation, which restricts the
attention area into a large scope at higher layers while restricts the
attention area into a small scope at lower layers. Experiments on Librispeech
dataset indicate the effectiveness of our proposed method. Compared with
HuBERT, PMS-SSL achieves 13.7% / 12.7% relative WER reduction on test other
evaluation subsets respectively when fine-tuned on 10hours / 100hours subsets.",Submitted to ICASSP 2023,
Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information,"[arxiv.Result.Author('Fenglin Ding'), arxiv.Result.Author('Genshun Wan'), arxiv.Result.Author('Pengcheng Li'), arxiv.Result.Author('Jia Pan'), arxiv.Result.Author('Cong Liu')]",2022-12-07 06:18:59+00:00,"Multilingual end-to-end models have shown great improvement over monolingual
systems. With the development of pre-training methods on speech,
self-supervised multilingual speech representation learning like XLSR has shown
success in improving the performance of multilingual automatic speech
recognition (ASR). However, similar to the supervised learning, multilingual
pre-training may also suffer from language interference and further affect the
application of multilingual system. In this paper, we introduce several
techniques for improving self-supervised multilingual pre-training by
leveraging auxiliary language information, including the language adversarial
training, language embedding and language adaptive training during the
pre-training stage. We conduct experiments on a multilingual ASR task
consisting of 16 languages. Our experimental results demonstrate 14.3% relative
gain over the standard XLSR model, and 19.8% relative gain over the no
pre-training multilingual model.",Subimitted to ICASSP 2023,
Improving trajectory localization accuracy via direction-of-arrival derivative estimation,"[arxiv.Result.Author('Ruchi Pandey'), arxiv.Result.Author('Shreyas Jaiswal'), arxiv.Result.Author('Huy Phan'), arxiv.Result.Author('Santosh Nannuru')]",2022-12-07 05:33:45+00:00,"Sound source localization is crucial in acoustic sensing and
monitoring-related applications. In this paper, we do a comprehensive analysis
of improvement in sound source localization by combining the direction of
arrivals (DOAs) with their derivatives which quantify the changes in the
positions of sources over time. This study uses the SALSA-Lite feature with a
convolutional recurrent neural network (CRNN) model for predicting DOAs and
their first-order derivatives. An update rule is introduced to combine the
predicted DOAs with the estimated derivatives to obtain the final DOAs. The
experimental validation is done using TAU-NIGENS Spatial Sound Events (TNSSE)
2021 dataset. We compare the performance of the networks predicting DOAs with
derivative vs. the one predicting only the DOAs at low SNR levels. The results
show that combining the derivatives with the DOAs improves the localization
accuracy of moving sources.",,
Improve Bilingual TTS Using Dynamic Language and Phonology Embedding,"[arxiv.Result.Author('Fengyu Yang'), arxiv.Result.Author('Jian Luan'), arxiv.Result.Author('Yujun Wang')]",2022-12-07 03:46:18+00:00,"In most cases, bilingual TTS needs to handle three types of input scripts:
first language only, second language only, and second language embedded in the
first language. In the latter two situations, the pronunciation and intonation
of the second language are usually quite different due to the influence of the
first language. Therefore, it is a big challenge to accurately model the
pronunciation and intonation of the second language in different contexts
without mutual interference. This paper builds a Mandarin-English TTS system to
acquire more standard spoken English speech from a monolingual Chinese speaker.
We introduce phonology embedding to capture the English differences between
different phonology. Embedding mask is applied to language embedding for
distinguishing information between different languages and to phonology
embedding for focusing on English expression. We specially design an embedding
strength modulator to capture the dynamic strength of language and phonology.
Experiments show that our approach can produce significantly more natural and
standard spoken English speech of the monolingual Chinese speaker. From
analysis, we find that suitable phonology control contributes to better
performance in different scenarios.",Submitted to ICASSP2023,
Selector-Enhancer: Learning Dynamic Selection of Local and Non-local Attention Operation for Speech Enhancement,"[arxiv.Result.Author('Xinmeng Xu'), arxiv.Result.Author('Weiping Tu'), arxiv.Result.Author('Yuhong Yang')]",2022-12-07 02:34:03+00:00,"Attention mechanisms, such as local and non-local attention, play a
fundamental role in recent deep learning based speech enhancement (SE) systems.
However, natural speech contains many fast-changing and relatively brief
acoustic events, therefore, capturing the most informative speech features by
indiscriminately using local and non-local attention is challenged. We observe
that the noise type and speech feature vary within a sequence of speech and the
local and non-local operations can respectively extract different features from
corrupted speech. To leverage this, we propose Selector-Enhancer, a
dual-attention based convolution neural network (CNN) with a feature-filter
that can dynamically select regions from low-resolution speech features and
feed them to local or non-local attention operations. In particular, the
proposed feature-filter is trained by using reinforcement learning (RL) with a
developed difficulty-regulated reward that is related to network performance,
model complexity, and ""the difficulty of the SE task"". The results show that
our method achieves comparable or superior performance to existing approaches.
In particular, Selector-Enhancer is potentially effective for real-world
denoising, where the number and types of noise are varies on a single noisy
mixture.",Accepted by AAAI 2023,
MIMO-DBnet: Multi-channel Input and Multiple Outputs DOA-aware Beamforming Network for Speech Separation,"[arxiv.Result.Author('Yanjie Fu'), arxiv.Result.Author('Haoran Yin'), arxiv.Result.Author('Meng Ge'), arxiv.Result.Author('Longbiao Wang'), arxiv.Result.Author('Gaoyan Zhang'), arxiv.Result.Author('Jianwu Dang'), arxiv.Result.Author('Chengyun Deng'), arxiv.Result.Author('Fei Wang')]",2022-12-07 01:52:40+00:00,"Recently, many deep learning based beamformers have been proposed for
multi-channel speech separation. Nevertheless, most of them rely on extra cues
known in advance, such as speaker feature, face image or directional
information. In this paper, we propose an end-to-end beamforming network for
direction guided speech separation given merely the mixture signal, namely
MIMO-DBnet. Specifically, we design a multi-channel input and multiple outputs
architecture to predict the direction-of-arrival based embeddings and
beamforming weights for each source. The precisely estimated directional
embedding provides quite effective spatial discrimination guidance for the
neural beamformer to offset the effect of phase wrapping, thus allowing more
accurate reconstruction of two sources' speech signals. Experiments show that
our proposed MIMO-DBnet not only achieves a comprehensive decent improvement
compared to baseline systems, but also maintain the performance on high
frequency bands when phase wrapping occurs.",Submitted to ICASSP 2023,
Analysis and Utilization of Entrainment on Acoustic and Emotion Features in User-agent Dialogue,"[arxiv.Result.Author('Daxin Tan'), arxiv.Result.Author('Nikos Kargas'), arxiv.Result.Author('David McHardy'), arxiv.Result.Author('Constantinos Papayiannis'), arxiv.Result.Author('Antonio Bonafonte'), arxiv.Result.Author('Marek Strelec'), arxiv.Result.Author('Jonas Rohnke'), arxiv.Result.Author('Agis Oikonomou Filandras'), arxiv.Result.Author('Trevor Wood')]",2022-12-07 01:45:15+00:00,"Entrainment is the phenomenon by which an interlocutor adapts their speaking
style to align with their partner in conversations. It has been found in
different dimensions as acoustic, prosodic, lexical or syntactic. In this work,
we explore and utilize the entrainment phenomenon to improve spoken dialogue
systems for voice assistants. We first examine the existence of the entrainment
phenomenon in human-to-human dialogues in respect to acoustic feature and then
extend the analysis to emotion features. The analysis results show strong
evidence of entrainment in terms of both acoustic and emotion features. Based
on this findings, we implement two entrainment policies and assess if the
integration of entrainment principle into a Text-to-Speech (TTS) system
improves the synthesis performance and the user experience. It is found that
the integration of the entrainment principle into a TTS system brings
performance improvement when considering acoustic features, while no obvious
improvement is observed when considering emotion features.","This version has been removed by arXiv administrators because the
  submitter did not have the right to assign a license at the time of
  submission",
Learning the joint distribution of two sequences using little or no paired data,"[arxiv.Result.Author('Soroosh Mariooryad'), arxiv.Result.Author('Matt Shannon'), arxiv.Result.Author('Siyuan Ma'), arxiv.Result.Author('Tom Bagby'), arxiv.Result.Author('David Kao'), arxiv.Result.Author('Daisy Stanton'), arxiv.Result.Author('Eric Battenberg'), arxiv.Result.Author('RJ Skerry-Ryan')]",2022-12-06 18:56:15+00:00,"We present a noisy channel generative model of two sequences, for example
text and speech, which enables uncovering the association between the two
modalities when limited paired data is available. To address the intractability
of the exact model under a realistic data setup, we propose a variational
inference approximation. To train this variational model with categorical data,
we propose a KL encoder loss approach which has connections to the wake-sleep
algorithm. Identifying the joint or conditional distributions by only observing
unpaired samples from the marginals is only possible under certain conditions
in the data distribution and we discuss under what type of conditional
independence assumptions that might be achieved, which guides the architecture
designs. Experimental results show that even tiny amount of paired data (5
minutes) is sufficient to learn to relate the two modalities (graphemes and
phonemes here) when a massive amount of unpaired data is available, paving the
path to adopting this principled approach for all seq2seq models in low data
resource regimes.",,
Robust Speech Recognition via Large-Scale Weak Supervision,"[arxiv.Result.Author('Alec Radford'), arxiv.Result.Author('Jong Wook Kim'), arxiv.Result.Author('Tao Xu'), arxiv.Result.Author('Greg Brockman'), arxiv.Result.Author('Christine McLeavey'), arxiv.Result.Author('Ilya Sutskever')]",2022-12-06 18:46:04+00:00,"We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",,
Label-free Knowledge Distillation with Contrastive Loss for Light-weight Speaker Recognition,"[arxiv.Result.Author('Zhiyuan Peng'), arxiv.Result.Author('Xuanji He'), arxiv.Result.Author('Ke Ding'), arxiv.Result.Author('Tan Lee'), arxiv.Result.Author('Guanglu Wan')]",2022-12-06 16:01:59+00:00,"Very deep models for speaker recognition (SR) have demonstrated remarkable
performance improvement in recent research. However, it is impractical to
deploy these models for on-device applications with constrained computational
resources. On the other hand, light-weight models are highly desired in
practice despite their sub-optimal performance. This research aims to improve
light-weight SR models through large-scale label-free knowledge distillation
(KD). Existing KD approaches for SR typically require speaker labels to learn
task-specific knowledge, due to the inefficiency of conventional loss for
distillation. To address the inefficiency problem and achieve label-free KD, we
propose to employ the contrastive loss from self-supervised learning for
distillation. Extensive experiments are conducted on a collection of public
speech datasets from diverse sources. Results on light-weight SR models show
that the proposed approach of label-free KD with contrastive loss consistently
outperforms both conventional distillation methods and self-supervised learning
methods by a significant margin.",,
Covariance Regularization for Probabilistic Linear Discriminant Analysis,"[arxiv.Result.Author('Zhiyuan Peng'), arxiv.Result.Author('Mingjie Shao'), arxiv.Result.Author('Xuanji He'), arxiv.Result.Author('Xu Li'), arxiv.Result.Author('Tan Lee'), arxiv.Result.Author('Ke Ding'), arxiv.Result.Author('Guanglu Wan')]",2022-12-06 15:12:58+00:00,"Probabilistic linear discriminant analysis (PLDA) is commonly used in speaker
verification systems to score the similarity of speaker embeddings. Recent
studies improved the performance of PLDA in domain-matched conditions by
diagonalizing its covariance. We suspect such brutal pruning approach could
eliminate its capacity in modeling dimension correlation of speaker embeddings,
leading to inadequate performance with domain adaptation. This paper explores
two alternative covariance regularization approaches, namely, interpolated PLDA
and sparse PLDA, to tackle the problem. The interpolated PLDA incorporates the
prior knowledge from cosine scoring to interpolate the covariance of PLDA. The
sparse PLDA introduces a sparsity penalty to update the covariance.
Experimental results demonstrate that both approaches outperform diagonal
regularization noticeably with domain adaptation. In addition, in-domain data
can be significantly reduced when training sparse PLDA for domain adaptation.",,
FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar Tablature Transcription,"[arxiv.Result.Author('Frank Cwitkowitz'), arxiv.Result.Author('Toni Hirvonen'), arxiv.Result.Author('Anssi Klapuri')]",2022-12-06 14:51:27+00:00,"In recent years, the task of Automatic Music Transcription (AMT), whereby
various attributes of music notes are estimated from audio, has received
increasing attention. At the same time, the related task of Multi-Pitch
Estimation (MPE) remains a challenging but necessary component of almost all
AMT approaches, even if only implicitly. In the context of AMT, pitch
information is typically quantized to the nominal pitches of the Western music
scale. Even in more general contexts, MPE systems typically produce pitch
predictions with some degree of quantization. In certain applications of AMT,
such as Guitar Tablature Transcription (GTT), it is more meaningful to estimate
continuous-valued pitch contours. Guitar tablature has the capacity to
represent various playing techniques, some of which involve pitch modulation.
Contemporary approaches to AMT do not adequately address pitch modulation, and
offer only less quantization at the expense of more model complexity. In this
paper, we present a GTT formulation that estimates continuous-valued pitch
contours, grouping them according to their string and fret of origin. We
demonstrate that for this task, the proposed method significantly improves the
resolution of MPE and simultaneously yields tablature estimation results
competitive with baseline models.",Submitted to ICASSP 2023,
BC-VAD: A Robust Bone Conduction Voice Activity Detection,"[arxiv.Result.Author(""Niccolo' Polvani""), arxiv.Result.Author('Damien Ronssin'), arxiv.Result.Author('Milos Cernak')]",2022-12-06 14:14:00+00:00,"Voice Activity Detection (VAD) is a fundamental module in many audio
applications. Recent state-of-the-art VAD systems are often based on neural
networks, but they require a computational budget that usually exceeds the
capabilities of a small battery-operated device when preserving the performance
of larger models. In this work, we rely on the input from a bone conduction
microphone (BCM) to design an efficient VAD (BC-VAD) robust against residual
non-stationary noises originating from the environment or speakers not wearing
the BCM.We first show that a larger VAD system (58k parameters) achieves
state-of-the-art results on a publicly available benchmark but fails when
running on bone conduction signals. We then compare its variant BC-VAD (5k
parameters and trained on BC data) with a baseline especially designed for a
BCM and show that the proposed method achieves better performances under
various metrics while keeping the realtime processing requirement for a
microcontroller.",,
Self-Supervised Audio-Visual Speech Representations Learning By Multimodal Self-Distillation,"[arxiv.Result.Author('Jing-Xuan Zhang'), arxiv.Result.Author('Genshun Wan'), arxiv.Result.Author('Zhen-Hua Ling'), arxiv.Result.Author('Jia Pan'), arxiv.Result.Author('Jianqing Gao'), arxiv.Result.Author('Cong Liu')]",2022-12-06 06:37:38+00:00,"In this work, we present a novel method, named AV2vec, for learning
audio-visual speech representations by multimodal self-distillation. AV2vec has
a student and a teacher module, in which the student performs a masked latent
feature regression task using the multimodal target features generated online
by the teacher. The parameters of the teacher model are a momentum update of
the student. Since our target features are generated online, AV2vec needs no
iteration step like AV-HuBERT and the total training time cost is reduced to
less than one-fifth. We further propose AV2vec-MLM in this study, which
augments AV2vec with a masked language model (MLM)-style loss using multitask
learning. Our experimental results show that AV2vec achieved comparable
performance to the AV-HuBERT baseline. When combined with an MLM-style loss,
AV2vec-MLM outperformed baselines and achieved the best performance on the
downstream tasks.",submitted to ICASSP 2023,
Parameter Efficient Transfer Learning for Various Speech Processing Tasks,"[arxiv.Result.Author('Shinta Otake'), arxiv.Result.Author('Rei Kawakami'), arxiv.Result.Author('Nakamasa Inoue')]",2022-12-06 06:33:20+00:00,"Fine-tuning of self-supervised models is a powerful transfer learning method
in a variety of fields, including speech processing, since it can utilize
generic feature representations obtained from large amounts of unlabeled data.
Fine-tuning, however, requires a new parameter set for each downstream task,
which is parameter inefficient. Adapter architecture is proposed to partially
solve this issue by inserting lightweight learnable modules into a frozen
pre-trained model. However, existing adapter architectures fail to adaptively
leverage low- to high-level features stored in different layers, which is
necessary for solving various kinds of speech processing tasks. Thus, we
propose a new adapter architecture to acquire feature representations more
flexibly for various speech tasks. In experiments, we applied this adapter to
WavLM on four speech tasks. It performed on par or better than naive
fine-tuning, with only 11% of learnable parameters. It also outperformed an
existing adapter architecture.",,
Sound emergence as a predictor of short-term annoyance from wind turbine noise,"[arxiv.Result.Author('Elise Ruaud'), arxiv.Result.Author('Guillaume Dutilleux')]",2022-12-05 21:59:28+00:00,"While sound emergence is used in several countries to regulate wind energy
development, there is no published evidence that it is a relevant noise
descriptor for this purpose. In the present work, we carried out two listening
tests to evaluate the merits of sound emergence. Three definitions of sound
emergence were considered: the one in ISO 1996-1, sound emergence under
audibility condition $e_{UAC}$, and spectral emergence $e_{SP}$. We also
considered the specific to residual ratio and loudness metrics. In each
listening test, the sound stimuli consisted of 48 sound stimuli at 3 A-weighted
sound pressure levels $\{30, 40, 50\}$~dB and 4 specific-to-residual ratios
$\{-10, -5, 0, +5 \}$~dB. The results lead to the conclusion that short term
annoyance is better predicted by the total sound pressure level than by sound
emergence, whatever the definition considered for the latter, or than by the
specific to residual ratio. Short-term annoyance is slightly better predicted
by $e_{UAC}$ than by $e$, while $e$ is a better predictor than $e_{SP}$. The
total sound pressure level and the loudness metrics performed similarly.
Furthermore, the results provide evidence that sound emergence is a poor
predictor of the audibility of wind turbine sounds.","Accepted for publication in the Journal or the Acoustical Society of
  America. 17 pages, 8 figures, 3 tables",
Audio Latent Space Cartography,"[arxiv.Result.Author('Nicolas Jonason'), arxiv.Result.Author('Bob L. T. Sturm')]",2022-12-05 21:51:33+00:00,"We explore the generation of visualisations of audio latent spaces using an
audio-to-image generation pipeline. We believe this can help with the
interpretability of audio latent spaces. We demonstrate a variety of results on
the NSynth dataset. A web demo is available.","Late Breaking / Demo, ISMIR 2022
  (https://ismir2022program.ismir.net/lbd_413.html)",
MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning,"[arxiv.Result.Author('Yizhi Li'), arxiv.Result.Author('Ruibin Yuan'), arxiv.Result.Author('Ge Zhang'), arxiv.Result.Author('Yinghao Ma'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Xingran Chen'), arxiv.Result.Author('Anton Ragni'), arxiv.Result.Author('Hanzhi Yin'), arxiv.Result.Author('Zhijie Hu'), arxiv.Result.Author('Haoyu He'), arxiv.Result.Author('Emmanouil Benetos'), arxiv.Result.Author('Norbert Gyenge'), arxiv.Result.Author('Ruibo Liu'), arxiv.Result.Author('Jie Fu')]",2022-12-05 16:04:26+00:00,"The deep learning community has witnessed an exponentially growing interest
in self-supervised learning (SSL). However, it still remains unexplored how to
build a framework for learning useful representations of raw music waveforms in
a self-supervised manner. In this work, we design Music2Vec, a framework
exploring different SSL algorithmic components and tricks for music audio
recordings. Our model achieves comparable results to the state-of-the-art
(SOTA) music SSL model Jukebox, despite being significantly smaller with less
than 2% of parameters of the latter. The model will be released on
Huggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)",,
Fake News and Hate Speech: Language in Common,"[arxiv.Result.Author('Berta Chulvi'), arxiv.Result.Author('Alejandro Toselli'), arxiv.Result.Author('Paolo Rosso')]",2022-12-05 15:35:10+00:00,"In this paper we raise the research question of whether fake news and hate
speech spreaders share common patterns in language. We compute a novel index,
the ingroup vs outgroup index, in three different datasets and we show that
both phenomena share an ""us vs them"" narrative.",2 pages,
Audio-Driven Co-Speech Gesture Video Generation,"[arxiv.Result.Author('Xian Liu'), arxiv.Result.Author('Qianyi Wu'), arxiv.Result.Author('Hang Zhou'), arxiv.Result.Author('Yuanqi Du'), arxiv.Result.Author('Wayne Wu'), arxiv.Result.Author('Dahua Lin'), arxiv.Result.Author('Ziwei Liu')]",2022-12-05 15:28:22+00:00,"Co-speech gesture is crucial for human-machine interaction and digital
entertainment. While previous works mostly map speech audio to human skeletons
(e.g., 2D keypoints), directly generating speakers' gestures in the image
domain remains unsolved. In this work, we formally define and study this
challenging problem of audio-driven co-speech gesture video generation, i.e.,
using a unified framework to generate speaker image sequence driven by speech
audio. Our key insight is that the co-speech gestures can be decomposed into
common motion patterns and subtle rhythmic dynamics. To this end, we propose a
novel framework, Audio-driveN Gesture vIdeo gEneration (ANGIE), to effectively
capture the reusable co-speech gesture patterns as well as fine-grained
rhythmic movements. To achieve high-fidelity image sequence generation, we
leverage an unsupervised motion representation instead of a structural human
body prior (e.g., 2D skeletons). Specifically, 1) we propose a vector quantized
motion extractor (VQ-Motion Extractor) to summarize common co-speech gesture
patterns from implicit motion representation to codebooks. 2) Moreover, a
co-speech gesture GPT with motion refinement (Co-Speech GPT) is devised to
complement the subtle prosodic motion details. Extensive experiments
demonstrate that our framework renders realistic and vivid co-speech gesture
video. Demo video and more resources can be found in:
https://alvinliu0.github.io/projects/ANGIE","Accepted by Advances in Neural Information Processing Systems
  (NeurIPS), 2022 (Spotlight Presentation). Camera-Ready Version, 19 Pages",
DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking,"[arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Jie Zhang'), arxiv.Result.Author('Han Fang'), arxiv.Result.Author('Zehua Ma'), arxiv.Result.Author('Weiming Zhang'), arxiv.Result.Author('Nenghai Yu')]",2022-12-05 15:15:10+00:00,"Audio watermarking is widely used for leaking source tracing. The robustness
of the watermark determines the traceability of the algorithm. With the
development of digital technology, audio re-recording (AR) has become an
efficient and covert means to steal secrets. AR process could drastically
destroy the watermark signal while preserving the original information. This
puts forward a new requirement for audio watermarking at this stage, that is,
to be robust to AR distortions. Unfortunately, none of the existing algorithms
can effectively resist AR attacks due to the complexity of the AR process. To
address this limitation, this paper proposes DeAR, a deep-learning-based audio
re-recording resistant watermarking. Inspired by DNN-based image watermarking,
we pioneer a deep learning framework for audio carriers, based on which the
watermark signal can be effectively embedded and extracted. Meanwhile, in order
to resist the AR attack, we delicately analyze the distortions that occurred in
the AR process and design the corresponding distortion layer to cooperate with
the proposed watermarking framework. Extensive experiments show that the
proposed algorithm can resist not only common electronic channel distortions
but also AR distortions. Under the premise of high-quality embedding
(SNR=25.86dB), in the case of a common re-recording distance (20cm), the
algorithm can effectively achieve an average bit recovery accuracy of 98.55%.",Accepted by AAAI2023,
SoftCTC $\unicode{x2013}$ Semi-Supervised Learning for Text Recognition using Soft Pseudo-Labels,"[arxiv.Result.Author('Martin Kišš'), arxiv.Result.Author('Michal Hradiš'), arxiv.Result.Author('Karel Beneš'), arxiv.Result.Author('Petr Buchal'), arxiv.Result.Author('Michal Kula')]",2022-12-05 10:13:50+00:00,"This paper explores semi-supervised training for sequence tasks, such as
Optical Character Recognition or Automatic Speech Recognition. We propose a
novel loss function $\unicode{x2013}$ SoftCTC $\unicode{x2013}$ which is an
extension of CTC allowing to consider multiple transcription variants at the
same time. This allows to omit the confidence based filtering step which is
otherwise a crucial component of pseudo-labeling approaches to semi-supervised
learning. We demonstrate the effectiveness of our method on a challenging
handwriting recognition task and conclude that SoftCTC matches the performance
of a finely-tuned filtering based pipeline. We also evaluated SoftCTC in terms
of computational efficiency, concluding that it is significantly more efficient
than a na\""ive CTC-based approach for training on multiple transcription
variants, and we make our GPU implementation public.","18 pages, 6 figures, 6 tables",
A Transformer-Based User Satisfaction Prediction for Proactive Interaction Mechanism in DuerOS,"[arxiv.Result.Author('Wei Shen'), arxiv.Result.Author('Xiaonan He'), arxiv.Result.Author('Chuheng Zhang'), arxiv.Result.Author('Xuyun Zhang'), arxiv.Result.Author('Jian XIe')]",2022-12-05 09:17:49+00:00,"Recently, spoken dialogue systems have been widely deployed in a variety of
applications, serving a huge number of end-users. A common issue is that the
errors resulting from noisy utterances, semantic misunderstandings, or lack of
knowledge make it hard for a real system to respond properly, possibly leading
to an unsatisfactory user experience. To avoid such a case, we consider a
proactive interaction mechanism where the system predicts the user satisfaction
with the candidate response before giving it to the user. If the user is not
likely to be satisfied according to the prediction, the system will ask the
user a suitable question to determine the real intent of the user instead of
providing the response directly. With such an interaction with the user, the
system can give a better response to the user. Previous models that predict the
user satisfaction are not applicable to DuerOS which is a large-scale
commercial dialogue system. They are based on hand-crafted features and thus
can hardly learn the complex patterns lying behind millions of conversations
and temporal dependency in multiple turns of the conversation. Moreover, they
are trained and evaluated on the benchmark datasets with adequate labels, which
are expensive to obtain in a commercial dialogue system. To face these
challenges, we propose a pipeline to predict the user satisfaction to help
DuerOS decide whether to ask for clarification in each turn. Specifically, we
propose to first generate a large number of weak labels and then train a
transformer-based model to predict the user satisfaction with these weak
labels. Empirically, we deploy and evaluate our model on DuerOS, and observe a
19% relative improvement on the accuracy of user satisfaction prediction and
2.3% relative improvement on user experience.",Accepted by CIKM-22,
Human-in-the-Loop Hate Speech Classification in a Multilingual Context,"[arxiv.Result.Author('Ana Kotarcic'), arxiv.Result.Author('Dominik Hangartner'), arxiv.Result.Author('Fabrizio Gilardi'), arxiv.Result.Author('Selina Kurer'), arxiv.Result.Author('Karsten Donnay')]",2022-12-05 09:05:40+00:00,"The shift of public debate to the digital sphere has been accompanied by a
rise in online hate speech. While many promising approaches for hate speech
classification have been proposed, studies often focus only on a single
language, usually English, and do not address three key concerns:
post-deployment performance, classifier maintenance and infrastructural
limitations. In this paper, we introduce a new human-in-the-loop BERT-based
hate speech classification pipeline and trace its development from initial data
collection and annotation all the way to post-deployment. Our classifier,
trained using data from our original corpus of over 422k examples, is
specifically developed for the inherently multilingual setting of Switzerland
and outperforms with its F1 score of 80.5 the currently best-performing
BERT-based multilingual classifier by 5.8 F1 points in German and 3.6 F1 points
in French. Our systematic evaluations over a 12-month period further highlight
the vital importance of continuous, human-in-the-loop classifier maintenance to
ensure robust hate speech classification post-deployment.",Findings of EMNLP 2022,
LMEC: Learnable Multiplicative Absolute Position Embedding Based Conformer for Speech Recognition,"[arxiv.Result.Author('Yuguang Yang'), arxiv.Result.Author('Yu Pan'), arxiv.Result.Author('Jingjing Yin'), arxiv.Result.Author('Heng Lu')]",2022-12-05 08:36:17+00:00,"This paper proposes a Learnable Multiplicative absolute position Embedding
based Conformer (LMEC). It contains a kernelized linear attention (LA) module
called LMLA to solve the time-consuming problem for long sequence speech
recognition as well as an alternative to the FFN structure. First, the ELU
function is adopted as the kernel function of our proposed LA module. Second,
we propose a novel Learnable Multiplicative Absolute Position Embedding
(LM-APE) based re-weighting mechanism that can reduce the well-known quadratic
temporal-space complexity of softmax self-attention. Third, we use Gated Linear
Units (GLU) to substitute the Feed Forward Network (FFN) for better
performance. Extensive experiments have been conducted on the public
LibriSpeech datasets. Compared to the Conformer model with cosFormer style
linear attention, our proposed method can achieve up to 0.63% word-error-rate
improvement on test-other and improve the inference speed by up to 13% (left
product) and 33% (right product) on the LA module.",NCMMSC2022,
End-to-end Recording Device Identification Based on Deep Representation Learning,"[arxiv.Result.Author('Chunyan Zeng'), arxiv.Result.Author('Dongliang Zhu'), arxiv.Result.Author('Zhifeng Wang'), arxiv.Result.Author('Minghu Wu'), arxiv.Result.Author('Wei Xiong'), arxiv.Result.Author('Nan Zhao')]",2022-12-05 07:56:04+00:00,"Deep learning techniques have achieved specific results in recording device
source identification. The recording device source features include spatial
information and certain temporal information. However, most recording device
source identification methods based on deep learning only use spatial
representation learning from recording device source features, which cannot
make full use of recording device source information. Therefore, in this paper,
to fully explore the spatial information and temporal information of recording
device source, we propose a new method for recording device source
identification based on the fusion of spatial feature information and temporal
feature information by using an end-to-end framework. From a feature
perspective, we designed two kinds of networks to extract recording device
source spatial and temporal information. Afterward, we use the attention
mechanism to adaptively assign the weight of spatial information and temporal
information to obtain fusion features. From a model perspective, our model uses
an end-to-end framework to learn the deep representation from spatial feature
and temporal feature and train using deep and shallow loss to joint optimize
our network. This method is compared with our previous work and baseline
system. The results show that the proposed method is better than our previous
work and baseline system under general conditions.","20 pages, 5 figures, recording device identification",
NBC2: Multichannel Speech Separation with Revised Narrow-band Conformer,"[arxiv.Result.Author('Changsheng Quan'), arxiv.Result.Author('Xiaofei Li')]",2022-12-05 07:44:32+00:00,"This work proposes a multichannel narrow-band speech separation network. In
the short-time Fourier transform (STFT) domain, the proposed network processes
each frequency independently, and all frequencies use a shared network. For
each frequency, the network performs end-to-end speech separation, namely
taking as input the STFT coefficients of microphone signals, and predicting the
separated STFT coefficients of multiple speakers. The proposed network learns
to cluster the frame-wise spatial/steering vectors that belong to different
speakers. It is mainly composed of three components. First, a self-attention
network. Clustering of spatial vectors shares a similar principle with the
self-attention mechanism in the sense of computing the similarity of vectors
and then aggregating similar vectors. Second, a convolutional feed-forward
network. The convolutional layers are employed for signal smoothing and
reverberation processing. Third, a novel hidden-layer normalization method,
i.e. group batch normalization (GBN), is especially designed for the proposed
narrow-band network to maintain the distribution of hidden units over
frequencies. Overall, the proposed network is named NBC2, as it is a revised
version of our previous NBC (narrow-band conformer) network. Experiments show
that 1) the proposed network outperforms other state-of-the-art methods by a
large margin, 2) the proposed GBN improves the signal-to-distortion ratio by 3
dB, relative to other normalization methods, such as batch/layer/group
normalization, 3) the proposed narrow-band network is spectrum-agnostic, as it
does not learn spectral patterns, and 4) the proposed network is indeed
performing frame clustering (demonstrated by the attention maps).",submitted to TASLP,
Effect of Spoken Speech in Decoding Imagined Speech from Non-Invasive Human Brain Signals,"[arxiv.Result.Author('Seo-Hyun Lee'), arxiv.Result.Author('Young-Eun Lee'), arxiv.Result.Author('Soowon Kim'), arxiv.Result.Author('Byung-Kwan Ko'), arxiv.Result.Author('Seong-Whan Lee')]",2022-12-05 05:52:31+00:00,"Decoding imagined speech from human brain signals is a challenging and
important issue that may enable human communication via brain signals. While
imagined speech can be the paradigm for silent communication via brain signals,
it is always hard to collect enough stable data to train the decoding model.
Meanwhile, spoken speech data is relatively easy and to obtain, implying the
significance of utilizing spoken speech brain signals to decode imagined
speech. In this paper, we performed a preliminary analysis to check whether if
it would be possible to utilize spoken speech electroencephalography data to
decode imagined speech, by simply applying the pre-trained model with spoken
speech brain signals to decode imagined speech brain signals. While the
classification performance of imagined speech data solely used to train and
validation was 30.5 \%, the transferred performance of spoken speech based
classifier to imagined speech data was 26.8 \% with no significant difference
found compared to the imagined speech based classifier (p = 0.0983, chi-square
= 4.64). For more comprehensive analysis, we compared the result with the
visual imagery dataset, which would naturally be less related to spoken speech
compared to the imagined speech. As a result, visual imagery have shown solely
trained performance of 31.8 \% and transferred performance of 26.3 \% which had
shown significant statistical difference between each other (p = 0.022,
chi-square = 7.64). Our results imply the potential of applying spoken speech
to decode imagined speech, as well as their underlying common features.","4 pages, 2 figures",
Towards Generating Diverse Audio Captions via Adversarial Training,"[arxiv.Result.Author('Xinhao Mei'), arxiv.Result.Author('Xubo Liu'), arxiv.Result.Author('Jianyuan Sun'), arxiv.Result.Author('Mark D. Plumbley'), arxiv.Result.Author('Wenwu Wang')]",2022-12-05 05:06:19+00:00,"Automated audio captioning is a cross-modal translation task for describing
the content of audio clips with natural language sentences. This task has
attracted increasing attention and substantial progress has been made in recent
years. Captions generated by existing models are generally faithful to the
content of audio clips, however, these machine-generated captions are often
deterministic (e.g., generating a fixed caption for a given audio clip), simple
(e.g., using common words and simple grammar), and generic (e.g., generating
the same caption for similar audio clips). When people are asked to describe
the content of an audio clip, different people tend to focus on different sound
events and describe an audio clip diversely from various aspects using distinct
words and grammar. We believe that an audio captioning system should have the
ability to generate diverse captions, either for a fixed audio clip, or across
similar audio clips. To this end, we propose an adversarial training framework
based on a conditional generative adversarial network (C-GAN) to improve
diversity of audio captioning systems. A caption generator and two hybrid
discriminators compete and are learned jointly, where the caption generator can
be any standard encoder-decoder captioning model used to generate captions, and
the hybrid discriminators assess the generated captions from different
criteria, such as their naturalness and semantics. We conduct experiments on
the Clotho dataset. The results show that our proposed model can generate
captions with better diversity as compared to state-of-the-art methods.",Submitted to TASLP,
GNN-SL: Sequence Labeling Based on Nearest Examples via GNN,"[arxiv.Result.Author('Shuhe Wang'), arxiv.Result.Author('Yuxian Meng'), arxiv.Result.Author('Rongbin Ouyang'), arxiv.Result.Author('Jiwei Li'), arxiv.Result.Author('Tianwei Zhang'), arxiv.Result.Author('Lingjuan Lyu'), arxiv.Result.Author('Guoyin Wang')]",2022-12-05 04:22:00+00:00,"To better handle long-tail cases in the sequence labeling (SL) task, in this
work, we introduce graph neural networks sequence labeling (GNN-SL), which
augments the vanilla SL model output with similar tagging examples retrieved
from the whole training set. Since not all the retrieved tagging examples
benefit the model prediction, we construct a heterogeneous graph, and leverage
graph neural networks (GNNs) to transfer information between the retrieved
tagging examples and the input word sequence. The augmented node which
aggregates information from neighbors is used to do prediction. This strategy
enables the model to directly acquire similar tagging examples and improves the
general quality of predictions. We conduct a variety of experiments on three
typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech
Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant
performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2)
on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the
CWS task, and results comparable to SOTA performances on NER datasets, and POS
datasets.",preprint,
Evince the artifacts of Spoof Speech by blending Vocal Tract and Voice Source Features,"[arxiv.Result.Author('Tadipatri Uday Kiran Reddy'), arxiv.Result.Author('Sahukari Chaitanya Varun'), arxiv.Result.Author('Kota Pranav Kumar Sankala Sreekanth'), arxiv.Result.Author('Kodukula Sri Rama Murty')]",2022-12-05 04:02:50+00:00,"With the rapid advancement in synthetic speech generation technologies, great
interest in differentiating spoof speech from the natural speech is emerging in
the research community. The identification of these synthetic signals is a
difficult task not only for the cutting-edge classification models but also for
humans themselves. To prevent potential adverse effects, it becomes crucial to
detect spoof signals. From a forensics perspective, it is also important to
predict the algorithm which generated them to identify the forger. This needs
an understanding of the underlying attributes of spoof signals which serve as a
signature for the synthesizer. This study emphasizes the segments of speech
signals critical in identifying their authenticity by utilizing the Vocal Tract
System(\textit{VTS}) and Voice Source(\textit{VS}) features.
  In this paper, we propose a system that detects spoof signals as well as
identifies the corresponding speech-generating algorithm. We achieve 99.58\% in
algorithm classification accuracy. From experiments, we found that a VS
feature-based system gives more attention to the transition of phonemes, while,
a VTS feature-based system gives more attention to stationary segments of
speech signals. We perform model fusion techniques on the VS-based and
VTS-based systems to exploit the complementary information to develop a robust
classifier. Upon analyzing the confusion plots we found that WaveRNN is poorly
classified depicting more naturalness. On the other hand, we identified that
synthesizer like Waveform Concatenation, and Neural Source Filter is classified
with the highest accuracy. Practical implications of this work can aid
researchers from both forensics (leverage artifacts) and the speech communities
(mitigate artifacts).",,
Fast and accurate factorized neural transducer for text adaption of end-to-end speech recognition models,"[arxiv.Result.Author('Rui Zhao'), arxiv.Result.Author('Jian Xue'), arxiv.Result.Author('Partha Parthasarathy'), arxiv.Result.Author('Veljko Miljanic'), arxiv.Result.Author('Jinyu Li')]",2022-12-05 02:52:21+00:00,"Neural transducer is now the most popular end-to-end model for speech
recognition, due to its naturally streaming ability. However, it is challenging
to adapt it with text-only data. Factorized neural transducer (FNT) model was
proposed to mitigate this problem. The improved adaptation ability of FNT on
text-only adaptation data came at the cost of lowered accuracy compared to the
standard neural transducer model. We propose several methods to improve the
performance of the FNT model. They are: adding CTC criterion during training,
adding KL divergence loss during adaptation, using a pre-trained language model
to seed the vocabulary predictor, and an efficient adaptation approach by
interpolating the vocabulary predictor with the n-gram language model. A
combination of these approaches results in a relative word-error-rate reduction
of 9.48\% from the standard FNT model. Furthermore, n-gram interpolation with
the vocabulary predictor improves the adaptation speed hugely with satisfactory
adaptation performance.",,
Speech MOS multi-task learning and rater bias correction,"[arxiv.Result.Author('Haleh Akrami'), arxiv.Result.Author('Hannes Gamper')]",2022-12-04 20:06:27+00:00,"Perceptual speech quality is an important performance metric for
teleconferencing applications. The mean opinion score (MOS) is standardized for
the perceptual evaluation of speech quality and is obtained by asking listeners
to rate the quality of a speech sample. Recently, there has been increasing
research interest in developing models for estimating MOS blindly. Here we
propose a multi-task framework to include additional labels and data in
training to improve the performance of a blind MOS estimation model.
Experimental results indicate that the proposed model can be trained to jointly
estimate MOS, reverberation time (T60), and clarity (C50) by combining two
disjoint data sets in training, one containing only MOS labels and the other
containing only T60 and C50 labels. Furthermore, we use a semi-supervised
framework to combine two MOS data sets in training, one containing only MOS
labels (per ITU-T Recommendation P.808), and the other containing separate
scores for speech signal, background noise, and overall quality (per ITU-T
Recommendation P.835). Finally, we present preliminary results for addressing
individual rater bias in the MOS labels.",Submitted to ICASSP 2023,
Tragic Talkers: A Shakespearean Sound- and Light-Field Dataset for Audio-Visual Machine Learning Research,"[arxiv.Result.Author('Davide Berghi'), arxiv.Result.Author('Marco Volino'), arxiv.Result.Author('Philip J. B. Jackson')]",2022-12-04 18:48:44+00:00,"3D audio-visual production aims to deliver immersive and interactive
experiences to the consumer. Yet, faithfully reproducing real-world 3D scenes
remains a challenging task. This is partly due to the lack of available
datasets enabling audio-visual research in this direction. In most of the
existing multi-view datasets, the accompanying audio is neglected. Similarly,
datasets for spatial audio research primarily offer unimodal content, and when
visual data is included, the quality is far from meeting the standard
production needs. We present ""Tragic Talkers"", an audio-visual dataset
consisting of excerpts from the ""Romeo and Juliet"" drama captured with
microphone arrays and multiple co-located cameras for light-field video. Tragic
Talkers provides ideal content for object-based media (OBM) production. It is
designed to cover various conventional talking scenarios, such as monologues,
two-people conversations, and interactions with considerable movement and
occlusion, yielding 30 sequences captured from a total of 22 different points
of view and two 16-element microphone arrays. Additionally, we provide voice
activity labels, 2D face bounding boxes for each camera view, 2D pose detection
keypoints, 3D tracking data of the mouth of the actors, and dialogue
transcriptions. We believe the community will benefit from this dataset as it
can assist multidisciplinary research. Possible uses of the dataset are
discussed.",,
Melody transcription via generative pre-training,"[arxiv.Result.Author('Chris Donahue'), arxiv.Result.Author('John Thickstun'), arxiv.Result.Author('Percy Liang')]",2022-12-04 18:09:23+00:00,"Despite the central role that melody plays in music perception, it remains an
open challenge in music information retrieval to reliably detect the notes of
the melody present in an arbitrary music recording. A key challenge in melody
transcription is building methods which can handle broad audio containing any
number of instrument ensembles and musical styles - existing strategies work
well for some melody instruments or styles but not all. To confront this
challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a
generative model of broad music audio, thereby improving performance on melody
transcription by $20$% relative to conventional spectrogram features. Another
obstacle in melody transcription is a lack of training data - we derive a new
dataset containing $50$ hours of melody transcriptions from crowdsourced
annotations of broad music. The combination of generative pre-training and a
new dataset for this task results in $77$% stronger performance on melody
transcription relative to the strongest available baseline. By pairing our new
melody transcription approach with solutions for beat detection, key
estimation, and chord recognition, we build Sheet Sage, a system capable of
transcribing human-readable lead sheets directly from music audio.
  Audio examples can be found at https://chrisdonahue.com/sheetsage and code at
https://github.com/chrisdonahue/sheetsage .",Published as a conference paper at ISMIR 2022,
Improving End-to-end Speech Translation by Leveraging Auxiliary Speech and Text Data,"[arxiv.Result.Author('Yuhao Zhang'), arxiv.Result.Author('Chen Xu'), arxiv.Result.Author('Bojie Hu'), arxiv.Result.Author('Chunliang Zhang'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('Jingbo Zhu')]",2022-12-04 09:27:56+00:00,"We present a method for introducing a text encoder into pre-trained
end-to-end speech translation systems. It enhances the ability of adapting one
modality (i.e., source-language speech) to another (i.e., source-language
text). Thus, the speech translation model can learn from both unlabeled and
labeled data, especially when the source-language text data is abundant. Beyond
this, we present a denoising method to build a robust text encoder that can
deal with both normal and noisy text data. Our system sets new
state-of-the-arts on the MuST-C En-De, En-Fr, and LibriSpeech En-Fr tasks.",Accepted to AAAI 2023,
"Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech","[arxiv.Result.Author('Dominik Wagner'), arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Hector A. Cordourier Maruri'), arxiv.Result.Author('Tobias Bocklet')]",2022-12-04 09:06:18+00:00,"This work adapts two recent architectures of generative models and evaluates
their effectiveness for the conversion of whispered speech to normal speech. We
incorporate the normal target speech into the training criterion of
vector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby
conditioning the systems to recover voiced speech from whispered inputs.
Objective and subjective quality measures indicate that both VQ-VAEs and
MelGANs can be modified to perform the conversion task. We find that the
proposed approaches significantly improve the Mel cepstral distortion (MCD)
metric by at least 25% relative to a DiscoGAN baseline. Subjective listening
tests suggest that the MelGAN-based system significantly improves naturalness,
intelligibility, and voicing compared to the whispered input speech. A novel
evaluation measure based on differences between latent speech representations
also indicates that our MelGAN-based approach yields improvements relative to
the baseline.",Accepted at SLT 2022,
A subjective study of the perceptual acceptability of audio-video desynchronization in sports videos,[arxiv.Result.Author('Joshua Peter Ebenezer')],2022-12-03 20:35:16+00:00,"This paper presents the results of a study conducted on the perceptual
acceptability of audio-video desynchronization for sports videos. The study was
conducted with 45 videos generated by applying 8 audio-video offsets on 5
source contents. 20 subjects participated in the study. The results show that
humans are more sensitive to audio-video offset errors for speech stimuli, and
the complex events that occur in sports broadcasts have higher thresholds of
acceptability. This suggests the tuning of audio-video synchronization
requirements in broadcasting to the content of the broadcast.","6 pages; for associated code see
  https://github.com/JoshuaEbenezer/avsync_study",
Unsupervised Fine-Tuning Data Selection for ASR Using Self-Supervised Speech Models,"[arxiv.Result.Author('Reem Gody'), arxiv.Result.Author('David Harwath')]",2022-12-03 18:05:08+00:00,"Self-supervised learning (SSL) has been able to leverage unlabeled data to
boost the performance of automatic speech recognition (ASR) models when we have
access to only a small amount of transcribed speech data. However, this raises
the question of which subset of the available unlabeled data should be selected
for transcription. Our work investigates different unsupervised data selection
techniques for fine-tuning the HuBERT model under a limited transcription
budget. We investigate the impact of speaker diversity, gender bias, and topic
diversity on the downstream ASR performance. We also devise two novel
techniques for unsupervised data selection: pre-training loss based data
selection and the perplexity of byte pair encoded clustered units (PBPE) and we
show how these techniques compare to pure random data selection. Finally, we
analyze the correlations between the inherent characteristics of the selected
fine-tuning subsets as well as how these characteristics correlate with the
resultant word error rate. We demonstrate the importance of token diversity,
speaker diversity, and topic diversity in achieving the best performance in
terms of WER.",,
A dataset for audio-video based vehicle speed estimation,"[arxiv.Result.Author('Slobodan Djukanović'), arxiv.Result.Author('Nikola Bulatović'), arxiv.Result.Author('Ivana Čavor')]",2022-12-03 17:02:57+00:00,"Accurate speed estimation of road vehicles is important for several reasons.
One is speed limit enforcement, which represents a crucial tool in decreasing
traffic accidents and fatalities. Compared with other research areas and
domains, the number of available datasets for vehicle speed estimation is still
very limited. We present a dataset of on-road audio-video recordings of single
vehicles passing by a camera at known speeds, maintained stable by the on-board
cruise control. The dataset contains thirteen vehicles, selected to be as
diverse as possible in terms of manufacturer, production year, engine type,
power and transmission, resulting in a total of $ 400 $ annotated audio-video
recordings. The dataset is fully available and intended as a public benchmark
to facilitate research in audio-video vehicle speed estimation. In addition to
the dataset, we propose a cross-validation strategy which can be used in a
machine learning model for vehicle speed estimation. Two approaches to
training-validation split of the dataset are proposed.","30th Telecommunications Forum TELFOR 2022, Belgrade, Serbia, November
  15-16, 2022. 5 pages, 2 figures, 1 table",
UniSyn: An End-to-End Unified Model for Text-to-Speech and Singing Voice Synthesis,"[arxiv.Result.Author('Yi Lei'), arxiv.Result.Author('Shan Yang'), arxiv.Result.Author('Xinsheng Wang'), arxiv.Result.Author('Qicong Xie'), arxiv.Result.Author('Jixun Yao'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Dan Su')]",2022-12-03 05:58:10+00:00,"Text-to-speech (TTS) and singing voice synthesis (SVS) aim at generating
high-quality speaking and singing voice according to textual input and music
scores, respectively. Unifying TTS and SVS into a single system is crucial to
the applications requiring both of them. Existing methods usually suffer from
some limitations, which rely on either both singing and speaking data from the
same person or cascaded models of multiple tasks. To address these problems, a
simplified elegant framework for TTS and SVS, named UniSyn, is proposed in this
paper. It is an end-to-end unified model that can make a voice speak and sing
with only singing or speaking data from this person. To be specific, a
multi-conditional variational autoencoder (MC-VAE), which constructs two
independent latent sub-spaces with the speaker- and style-related (i.e. speak
or sing) conditions for flexible control, is proposed in UniSyn. Moreover,
supervised guided-VAE and timbre perturbation with the Wasserstein distance
constraint are leveraged to further disentangle the speaker timbre and style.
Experiments conducted on two speakers and two singers demonstrate that UniSyn
can generate natural speaking and singing voice without corresponding training
data. The proposed approach outperforms the state-of-the-art end-to-end voice
generation work, which proves the effectiveness and advantages of UniSyn.",,
Can we still use PEAQ? A Performance Analysis of the ITU Standard for the Objective Assessment of Perceived Audio Quality,"[arxiv.Result.Author('Pablo M. Delgado'), arxiv.Result.Author('Jürgen Herre')]",2022-12-02 22:09:15+00:00,"The Perceptual Evaluation of Audio Quality (PEAQ) method as described in the
International Telecommunication Union (ITU) recommendation ITU-R BS.1387 has
been widely used for computationally estimating the quality of perceptually
coded audio signals without the need for extensive subjective listening tests.
However, many reports have highlighted clear limitations of the scheme after
the end of its standardization, particularly involving signals coded with newer
technologies such as bandwidth extension or parametric multi-channel coding.
Until now, no other method for measuring the quality of both speech and audio
signals has been standardized by the ITU. Therefore, a further investigation of
the causes for these limitations would be beneficial to a possible update of
said scheme. Our experimental results indicate that the performance of PEAQ's
model of disturbance loudness is still as good as (and sometimes superior to)
other state-of-the-art objective measures, albeit with varying performance
depending on the type of degraded signal content (i.e. speech or music). This
finding evidences the need for an improved cognitive model. In addition,
results indicate that an updated mapping of Model Output Values (MOVs) to
PEAQ's Distortion Index (DI) based on newer training data can greatly improve
performance. Finally, some suggestions for the improvement of PEAQ are provided
based on the reported results and comparison to other systems.","Accepter manuscript for 2020 Twelfth International Conference on
  Quality of Multimedia Experience (QoMEX 2020)","2020 Twelfth International Conference on Quality of Multimedia
  Experience (QoMEX), 2020, pp. 1-6,"
NEAL: An open-source tool for audio annotation,"[arxiv.Result.Author('Anthony Gibbons'), arxiv.Result.Author('Ian Donohue'), arxiv.Result.Author('Courtney E. Gorman'), arxiv.Result.Author('Emma King'), arxiv.Result.Author('Andrew Parnell')]",2022-12-02 21:45:12+00:00,"Passive acoustic monitoring is used widely in ecology, biodiversity, and
conservation studies. Data sets collected via acoustic monitoring are often
extremely large and built to be processed automatically using Artificial
Intelligence and Machine learning models, which aim to replicate the work of
domain experts. These models, being supervised learning algorithms, need to be
trained on high quality annotations produced by experts. Since the experts are
often resource-limited, a cost-effective process for annotating audio is needed
to get maximal use out of the data. We present an open-source interactive audio
data annotation tool, NEAL (Nature+Energy Audio Labeller). Built using R and
the associated Shiny framework, the tool provides a reactive environment where
users can quickly annotate audio files and adjust settings that automatically
change the corresponding elements of the user interface. The app has been
designed with the goal of having both expert birders and citizen scientists
contribute to acoustic annotation projects. The popularity and flexibility of R
programming in bioacoustics means that the Shiny app can be modified for other
bird labelling data sets, or even to generic audio labelling tasks. We
demonstrate the app by labelling data collected from wind farm sites across
Ireland.",,
Objective Assessment of Spatial Audio Quality using Directional Loudness Maps,"[arxiv.Result.Author('Pablo M. Delgado'), arxiv.Result.Author('Jürgen Herre')]",2022-12-02 21:22:29+00:00,"This work introduces a feature extracted from stereophonic/binaural audio
signals aiming to represent a measure of perceived quality degradation in
processed spatial auditory scenes. The feature extraction technique is based on
a simplified stereo signal model considering auditory events positioned towards
a given direction in the stereo field using amplitude panning (AP) techniques.
We decompose the stereo signal into a set of directional signals for given AP
values in the Short-Time Fourier Transform domain and calculate their overall
loudness to form a directional loudness representation or maps. Then, we
compare directional loudness maps of a reference signal and a deteriorated
version to derive a distortion measure aiming to describe the associated
perceived degradation scores reported in listening tests. The measure is then
tested on an extensive listening test database with stereo signals processed by
state-of-the-art perceptual audio codecs using non waveform-preserving
techniques such as bandwidth extension and joint stereo coding, known for
presenting a challenge to existing quality predictors. Results suggest that the
derived distortion measure can be incorporated as an extension to existing
automated perceptual quality assessment algorithms for improving prediction on
spatially coded audio signals.",Accepted paper at ICASSP 2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)"
Investigations on the Influence of Combined Inter-Aural Cue Distortions on Overall Audio Quality,"[arxiv.Result.Author('Pablo M. Delgado'), arxiv.Result.Author('Jürgen Herre')]",2022-12-02 20:20:16+00:00,"There is a considerable interest in developing algorithms that can predict
audio quality of perceptually coded signals to avoid the cost of extensive
listening tests during development time. While many established algorithms for
predicting the perceived quality of signals with monaural (timbral) distortions
are available (PEAQ, POLQA), predicting the quality degradation of stereo and
multi-channel spatial signals is still considered a challenge. Audio quality
degradation arising from spatial distortions is usually measured in terms of
well known inter-aural cue distortion measures such as Inter-aural Level
Difference Distortions (ILDD), Inter-aural Time Difference Distortions (ITDD)
and Inter-aural Cross Correlation Distortions (IACCD). However, the extent to
which their interaction influences the overall audio quality degradation in
complex signals as expressed - for example - in a multiple stimuli test is not
yet thoroughly studied. We propose a systematic approach that introduces
controlled combinations of spatial distortions on a representative set of
signals and evaluates their influence on overall perceived quality degradation
by analyzing listening test scores over said signals. From this study we derive
guidelines for designing meaningful distortion measures that consider
inter-aural cue distortion interactions.","A previous version of this paper (minus errata) was presented at
  Fortschritte der Akustik - DAGA 2019 (Rostock, Germany)","Tagungsband - DAGA 2019 - 45. Jahrestagung f\""ur Akustik"
Continual Learning for On-Device Speech Recognition using Disentangled Conformers,"[arxiv.Result.Author('Anuj Diwan'), arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Paden Tomasello'), arxiv.Result.Author('Eunsol Choi'), arxiv.Result.Author('David Harwath'), arxiv.Result.Author('Abdelrahman Mohamed')]",2022-12-02 18:58:51+00:00,"Automatic speech recognition research focuses on training and evaluating on
static datasets. Yet, as speech models are increasingly deployed on personal
devices, such models encounter user-specific distributional shifts. To simulate
this real-world scenario, we introduce LibriContinual, a continual learning
benchmark for speaker-specific domain adaptation derived from LibriVox
audiobooks, with data corresponding to 118 individual speakers and 6 train
splits per speaker of different sizes. Additionally, current speech recognition
models and continual learning algorithms are not optimized to be
compute-efficient. We adapt a general-purpose training algorithm NetAug for ASR
and create a novel Conformer variant called the DisConformer (Disentangled
Conformer). This algorithm produces ASR models consisting of a frozen 'core'
network for general-purpose use and several tunable 'augment' networks for
speaker-specific tuning. Using such models, we propose a novel
compute-efficient continual learning algorithm called DisentangledCL. Our
experiments show that the DisConformer models significantly outperform
baselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On
speaker-specific LibriContinual they significantly outperform
trainable-parameter-matched baselines (by 20.65% rel. WER on test) and even
match fully finetuned baselines in some settings.","8 pages, 2 figures. Submitted to ICASSP 2023",
Relative Acoustic Features for Distance Estimation in Smart-Homes,"[arxiv.Result.Author('Francesco Nespoli'), arxiv.Result.Author('Daniel Barreda'), arxiv.Result.Author('Patrick A. Naylor')]",2022-12-02 16:58:35+00:00,"Any audio recording encapsulates the unique fingerprint of the associated
acoustic environment, namely the background noise and reverberation.
Considering the scenario of a room equipped with a fixed smart speaker device
with one or more microphones and a wearable smart device (watch, glasses or
smartphone), we employed the improved proportionate normalized least mean
square adaptive filter to estimate the relative room impulse response mapping
the audio recordings of the two devices. We performed inter-device distance
estimation by exploiting a new set of features obtained extending the
definition of some acoustic attributes of the room impulse response to its
relative version. In combination with the sparseness measure of the estimated
relative room impulse response, the relative features allow precise
inter-device distance estimation which can be exploited for tasks such as best
microphone selection or acoustic scene analysis. Experimental results from
simulated rooms of different dimensions and reverberation times demonstrate the
effectiveness of this computationally lightweight approach for smart home
acoustic ranging applications",,Interspeech 2022
SumREN: Summarizing Reported Speech about Events in News,"[arxiv.Result.Author('Revanth Gangi Reddy'), arxiv.Result.Author('Heba Elfardy'), arxiv.Result.Author('Hou Pong Chan'), arxiv.Result.Author('Kevin Small'), arxiv.Result.Author('Heng Ji')]",2022-12-02 12:51:39+00:00,"A primary objective of news articles is to establish the factual record for
an event, frequently achieved by conveying both the details of the specified
event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and
how people reacted to it (i.e., reported statements). However, existing work on
news summarization almost exclusively focuses on the event details. In this
work, we propose the novel task of summarizing the reactions of different
speakers, as expressed by their reported statements, to a given event. To this
end, we create a new multi-document summarization benchmark, SUMREN, comprising
745 summaries of reported statements from various public figures obtained from
633 news articles discussing 132 events. We propose an automatic silver
training data generation approach for our task, which helps smaller models like
BART achieve GPT-3 level performance on this task. Finally, we introduce a
pipeline-based framework for summarizing reported speech, which we empirically
show to generate summaries that are more abstractive and factual than baseline
query-focused summarization approaches.",Accepted at AAAI 2023,
ExARN: self-attending RNN for target speaker extraction,"[arxiv.Result.Author('Pengjie Shen'), arxiv.Result.Author('Shulin He'), arxiv.Result.Author('Xueliang Zhang')]",2022-12-02 11:38:42+00:00,"Target speaker extraction is to extract the target speaker, specified by
enrollment utterance, in an environment with other competing speakers.
Therefore, the task needs to solve two problems, speaker identification and
separation, at the same time. In this paper, we combine self-attention and
Recurrent Neural Networks (RNN). Further, we exploit various ways to combining
different auxiliary information with mixed representations. Experimental
results show that our proposed model achieves excellent performance on the task
of target speaker extraction.","Submitted to ICASSP 2023, 5 pages with 3 figures",
Cross-Modal Mutual Learning for Cued Speech Recognition,"[arxiv.Result.Author('Lei Liu'), arxiv.Result.Author('Li Liu')]",2022-12-02 10:45:33+00:00,"Automatic Cued Speech Recognition (ACSR) provides an intelligent
human-machine interface for visual communications, where the Cued Speech (CS)
system utilizes lip movements and hand gestures to code spoken language for
hearing-impaired people. Previous ACSR approaches often utilize direct feature
concatenation as the main fusion paradigm. However, the asynchronous modalities
(\textit{i.e.}, lip, hand shape and hand position) in CS may cause interference
for feature concatenation. To address this challenge, we propose a transformer
based cross-modal mutual learning framework to prompt multi-modal interaction.
Compared with the vanilla self-attention, our model forces modality-specific
information of different modalities to pass through a modality-invariant
codebook, collating linguistic representations for tokens of each modality.
Then the shared linguistic knowledge is used to re-synchronize multi-modal
sequences. Moreover, we establish a novel large-scale multi-speaker CS dataset
for Mandarin Chinese. To our knowledge, this is the first work on ACSR for
Mandarin Chinese. Extensive experiments are conducted for different languages
(\textit{i.e.}, Chinese, French, and British English). Results demonstrate that
our model exhibits superior recognition performance to the state-of-the-art by
a large margin.",,
AccEar: Accelerometer Acoustic Eavesdropping with Unconstrained Vocabulary,"[arxiv.Result.Author('Pengfei Hu'), arxiv.Result.Author('Hui Zhuang'), arxiv.Result.Author('Panneer Selvam Santhalingamy'), arxiv.Result.Author('Riccardo Spolaor'), arxiv.Result.Author('Parth Pathaky'), arxiv.Result.Author('Guoming Zhang'), arxiv.Result.Author('Xiuzhen Cheng')]",2022-12-02 09:13:28+00:00,"With the increasing popularity of voice-based applications, acoustic
eavesdropping has become a serious threat to users' privacy. While on
smartphones the access to microphones needs an explicit user permission,
acoustic eavesdropping attacks can rely on motion sensors (such as
accelerometer and gyroscope), which access is unrestricted. However, previous
instances of such attacks can only recognize a limited set of pre-trained words
or phrases. In this paper, we present AccEar, an accelerometerbased acoustic
eavesdropping attack that can reconstruct any audio played on the smartphone's
loudspeaker with unconstrained vocabulary. We show that an attacker can employ
a conditional Generative Adversarial Network (cGAN) to reconstruct highfidelity
audio from low-frequency accelerometer signals. The presented cGAN model learns
to recreate high-frequency components of the user's voice from low-frequency
accelerometer signals through spectrogram enhancement. We assess the
feasibility and effectiveness of AccEar attack in a thorough set of experiments
using audio from 16 public personalities. As shown by the results in both
objective and subjective evaluations, AccEar successfully reconstructs user
speeches from accelerometer signals in different scenarios including varying
sampling rate, audio volume, device model, etc.",2022 IEEE Symposium on Security and Privacy (SP),2022 IEEE Symposium on Security and Privacy (SP)
Role of Audio in Audio-Visual Video Summarization,"[arxiv.Result.Author('Ibrahim Shoer'), arxiv.Result.Author('Berkay Kopru'), arxiv.Result.Author('Engin Erzin')]",2022-12-02 09:11:49+00:00,"Video summarization attracts attention for efficient video representation,
retrieval, and browsing to ease volume and traffic surge problems. Although
video summarization mostly uses the visual channel for compaction, the benefits
of audio-visual modeling appeared in recent literature. The information coming
from the audio channel can be a result of audio-visual correlation in the video
content. In this study, we propose a new audio-visual video summarization
framework integrating four ways of audio-visual information fusion with
GRU-based and attention-based networks. Furthermore, we investigate a new
explainability methodology using audio-visual canonical correlation analysis
(CCA) to better understand and explain the role of audio in the video
summarization task. Experimental evaluations on the TVSum dataset attain F1
score and Kendall-tau score improvements for the audio-visual video
summarization. Furthermore, splitting video content on TVSum and COGNIMUSE
datasets based on audio-visual CCA as positively and negatively correlated
videos yields a strong performance improvement over the positively correlated
videos for audio-only and audio-visual video summarization.",,
SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition,"[arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Wenjie Liu'), arxiv.Result.Author('Kaitao Song'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Xiang-Yang Li'), arxiv.Result.Author('Tao Qin'), arxiv.Result.Author('Edward Lin'), arxiv.Result.Author('Tie-Yan Liu')]",2022-12-02 09:11:32+00:00,"Error correction in automatic speech recognition (ASR) aims to correct those
incorrect words in sentences generated by ASR models. Since recent ASR models
usually have low word error rate (WER), to avoid affecting originally correct
tokens, error correction models should only modify incorrect words, and
therefore detecting incorrect words is important for error correction. Previous
works on error correction either implicitly detect error words through
target-source attention or CTC (connectionist temporal classification) loss, or
explicitly locate specific deletion/substitution/insertion errors. However,
implicit error detection does not provide clear signal about which tokens are
incorrect and explicit error detection suffers from low detection accuracy. In
this paper, we propose SoftCorrect with a soft error detection mechanism to
avoid the limitations of both explicit and implicit error detection.
Specifically, we first detect whether a token is correct or not through a
probability produced by a dedicatedly designed language model, and then design
a constrained CTC loss that only duplicates the detected incorrect tokens to
let the decoder focus on the correction of error tokens. Compared with implicit
error detection with CTC loss, SoftCorrect provides explicit signal about which
words are incorrect and thus does not need to duplicate every token but only
incorrect tokens; compared with explicit error detection, SoftCorrect does not
detect specific deletion/substitution/insertion errors but just leaves it to
CTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that
SoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming
previous works by a large margin, while still enjoying fast speed of parallel
generation.",AAAI 2023,
Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations,"[arxiv.Result.Author('Jaidev Shriram'), arxiv.Result.Author('Makarand Tapaswi'), arxiv.Result.Author('Vinoo Alluri')]",2022-12-02 08:57:20+00:00,"Reading, much like music listening, is an immersive experience that
transports readers while taking them on an emotional journey. Listening to
complementary music has the potential to amplify the reading experience,
especially when the music is stylistically cohesive and emotionally relevant.
In this paper, we propose the first fully automatic method to build a dense
soundtrack for books, which can play high-quality instrumental music for the
entirety of the reading duration. Our work employs a unique text processing and
music weaving pipeline that determines the context and emotional composition of
scenes in a chapter. This allows our method to identify and play relevant
excerpts from the soundtrack of the book's movie adaptation. By relying on the
movie composer's craftsmanship, our book soundtracks include expert-made motifs
and other scene-specific musical characteristics. We validate the design
decisions of our approach through a perceptual study. Our readers note that the
book soundtrack greatly enhanced their reading experience, due to high
immersiveness granted via uninterrupted and style-consistent music, and a
heightened emotional state attained via high precision emotion and scene
context recognition.","Accepted to ISMIR 2022. Project page:
  https://auto-book-soundtrack.github.io/",
Injecting Spatial Information for Monaural Speech Enhancement via Knowledge Distillation,"[arxiv.Result.Author('Xinmeng Xu'), arxiv.Result.Author('Weiping Tu'), arxiv.Result.Author('Yuhong Yang')]",2022-12-02 07:49:37+00:00,"Monaural speech enhancement (SE) provides a versatile and cost-effective
approach to SE tasks by utilizing recordings from a single microphone. However,
the monaural SE lags performance behind multi-channel SE as the monaural SE
methods are unable to extract spatial information from one-channel recordings,
which greatly limits their application scenarios. To address this issue, we
inject spatial information into the monaural SE model and propose a knowledge
distillation strategy to enable the monaural SE model to learn binaural speech
features from the binaural SE model, which makes monaural SE model possible to
reconstruct higher intelligibility and quality enhanced speeches under low
signal-to-noise ratio (SNR) conditions. Extensive experiments show that our
proposed monaural SE model by injecting spatial information via knowledge
distillation achieves favorable performance against other monaural SE models
with fewer parameters.",Submitted to ICASSP 2023,
A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling,"[arxiv.Result.Author('Z. Guo'), arxiv.Result.Author('J. Kang'), arxiv.Result.Author('D. Herremans')]",2022-12-02 05:04:31+00:00,"Following the success of the transformer architecture in the natural language
domain, transformer-like architectures have been widely applied to the domain
of symbolic music recently. Symbolic music and text, however, are two different
modalities. Symbolic music contains multiple attributes, both absolute
attributes (e.g., pitch) and relative attributes (e.g., pitch interval). These
relative attributes shape human perception of musical motifs. These important
relative attributes, however, are mostly ignored in existing symbolic music
modeling methods with the main reason being the lack of a musically-meaningful
embedding space where both the absolute and relative embeddings of the symbolic
music tokens can be efficiently represented. In this paper, we propose the
Fundamental Music Embedding (FME) for symbolic music based on a bias-adjusted
sinusoidal encoding within which both the absolute and the relative attributes
can be embedded and the fundamental musical properties (e.g., translational
invariance) are explicitly preserved. Taking advantage of the proposed FME, we
further propose a novel attention mechanism based on the relative index, pitch
and onset embeddings (RIPO attention) such that the musical domain knowledge
can be fully utilized for symbolic music modeling. Experiment results show that
our proposed model: RIPO transformer which utilizes FME and RIPO attention
outperforms the state-of-the-art transformers (i.e., music transformer, linear
transformer) in a melody completion task. Moreover, using the RIPO transformer
in a downstream music generation task, we notice that the notorious
degeneration phenomenon no longer exists and the music generated by the RIPO
transformer outperforms the music generated by state-of-the-art transformer
models in both subjective and objective evaluations.",This paper is accepted at AAAI 2023,
SOLD: Sinhala Offensive Language Dataset,"[arxiv.Result.Author('Tharindu Ranasinghe'), arxiv.Result.Author('Isuri Anuradha'), arxiv.Result.Author('Damith Premasiri'), arxiv.Result.Author('Kanishka Silva'), arxiv.Result.Author('Hansi Hettiarachchi'), arxiv.Result.Author('Lasitha Uyangodage'), arxiv.Result.Author('Marcos Zampieri')]",2022-12-01 20:18:21+00:00,"The widespread of offensive content online, such as hate speech and
cyber-bullying, is a global phenomenon. This has sparked interest in the
artificial intelligence (AI) and natural language processing (NLP) communities,
motivating the development of various systems trained to detect potentially
harmful content automatically. These systems require annotated datasets to
train the machine learning (ML) models. However, with a few notable exceptions,
most datasets on this topic have dealt with English and a few other
high-resource languages. As a result, the research in offensive language
identification has been limited to these languages. This paper addresses this
gap by tackling offensive language identification in Sinhala, a low-resource
Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce
the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments
on this dataset. SOLD is a manually annotated dataset containing 10,000 posts
from Twitter annotated as offensive and not offensive at both sentence-level
and token-level, improving the explainability of the ML models. SOLD is the
first large publicly available offensive language dataset compiled for Sinhala.
We also introduce SemiSOLD, a larger dataset containing more than 145,000
Sinhala tweets, annotated following a semi-supervised approach.","This is a preprint of an article submitted to Applied Intelligence,
  Springer",
Audio-Visual Activity Guided Cross-Modal Identity Association for Active Speaker Detection,"[arxiv.Result.Author('Rahul Sharma'), arxiv.Result.Author('Shrikanth Narayanan')]",2022-12-01 14:46:00+00:00,"Active speaker detection in videos addresses associating a source face,
visible in the video frames, with the underlying speech in the audio modality.
The two primary sources of information to derive such a speech-face
relationship are i) visual activity and its interaction with the speech signal
and ii) co-occurrences of speakers' identities across modalities in the form of
face and speech. The two approaches have their limitations: the audio-visual
activity models get confused with other frequently occurring vocal activities,
such as laughing and chewing, while the speakers' identity-based methods are
limited to videos having enough disambiguating information to establish a
speech-face association. Since the two approaches are independent, we
investigate their complementary nature in this work. We propose a novel
unsupervised framework to guide the speakers' cross-modal identity association
with the audio-visual activity for active speaker detection. Through
experiments on entertainment media videos from two benchmark datasets, the AVA
active speaker (movies) and Visual Person Clustering Dataset (TV shows), we
show that a simple late fusion of the two approaches enhances the active
speaker detection performance.",Under review at OJSP,
Identifying Different Layers of Online Misogyny,"[arxiv.Result.Author('Wienke Strathern'), arxiv.Result.Author('Juergen Pfeffer')]",2022-12-01 13:07:49+00:00,"Social media has become an everyday means of interaction and information
sharing on the Internet. However, posts on social networks are often aggressive
and toxic, especially when the topic is controversial or politically charged.
Radicalization, extreme speech, and in particular online misogyny against women
in the public eye have become alarmingly negative features of online
discussions. The present study proposes a methodological approach to contribute
to ongoing discussions about the multiple ways in which women, their
experiences, and their choices are attacked in polarized social media
responses. Based on a review of theories on and detection methods for misogyny,
we present a classification scheme that incorporates eleven different explicit
as well as implicit layers of online misogyny. We also apply our classes to a
case study related to online aggression against Amber Heard in the context of
her allegations of domestic violence against Johnny Depp. We finally evaluate
the reliability of Google's Perspective API -- a standard for detecting toxic
language -- for determining gender discrimination as toxicity. We show that a
large part of online misogyny, especially when verbalized without expletive
terms but instead more implicitly is not captured automatically.",,
Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition,"[arxiv.Result.Author('Alexandre Bittar'), arxiv.Result.Author('Philip N. Garner')]",2022-12-01 12:36:26+00:00,"Compared to conventional artificial neurons that produce dense and
real-valued responses, biologically-inspired spiking neurons transmit sparse
and binary information, which can also lead to energy-efficient
implementations. Recent research has shown that spiking neural networks can be
trained like standard recurrent neural networks using the surrogate gradient
method. They have shown promising results on speech command recognition tasks.
Using the same technique, we show that they are scalable to large vocabulary
continuous speech recognition, where they are capable of replacing LSTMs in the
encoder with only minor loss of performance. This suggests that they may be
applicable to more involved sequence-to-sequence tasks. Moreover, in contrast
to their recurrent non-spiking counterparts, they show robustness to exploding
gradient problems without the need to use gates.",Submitted to ICASSP 2023,
High Fidelity Speech Enhancement with Band-split RNN,"[arxiv.Result.Author('Jianwei Yu'), arxiv.Result.Author('Yi Luo'), arxiv.Result.Author('Hangting Chen'), arxiv.Result.Author('Rongzhi Gu'), arxiv.Result.Author('Chao Weng')]",2022-12-01 10:18:54+00:00,"This report presents the development of our speech enhancement system, which
includes the use of a recently proposed music separation model, the band-split
recurrent neural network (BSRNN), and a MetricGAN-based training objective to
improve non-differentiable quality metrics such as perceptual evaluation of
speech quality (PESQ) score. Experiment conducted on Interspeech 2021 DNS
challenge shows that our BSRNN system outperforms various top-ranking benchmark
systems in previous deep noise suppression (DNS) challenges and achieves
state-of-the-art (SOTA) result on the DNS-2020 non-blind test set in both
offline and online scenarios.",,
Deep neural network techniques for monaural speech enhancement: state of the art analysis,[arxiv.Result.Author('Peter Ochieng')],2022-12-01 08:59:21+00:00,"Deep neural networks (DNN) techniques have become pervasive in domains such
as natural language processing and computer vision. They have achieved great
success in these domains in task such as machine translation and image
generation. Due to their success, these data driven techniques have been
applied in audio domain. More specifically, DNN models have been applied in
speech enhancement domain to achieve denosing, dereverberation and
multi-speaker separation in monaural speech enhancement. In this paper, we
review some dominant DNN techniques being employed to achieve speech
separation. The review looks at the whole pipeline of speech enhancement from
feature extraction, how DNN based tools are modelling both global and local
features of speech and model training (supervised and unsupervised). We also
review the use of speech-enhancement pre-trained models to boost speech
enhancement process. The review is geared towards covering the dominant trends
with regards to DNN application in speech enhancement in speech obtained via a
single speaker.",conference,
CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models,"[arxiv.Result.Author('Zih-Ching Chen'), arxiv.Result.Author('Yu-Shun Sung'), arxiv.Result.Author('Hung-yi Lee')]",2022-12-01 08:50:12+00:00,"Self-supervised learning (SSL) is a powerful technique for learning
representations from unlabeled data. Transformer based models such as HuBERT,
which consist a feature extractor and transformer layers, are leading the field
in the speech domain. SSL models are fine-tuned on a wide range of downstream
tasks, which involves re-training the majority of the model for each task.
Previous studies have introduced applying adapters, which are small lightweight
modules commonly used in Natural Language Processing (NLP) to adapt pre-trained
models to new tasks. However, such efficient tuning techniques only provide
adaptation at the transformer layer, but failed to perform adaptation at the
feature extractor. In this paper, we propose CHAPTER, an efficient tuning
method specifically designed for SSL speech model, by applying CNN adapters at
the feature extractor. Using this method, we can only fine-tune fewer than 5%
of parameters per task compared to fully fine-tuning and achieve better and
more stable performance. We empirically found that adding CNN adapters to the
feature extractor can help the adaptation on emotion and speaker tasks. For
instance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy
of ER is improved by 5%.",Submitted to ICASSP 2023. Under review,
A Novel Speech Feature Fusion Algorithm for Text-Independent Speaker Recognition,"[arxiv.Result.Author('Biao Ma'), arxiv.Result.Author('Chengben Xu'), arxiv.Result.Author('Ye Zhang')]",2022-12-01 07:28:06+00:00,"A novel speech feature fusion algorithm with independent vector analysis
(IVA) and parallel convolutional neural network (PCNN) is proposed for
text-independent speaker recognition. Firstly, some different feature types,
such as the time domain (TD) features and the frequency domain (FD) features,
can be extracted from a speaker's speech, and the TD and the FD features can be
considered as the linear mixtures of independent feature components (IFCs) with
an unknown mixing system. To estimate the IFCs, the TD and the FD features of
the speaker's speech are concatenated to build the TD and the FD feature
matrix, respectively. Then, a feature tensor of the speaker's speech is
obtained by paralleling the TD and the FD feature matrix. To enhance the
dependence on different feature types and remove the redundancies of the same
feature type, the independent vector analysis (IVA) can be used to estimate the
IFC matrices of TD and FD features with the feature tensor. The IFC matrices
are utilized as the input of the PCNN to extract the deep features of the TD
and FD features, respectively. The deep features can be integrated to obtain
the fusion feature of the speaker's speech. Finally, the fusion feature of the
speaker's speech is employed as the input of a deep convolutional neural
network (DCNN) classifier for speaker recognition. The experimental results
show the effectiveness and performances of the proposed speaker recognition
system.",,
Noisy Label Detection for Speaker Recognition,"[arxiv.Result.Author('Ruibin Yuan'), arxiv.Result.Author('Hanzhi Yin'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Yifan He'), arxiv.Result.Author('Yushi Ye'), arxiv.Result.Author('Lei Zhang')]",2022-12-01 03:09:33+00:00,"The success of deep neural networks requires both high annotation quality and
massive data. However, the size and the quality of a dataset are usually a
trade-off in practice, as data collection and cleaning are expensive and
time-consuming. Therefore, automatic noisy label detection (NLD) techniques are
critical to real-world applications, especially those using crowdsourcing
datasets. As this is an under-explored topic in automatic speaker verification
(ASV), we present a simple but effective solution to the task. First, we
compare the effectiveness of various commonly used metric learning loss
functions under different noise settings. Then, we propose two ranking-based
NLD methods, inter-class inconsistency and intra-class inconsistency ranking.
They leverage the inconsistent nature of noisy labels and show high detection
precision even under a high level of noise. Our solution gives rise to both
efficient and effective cleaning of large-scale speaker recognition datasets.",,
Gated Recurrent Neural Networks with Weighted Time-Delay Feedback,"[arxiv.Result.Author('N. Benjamin Erichson'), arxiv.Result.Author('Soon Hoe Lim'), arxiv.Result.Author('Michael W. Mahoney')]",2022-12-01 02:26:34+00:00,"We introduce a novel gated recurrent unit (GRU) with a weighted time-delay
feedback mechanism in order to improve the modeling of long-term dependencies
in sequential data. This model is a discretized version of a continuous-time
formulation of a recurrent unit, where the dynamics are governed by delay
differential equations (DDEs). By considering a suitable time-discretization
scheme, we propose $\tau$-GRU, a discrete-time gated recurrent unit with delay.
We prove the existence and uniqueness of solutions for the continuous-time
model, and we demonstrate that the proposed feedback mechanism can help improve
the modeling of long-term dependencies. Our empirical results show that
$\tau$-GRU can converge faster and generalize better than state-of-the-art
recurrent units and gated recurrent architectures on a range of tasks,
including time-series classification, human activity recognition, and speech
recognition.",,
Topological Data Analysis for Speech Processing,"[arxiv.Result.Author('Eduard Tulchinskii'), arxiv.Result.Author('Kristian Kuznetsov'), arxiv.Result.Author('Laida Kushnareva'), arxiv.Result.Author('Daniil Cherniavskii'), arxiv.Result.Author('Serguei Barannikov'), arxiv.Result.Author('Irina Piontkovskaya'), arxiv.Result.Author('Sergey Nikolenko'), arxiv.Result.Author('Evgeny Burnaev')]",2022-11-30 18:22:37+00:00,"We apply topological data analysis (TDA) to speech classification problems
and to the introspection of a pretrained speech model, HuBERT. To this end, we
introduce a number of topological and algebraic features derived from
Transformer attention maps and embeddings. We show that a simple linear
classifier built on top of such features outperforms a fine-tuned
classification head. In particular, we achieve an improvement of about $9\%$
accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed
feature set reaches a new state of the art performance with accuracy $80.155$.
We also show that topological features are able to reveal functional roles of
speech Transformer heads; e.g., we find the heads capable to distinguish
between pairs of sample sources (natural/synthetic) or voices without any
downstream fine-tuning. Our results demonstrate that TDA is a promising new
approach for speech analysis, especially for tasks that require structural
prediction. Appendices, an introduction to TDA, and other additional materials
are available here - https://topohubert.github.io/speech-topology-webpages/","Submitted to ICASSP 2023 conference, awaiting review",
Assisted RTF-Vector-Based Binaural Direction of Arrival Estimation Exploiting a Calibrated External Microphone Array,"[arxiv.Result.Author('Daniel Fejgin'), arxiv.Result.Author('Simon Doclo')]",2022-11-30 17:51:13+00:00,"Recently, a relative transfer function (RTF)-vector-based method has been
proposed to estimate the direction of arrival (DOA) of a target speaker for a
binaural hearing aid setup, assuming the availability of external microphones.
This method exploits the external microphones to estimate the RTF vector
corresponding to the binaural hearing aid and constructs a one-dimensional
spatial spectrum by comparing the estimated RTF vector against a database of
anechoic prototype RTF vectors for several directions. In this paper we assume
the availability of a calibrated array of external microphones, which is
characterized by a second database of anechoic prototype RTF vectors. We
propose a method, where the external microphones are not only exploited to
estimate the RTF vector corresponding to the binaural hearing aid but also
assist in estimating the DOA of the target speaker. Based on the estimated RTF
vector for all microphones and both prototype databases, a two-dimensional
spatial spectrum is constructed from which the DOA is estimated. Experimental
results for a reverberant environment with diffuse-like noise show that
assisted DOA estimation outperforms DOA estimation where the prototype database
characterizing the array of external microphones is not used.",Submitted to ICASSP 2023,
EURO: ESPnet Unsupervised ASR Open-source Toolkit,"[arxiv.Result.Author('Dongji Gao'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Shun-Po Chuang'), arxiv.Result.Author('Leibny Paola Garcia'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Sanjeev Khudanpur')]",2022-11-30 17:40:54+00:00,"This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),
an end-to-end open-source toolkit for unsupervised automatic speech recognition
(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the
Wav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised
speech representations and adversarial training. In addition to wav2vec2, EURO
extends the functionality and promotes reproducibility for UASR tasks by
integrating S3PRL and k2, resulting in flexible frontends from 27
self-supervised models and various graph-based decoding strategies. EURO is
implemented in ESPnet and follows its unified pipeline to provide UASR recipes
with a complete setup. This improves the pipeline's efficiency and allows EURO
to be easily applied to existing datasets in ESPnet. Extensive experiments on
three mainstream self-supervised models demonstrate the toolkit's effectiveness
and achieve state-of-the-art UASR performance on TIMIT and LibriSpeech
datasets. EURO will be publicly available at https://github.com/espnet/espnet,
aiming to promote this exciting and emerging research area based on UASR
through open-source activity.",,
Preliminary Study on SSCF-derived Polar Coordinate for ASR,"[arxiv.Result.Author('Sotheara Leang'), arxiv.Result.Author('Eric Castelli'), arxiv.Result.Author('Dominique Vaufreydaz'), arxiv.Result.Author('Sethserey Sam')]",2022-11-30 14:57:28+00:00,"The transition angles are defined to describe the vowel-to-vowel transitions
in the acoustic space of the Spectral Subband Centroids, and the findings show
that they are similar among speakers and speaking rates. In this paper, we
propose to investigate the usage of polar coordinates in favor of angles to
describe a speech signal by characterizing its acoustic trajectory and using
them in Automatic Speech Recognition. According to the experimental results
evaluated on the BRAF100 dataset, the polar coordinates achieved significantly
higher accuracy than the angles in the mixed and cross-gender speech
recognitions, demonstrating that these representations are superior at defining
the acoustic trajectory of the speech signal. Furthermore, the accuracy was
significantly improved when they were utilized with their first and
second-order derivatives ($\Delta$, $\Delta$$\Delta$), especially in
cross-female recognition. However, the results showed they were not much more
gender-independent than the conventional Mel-frequency Cepstral Coefficients
(MFCCs).",,"ACET 2022, Dec 2022, Phnom Penh, Cambodia"
A data set providing synthetic and real-world fisheye video sequences,"[arxiv.Result.Author('Andrea Eichenseer'), arxiv.Result.Author('André Kaup')]",2022-11-30 14:23:39+00:00,"In video surveillance as well as automotive applications, so-called fisheye
cameras are often employed to capture a very wide angle of view. As such
cameras depend on projections quite different from the classical perspective
projection, the resulting fisheye image and video data correspondingly exhibits
non-rectilinear image characteristics. Typical image and video processing
algorithms, however, are not designed for these fisheye characteristics. To be
able to develop and evaluate algorithms specifically adapted to fisheye images
and videos, a corresponding test data set is therefore introduced in this
paper. The first of those sequences were generated during the authors' own work
on motion estimation for fish-eye videos and further sequences have gradually
been added to create a more extensive collection. The data set now comprises
synthetically generated fisheye sequences, ranging from simple patterns to more
complex scenes, as well as fisheye video sequences captured with an actual
fisheye camera. For the synthetic sequences, exact information on the lens
employed is available, thus facilitating both verification and evaluation of
any adapted algorithms. For the real-world sequences, we provide calibration
data as well as the settings used during acquisition. The sequences are freely
available via www.lms.lnt.de/fisheyedataset/.",,"IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2016, pp. 1541-1545"
Extreme Audio Time Stretching Using Neural Synthesis,"[arxiv.Result.Author('Leonardo Fierro'), arxiv.Result.Author('Alec Wright'), arxiv.Result.Author('Vesa Välimäki'), arxiv.Result.Author('Matti Hämäläinen')]",2022-11-30 13:47:05+00:00,"A deep neural network solution for time-scale modification (TSM) focused on
large stretching factors is proposed, targeting environmental sounds.
Traditional TSM artifacts such as transient smearing, loss of presence, and
phasiness are heavily accentuated and cause poor audio quality when the TSM
factor is four or larger. The weakness of established TSM methods, often based
on a phase vocoder structure, lies in the poor description and scaling of the
transient and noise components, or nuances, of a sound. Our novel solution
combines a sines-transients-noise decomposition with an independent WaveNet
synthesizer to provide a better description of the noise component and an
improve sound quality for large stretching factors. Results of a subjective
listening test against four other TSM algorithms are reported, showing the
proposed method to be often superior. The proposed method is stereo compatible
and has a wide range of applications related to the slow motion of media
content.","Submitted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2023 on Oct 27, 2022",
How to (virtually) train your sound source localizer,"[arxiv.Result.Author('Prerak Srivastava'), arxiv.Result.Author('Antoine Deleforge'), arxiv.Result.Author('Archontis Politis'), arxiv.Result.Author('Emmanuel Vincent')]",2022-11-30 13:01:11+00:00,"Learning-based methods have become ubiquitous in sound source localization
(SSL). Existing systems rely on simulated training sets for the lack of
sufficiently large, diverse and annotated real datasets. Most room acoustic
simulators used for this purpose rely on the image source method (ISM) because
of its computational efficiency. This paper argues that carefully extending the
ISM to incorporate more realistic surface, source and microphone responses into
training sets can significantly boost the real-world performance of SSL
systems. It is shown that increasing the training-set realism of a
state-of-the-art direction-of-arrival estimator yields consistent improvements
across three different real test sets featuring human speakers in a variety of
rooms and various microphone arrays. An ablation study further reveals that
every added layer of realism contributes positively to these improvements.",Pre-Print,
VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing,"[arxiv.Result.Author('Yihan Wu'), arxiv.Result.Author('Junliang Guo'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Bohan Li'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Arul Menezes'), arxiv.Result.Author('Jiang Bian')]",2022-11-30 12:09:40+00:00,"Video dubbing aims to translate the original speech in a film or television
program into the speech in a target language, which can be achieved with a
cascaded system consisting of speech recognition, machine translation and
speech synthesis. To ensure the translated speech to be well aligned with the
corresponding video, the length/duration of the translated speech should be as
close as possible to that of the original speech, which requires strict length
control. Previous works usually control the number of words or characters
generated by the machine translation model to be similar to the source
sentence, without considering the isochronicity of speech as the speech
duration of words/characters in different languages varies. In this paper, we
propose a machine translation system tailored for the task of video dubbing,
which directly considers the speech duration of each token in translation, to
match the length of source and target speech. Specifically, we control the
speech length of generated sentence by guiding the prediction of each word with
the duration information, including the speech duration of itself as well as
how much duration is left for the remaining words. We design experiments on
four language directions (German -> English, Spanish -> English, Chinese <->
English), and the results show that the proposed method achieves better length
control ability on the generated speech than baseline methods. To make up the
lack of real-world datasets, we also construct a real-world test set collected
from films to provide comprehensive evaluations on the video dubbing task.",AAAI 2023 camera version,
An Overview of Indian Spoken Language Recognition from Machine Learning Perspective,"[arxiv.Result.Author('Spandan Dey'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Goutam Saha')]",2022-11-30 11:03:51+00:00,"Automatic spoken language identification (LID) is a very important research
field in the era of multilingual voice-command-based human-computer interaction
(HCI). A front-end LID module helps to improve the performance of many
speech-based applications in the multilingual scenario. India is a populous
country with diverse cultures and languages. The majority of the Indian
population needs to use their respective native languages for verbal
interaction with machines. Therefore, the development of efficient Indian
spoken language recognition systems is useful for adapting smart technologies
in every section of Indian society. The field of Indian LID has started gaining
momentum in the last two decades, mainly due to the development of several
standard multilingual speech corpora for the Indian languages. Even though
significant research progress has already been made in this field, to the best
of our knowledge, there are not many attempts to analytically review them
collectively. In this work, we have conducted one of the very first attempts to
present a comprehensive review of the Indian spoken language recognition
research field. In-depth analysis has been presented to emphasize the unique
challenges of low-resource and mutual influences for developing LID systems in
the Indian contexts. Several essential aspects of the Indian LID research, such
as the detailed description of the available speech corpora, the major research
contributions, including the earlier attempts based on statistical modeling to
the recent approaches based on different neural network architectures, and the
future research trends are discussed. This review work will help assess the
state of the present Indian LID research by any active researcher or any
research enthusiasts from related fields.","Accepted for publication in ACM Transactions on Asian and
  Low-Resource Language Information Processing","ACM Transactions on Asian and Low-Resource Language Information
  Processing, Volume 21, Issue 6 November 2022, Article No 128"
SNAC: Speaker-normalized affine coupling layer in flow-based architecture for zero-shot multi-speaker text-to-speech,"[arxiv.Result.Author('Byoung Jin Choi'), arxiv.Result.Author('Myeonghun Jeong'), arxiv.Result.Author('Joun Yeop Lee'), arxiv.Result.Author('Nam Soo Kim')]",2022-11-30 10:07:29+00:00,"Zero-shot multi-speaker text-to-speech (ZSM-TTS) models aim to generate a
speech sample with the voice characteristic of an unseen speaker. The main
challenge of ZSM-TTS is to increase the overall speaker similarity for unseen
speakers. One of the most successful speaker conditioning methods for
flow-based multi-speaker text-to-speech (TTS) models is to utilize the
functions which predict the scale and bias parameters of the affine coupling
layers according to the given speaker embedding vector. In this letter, we
improve on the previous speaker conditioning method by introducing a
speaker-normalized affine coupling (SNAC) layer which allows for unseen speaker
speech synthesis in a zero-shot manner leveraging a normalization-based
conditioning technique. The newly designed coupling layer explicitly normalizes
the input by the parameters predicted from a speaker embedding vector while
training, enabling an inverse process of denormalizing for a new speaker
embedding at inference. The proposed conditioning scheme yields the
state-of-the-art performance in terms of the speech quality and speaker
similarity in a ZSM-TTS setting.",Accepted to IEEE Signal Processing Letters,
Camelira: An Arabic Multi-Dialect Morphological Disambiguator,"[arxiv.Result.Author('Ossama Obeid'), arxiv.Result.Author('Go Inoue'), arxiv.Result.Author('Nizar Habash')]",2022-11-30 08:02:11+00:00,"We present Camelira, a web-based Arabic multi-dialect morphological
disambiguation tool that covers four major variants of Arabic: Modern Standard
Arabic, Egyptian, Gulf, and Levantine. Camelira offers a user-friendly web
interface that allows researchers and language learners to explore various
linguistic information, such as part-of-speech, morphological features, and
lemmas. Our system also provides an option to automatically choose an
appropriate dialect-specific disambiguator based on the prediction of a dialect
identification component. Camelira is publicly accessible at
http://camelira.camel-lab.com.",,
Two-branch Multi-scale Deep Neural Network for Generalized Document Recapture Attack Detection,"[arxiv.Result.Author('Jiaxing Li'), arxiv.Result.Author('Chenqi Kong'), arxiv.Result.Author('Shiqi Wang'), arxiv.Result.Author('Haoliang Li')]",2022-11-30 06:57:11+00:00,"The image recapture attack is an effective image manipulation method to erase
certain forensic traces, and when targeting on personal document images, it
poses a great threat to the security of e-commerce and other web applications.
Considering the current learning-based methods suffer from serious overfitting
problem, in this paper, we propose a novel two-branch deep neural network by
mining better generalized recapture artifacts with a designed frequency filter
bank and multi-scale cross-attention fusion module. In the extensive
experiment, we show that our method can achieve better generalization
capability compared with state-of-the-art techniques on different scenarios.","5 pages, 4 figures, 2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing, under review",
A General Deep Learning Speech Enhancement Framework Motivated by Taylor's Theorem,"[arxiv.Result.Author('Andong Li'), arxiv.Result.Author('Guochen Yu'), arxiv.Result.Author('Chengshi Zheng'), arxiv.Result.Author('Wenzhe Liu'), arxiv.Result.Author('Xiaodong Li')]",2022-11-30 06:05:59+00:00,"While deep neural networks greatly facilitate the proliferation of the speech
enhancement field, most of the existing methods are developed following either
heuristic or blind optimization criteria, which severely hampers
interpretability and transparency. Inspired by Taylor's theorem, we propose a
general unfolding framework for both single- and multi-channel speech
enhancement tasks. Concretely, we formulate the complex spectrum recovery into
the spectral magnitude mapping in the neighboring space of the noisy mixture,
in which the sparse prior is introduced for phase modification in advance.
Based on that, the mapping function is decomposed into the superimposition of
the 0th-order and high-order polynomials in Taylor's series, where the former
coarsely removes the interference in the magnitude domain and the latter
progressively complements the remaining spectral detail in the complex spectrum
domain. In addition, we study the relation between adjacent order term and
reveal that each high-order term can be recursively estimated with its
lower-order term, and each high-order term is then proposed to evaluate using a
surrogate function with trainable weights, so that the whole system can be
trained in an end-to-end manner. Extensive experiments are conducted on
WSJ0-SI84, DNS-Challenge, Voicebank+Demand, and spatialized Librispeech
datasets. Quantitative results show that the proposed approach not only yields
competitive performance over existing top-performed approaches, but also enjoys
decent internal transparency and flexibility.","Submitted to TASLP, 13 pages",
Automatic Identification of Motivation for Code-Switching in Speech Transcripts,"[arxiv.Result.Author('Ritu Belani'), arxiv.Result.Author('Jeffrey Flanigan')]",2022-11-30 05:45:05+00:00,"Code-switching, or switching between languages, occurs for many reasons and
has important linguistic, sociological, and cultural implications. Multilingual
speakers code-switch for a variety of purposes, such as expressing emotions,
borrowing terms, making jokes, introducing a new topic, etc. The reason for
code-switching may be quite useful for analysis, but is not readily apparent.
To remedy this situation, we annotate a new dataset of motivations for
code-switching in Spanish-English. We build the first system (to our knowledge)
to automatically identify a wide range of motivations that speakers code-switch
in everyday speech, achieving an accuracy of 75% across all motivations.
Additionally, we show that the system can be adapted to new language pairs,
achieving 66% accuracy on a new language pair (Hindi-English), demonstrating
the cross-lingual applicability of our annotation scheme",,
MSV Challenge 2022: NPU-HC Speaker Verification System for Low-resource Indian Languages,"[arxiv.Result.Author('Yue Li'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Namin Wang'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Lei Xie')]",2022-11-30 02:27:51+00:00,"This report describes the NPU-HC speaker verification system submitted to the
O-COCOSDA Multi-lingual Speaker Verification (MSV) Challenge 2022, which
focuses on developing speaker verification systems for low-resource Asian
languages. We participate in the I-MSV track, which aims to develop speaker
verification systems for various Indian languages. In this challenge, we first
explore different neural network frameworks for low-resource speaker
verification. Then we leverage vanilla fine-tuning and weight transfer
fine-tuning to transfer the out-domain pre-trained models to the in-domain
Indian dataset. Specifically, the weight transfer fine-tuning aims to constrain
the distance of the weights between the pre-trained model and the fine-tuned
model, which takes advantage of the previously acquired discriminative ability
from the large-scale out-domain datasets and avoids catastrophic forgetting and
overfitting at the same time. Finally, score fusion is adopted to further
improve performance. Together with the above contributions, we obtain 0.223%
EER on the public evaluation set, ranking 2nd place on the leaderboard. On the
private evaluation set, the EER of our submitted system is 2.123% and 0.630%
for the constrained and unconstrained sub-tasks of the I-MSV track, leading to
the 1st and 3rd place in the ranking, respectively.","6pages, submitted to the 9th International Workshop on Vietnamese
  Language and Speech Processing",
Better Transcription of UK Supreme Court Hearings,"[arxiv.Result.Author('Hadeel Saadany'), arxiv.Result.Author('Catherine Breslin'), arxiv.Result.Author('Constantin Orăsan'), arxiv.Result.Author('Sophie Walker')]",2022-11-29 17:02:00+00:00,"Transcription of legal proceedings is very important to enable access to
justice. However, speech transcription is an expensive and slow process. In
this paper we describe part of a combined research and industrial project for
building an automated transcription tool designed specifically for the Justice
sector in the UK. We explain the challenges involved in transcribing court room
hearings and the Natural Language Processing (NLP) techniques we employ to
tackle these challenges. We will show that fine-tuning a generic off-the-shelf
pre-trained Automatic Speech Recognition (ASR) system with an in-domain
language model as well as infusing common phrases extracted with a collocation
detection model can improve not only the Word Error Rate (WER) of the
transcribed hearings but avoid critical errors that are specific of the legal
jargon and terminology commonly used in British courts.",,
Analysis of constant-Q filterbank based representations for speech emotion recognition,"[arxiv.Result.Author('Premjeet Singh'), arxiv.Result.Author('Shefali Waldekar'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Goutam Saha')]",2022-11-29 16:45:47+00:00,"This work analyzes the constant-Q filterbank-based time-frequency
representations for speech emotion recognition (SER). Constant-Q filterbank
provides non-linear spectro-temporal representation with higher frequency
resolution at low frequencies. Our investigation reveals how the increased
low-frequency resolution benefits SER. The time-domain comparative analysis
between short-term mel-frequency spectral coefficients (MFSCs) and constant-Q
filterbank-based features, namely constant-Q transform (CQT) and continuous
wavelet transform (CWT), reveals that constant-Q representations provide higher
time-invariance at low-frequencies. This provides increased robustness against
emotion irrelevant temporal variations in pitch, especially for low-arousal
emotions. The corresponding frequency-domain analysis over different emotion
classes shows better resolution of pitch harmonics in constant-Q-based
time-frequency representations than MFSC. These advantages of constant-Q
representations are further consolidated by SER performance in the extensive
evaluation of features over four publicly available databases with six advanced
deep neural network architectures as the back-end classifiers. Our inferences
in this study hint toward the suitability and potentiality of constant-Q
features for SER.","Accepted for publication in Elsevier's Digital Signal Processing
  Journal","Volume 130, October 2022, 103712"
Controllable speech synthesis by learning discrete phoneme-level prosodic representations,"[arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('Myrsini Christidou'), arxiv.Result.Author('Alexandra Vioni'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis'), arxiv.Result.Author('Paris Mastorocostas')]",2022-11-29 15:43:36+00:00,"In this paper, we present a novel method for phoneme-level prosody control of
F0 and duration using intuitive discrete labels. We propose an unsupervised
prosodic clustering process which is used to discretize phoneme-level F0 and
duration features from a multispeaker speech dataset. These features are fed as
an input sequence of prosodic labels to a prosody encoder module which augments
an autoregressive attention-based text-to-speech model. We utilize various
methods in order to improve prosodic control range and coverage, such as
augmentation, F0 normalization, balanced clustering for duration and
speaker-independent clustering. The final model enables fine-grained
phoneme-level prosody control for all speakers contained in the training set,
while maintaining the speaker identity. Instead of relying on reference
utterances for inference, we introduce a prior prosody encoder which learns the
style of each speaker and enables speech synthesis without the requirement of
reference audio. We also fine-tune the multispeaker model to unseen speakers
with limited amounts of data, as a realistic application scenario and show that
the prosody control capabilities are maintained, verifying that the
speaker-independent prosodic clustering is effective. Experimental results show
that the model has high output speech quality and that the proposed method
allows efficient prosody control within each speaker's range despite the
variability that a multispeaker setting introduces.","Final published version available at: Speech Communication. arXiv
  admin note: substantial text overlap with arXiv:2111.10168",
Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation,"[arxiv.Result.Author('Stefan Braun'), arxiv.Result.Author('Erik McDermott'), arxiv.Result.Author('Roger Hsiao')]",2022-11-29 14:57:23+00:00,"The neural transducer is an end-to-end model for automatic speech recognition
(ASR). While the model is well-suited for streaming ASR, the training process
remains challenging. During training, the memory requirements may quickly
exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence
lengths. In this work, we analyze the time and space complexity of a typical
transducer training setup. We propose a memory-efficient training method that
computes the transducer loss and gradients sample by sample. We present
optimizations to increase the efficiency and parallelism of the sample-wise
method. In a set of thorough benchmarks, we show that our sample-wise method
significantly reduces memory usage, and performs at competitive speed when
compared to the default batched computation. As a highlight, we manage to
compute the transducer loss and gradients for a batch size of 1024, and audio
length of 40 seconds, using only 6 GB of memory.","5 pages, 4 figures, 1 table, 1 algorithm",
MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition,"[arxiv.Result.Author('Xiaohuan Zhou'), arxiv.Result.Author('Jiaming Wang'), arxiv.Result.Author('Zeyu Cui'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Zhijie Yan'), arxiv.Result.Author('Jingren Zhou'), arxiv.Result.Author('Chang Zhou')]",2022-11-29 13:16:09+00:00,"In this paper, we propose a novel multi-modal multi-task encoder-decoder
pre-training framework (MMSpeech) for Mandarin automatic speech recognition
(ASR), which employs both unlabeled speech and text data. The main difficulty
in speech-text joint pre-training comes from the significant difference between
speech and text modalities, especially for Mandarin speech and text. Unlike
English and other languages with an alphabetic writing system, Mandarin uses an
ideographic writing system where character and sound are not tightly mapped to
one another. Therefore, we propose to introduce the phoneme modality into
pre-training, which can help capture modality-invariant information between
Mandarin speech and text. Specifically, we employ a multi-task learning
framework including five self-supervised and supervised tasks with speech and
text data. For end-to-end pre-training, we introduce self-supervised
speech-to-pseudo-codes (S2C) and phoneme-to-text (P2T) tasks utilizing
unlabeled speech and text data, where speech-pseudo-codes pairs and
phoneme-text pairs are a supplement to the supervised speech-text pairs. To
train the encoder to learn better speech representation, we introduce
self-supervised masked speech prediction (MSP) and supervised phoneme
prediction (PP) tasks to learn to map speech into phonemes. Besides, we
directly add the downstream supervised speech-to-text (S2T) task into the
pre-training process, which can further improve the pre-training performance
and achieve better recognition results even without fine-tuning. Experiments on
AISHELL-1 show that our proposed method achieves state-of-the-art performance,
with a more than 40% relative improvement compared with other pre-training
methods.",Submitted to ICASSP 2023,
On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems,"[arxiv.Result.Author('Thilo von Neumann'), arxiv.Result.Author('Christoph Boeddeker'), arxiv.Result.Author('Keisuke Kinoshita'), arxiv.Result.Author('Marc Delcroix'), arxiv.Result.Author('Reinhold Haeb-Umbach')]",2022-11-29 11:35:13+00:00,"We present a general framework to compute the word error rate (WER) of ASR
systems that process recordings containing multiple speakers at their input and
that produce multiple output word sequences (MIMO). Such ASR systems are
typically required, e.g., for meeting transcription. We provide an efficient
implementation based on a dynamic programming search in a multi-dimensional
Levenshtein distance tensor under the constraint that a reference utterance
must be matched consistently with one hypothesis output. This also results in
an efficient implementation of the ORC WER which previously suffered from
exponential complexity. We give an overview of commonly used WER definitions
for multi-speaker scenarios and show that they are specializations of the above
MIMO WER tuned to particular application scenarios. We conclude with a
discussion of the pros and cons of the various WER definitions and a
recommendation when to use which.",Submitted to ICASSP 2023,
Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline,"[arxiv.Result.Author('Paul-Gauthier Noé'), arxiv.Result.Author('Xiaoxiao Miao'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Junichi Yamagishi'), arxiv.Result.Author('Jean-François Bonastre'), arxiv.Result.Author('Driss Matrouf')]",2022-11-29 10:20:31+00:00,"The use of modern vocoders in an analysis/synthesis pipeline allows us to
investigate high-quality voice conversion that can be used for privacy
purposes. Here, we propose to transform the speaker embedding and the pitch in
order to hide the sex of the speaker. ECAPA-TDNN-based speaker representation
fed into a HiFiGAN vocoder is protected using a neural-discriminant analysis
approach, which is consistent with the zero-evidence concept of privacy. This
approach significantly reduces the information in speech related to the
speaker's sex while preserving speech content and some consistency in the
resulting protected voices.","Submitted to ICASSP 2023, not peer-reviewed yet",
Evaluating and reducing the distance between synthetic and real speech distributions,"[arxiv.Result.Author('Christoph Minixhofer'), arxiv.Result.Author('Ondřej Klejch'), arxiv.Result.Author('Peter Bell')]",2022-11-29 09:50:24+00:00,"While modern Text-to-Speech (TTS) systems can produce speech rated highly in
terms of subjective evaluation, the distance between real and synthetic speech
distributions remains understudied, where we use the term \textit{distribution}
to mean the sample space of all possible real speech recordings from a given
set of speakers; or of the synthetic samples that could be generated for the
same set of speakers. We evaluate the distance of real and synthetic speech
distributions along the dimensions of the acoustic environment, speaker
characteristics and prosody using a range of speech processing measures and the
respective Wasserstein distances of their distributions. We reduce these
distribution distances along said dimensions by providing utterance-level
information derived from the measures to the model and show they can be
generated at inference time. The improvements to the dimensions translate to
overall distribution distance reduction approximated using Automatic Speech
Recognition (ASR) by evaluating the fitness of the synthetic data as training
data.",Submitted to ICASSP 2023,
Model Extraction Attack against Self-supervised Speech Models,"[arxiv.Result.Author('Tsu-Yuan Hsu'), arxiv.Result.Author('Chen-An Li'), arxiv.Result.Author('Tung-Yu Wu'), arxiv.Result.Author('Hung-yi Lee')]",2022-11-29 09:28:05+00:00,"Self-supervised learning (SSL) speech models generate meaningful
representations of given clips and achieve incredible performance across
various downstream tasks. Model extraction attack (MEA) often refers to an
adversary stealing the functionality of the victim model with only query
access. In this work, we study the MEA problem against SSL speech model with a
small number of queries. We propose a two-stage framework to extract the model.
In the first stage, SSL is conducted on the large-scale unlabeled corpus to
pre-train a small speech model. Secondly, we actively sample a small portion of
clips from the unlabeled corpus and query the target model with these clips to
acquire their representations as labels for the small model's second-stage
training. Experiment results show that our sampling methods can effectively
extract the target model without knowing any information about its model
architecture.",Submitted to ICASSP 2023,
jaCappella Corpus: A Japanese a Cappella Vocal Ensemble Corpus,"[arxiv.Result.Author('Tomohiko Nakamura'), arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Naoko Tanji'), arxiv.Result.Author('Satoru Fukayama'), arxiv.Result.Author('Hiroshi Saruwatari')]",2022-11-29 08:52:29+00:00,"We construct a corpus of Japanese a cappella vocal ensembles (jaCappella
corpus) for vocal ensemble separation and synthesis. It consists of 35
copyright-cleared vocal ensemble songs and their audio recordings of individual
voice parts. These songs were arranged from out-of-copyright Japanese
children's songs and have six voice parts (lead vocal, soprano, alto, tenor,
bass, and vocal percussion). They are divided into seven subsets, each of which
features typical characteristics of a music genre such as jazz and enka. The
variety in genre and voice part match vocal ensembles recently widespread in
social media services such as YouTube, although the main targets of
conventional vocal ensemble datasets are choral singing made up of soprano,
alto, tenor, and bass. Experimental evaluation demonstrates that our corpus is
a challenging resource for vocal ensemble separation. Our corpus is available
on our project page (https://tomohikonakamura.github.io/jaCappella_corpus/).",Submitted to ICASSP2023,
Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses,"[arxiv.Result.Author('Yang Ai'), arxiv.Result.Author('Zhen-Hua Ling')]",2022-11-29 07:16:24+00:00,"This paper presents a novel speech phase prediction model which predicts
wrapped phase spectra directly from amplitude spectra by neural networks. The
proposed model is a cascade of a residual convolutional network and a parallel
estimation architecture. The parallel estimation architecture is composed of
two parallel linear convolutional layers and a phase calculation formula,
imitating the process of calculating the phase spectra from the real and
imaginary parts of complex spectra and strictly restricting the predicted phase
values to the principal value interval. To avoid the error expansion issue
caused by phase wrapping, we design anti-wrapping training losses defined
between the predicted wrapped phase spectra and natural ones by activating the
instantaneous phase error, group delay error and instantaneous angular
frequency error using an anti-wrapping function. Experimental results show that
our proposed neural speech phase prediction model outperforms the iterative
Griffin-Lim algorithm and other neural network-based method, in terms of both
reconstructed speech quality and generation speed.",Submitted to ICASSP 2023,
Neural Vocoder Feature Estimation for Dry Singing Voice Separation,"[arxiv.Result.Author('Jaekwon Im'), arxiv.Result.Author('Soonbeom Choi'), arxiv.Result.Author('Sangeon Yong'), arxiv.Result.Author('Juhan Nam')]",2022-11-29 06:16:05+00:00,"Singing voice separation (SVS) is a task that separates singing voice audio
from its mixture with instrumental audio. Previous SVS studies have mainly
employed the spectrogram masking method which requires a large dimensionality
in predicting the binary masks. In addition, they focused on extracting a vocal
stem that retains the wet sound with the reverberation effect. This result may
hinder the reusability of the isolated singing voice. This paper addresses the
issues by predicting mel-spectrogram of dry singing voices from the mixed audio
as neural vocoder features and synthesizing the singing voice waveforms from
the neural vocoder. We experimented with two separation methods. One is
predicting binary masks in the mel-spectrogram domain and the other is directly
predicting the mel-spectrogram. Furthermore, we add a singing voice detector to
identify the singing voice segments over time more explicitly. We measured the
model performance in terms of audio, dereverberation, separation, and overall
quality. The results show that our proposed model outperforms state-of-the-art
singing voice separation models in both objective and subjective evaluation
except the audio quality.","6 pages, 4 figures","14th Asia Pacific Signal and Information Processing Association
  Annual Summit and Conference (APSIPA), 2022"
OK Computer Analysis: An Audio Corpus Study of Radiohead,[arxiv.Result.Author('Nick Collins')],2022-11-29 00:08:31+00:00,"The application of music information retrieval techniques in popular music
studies has great promise. In the present work, a corpus of Radiohead songs
across their career from 1992 to 2017 are subjected to automated audio
analysis. We examine findings from a number of granularities and perspectives,
including within song and between song examination of both timbral-rhythmic and
harmonic features. Chronological changes include possible career spanning
effects for a band's releases such as slowing tempi and reduced brightness, and
the timbral markers of Radiohead's expanding approach to instrumental resources
most identified with the Kid A and Amnesiac era. We conclude with a discussion
highlighting some challenges for this approach, and the potential for a field
of audio file based career analysis.",,
MuSFA: Improving Music Structural Function Analysis with Partially Labeled Data,"[arxiv.Result.Author('Ju-Chiang Wang'), arxiv.Result.Author('Jordan B. L. Smith'), arxiv.Result.Author('Yun-Ning Hung')]",2022-11-28 21:48:45+00:00,"Music structure analysis (MSA) systems aim to segment a song recording into
non-overlapping sections with useful labels. Previous MSA systems typically
predict abstract labels in a post-processing step and require the full context
of the song. By contrast, we recently proposed a supervised framework, called
""Music Structural Function Analysis"" (MuSFA), that models and predicts
meaningful labels like 'verse' and 'chorus' directly from audio, without
requiring the full context of a song. However, the performance of this system
depends on the amount and quality of training data. In this paper, we propose
to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve
the performance. HLSD contains over 18K excerpts of music sections originally
collected for studying automatic melody harmonization. We treat each excerpt as
a partially labeled song and provide a label mapping, so that HLSD can be used
together with other public datasets, such as SALAMI, RWC, and Isophonics. In
cross-dataset evaluations, we find that including HLSD in training can improve
state-of-the-art boundary detection and section labeling scores by ~3% and ~1%
respectively.","ISMIR2022, LBD paper",
Differentiable Dictionary Search: Integrating Linear Mixing with Deep Non-Linear Modelling for Audio Source Separation,"[arxiv.Result.Author('Lukáš Samuel Marták'), arxiv.Result.Author('Rainer Kelz'), arxiv.Result.Author('Gerhard Widmer')]",2022-11-28 16:37:02+00:00,"This paper describes several improvements to a new method for signal
decomposition that we recently formulated under the name of Differentiable
Dictionary Search (DDS). The fundamental idea of DDS is to exploit a class of
powerful deep invertible density estimators called normalizing flows, to model
the dictionary in a linear decomposition method such as NMF, effectively
creating a bijection between the space of dictionary elements and the
associated probability space, allowing a differentiable search through the
dictionary space, guided by the estimated densities. As the initial formulation
was a proof of concept with some practical limitations, we will present several
steps towards making it scalable, hoping to improve both the computational
complexity of the method and its signal decomposition capabilities. As a
testbed for experimental evaluation, we choose the task of frame-level piano
transcription, where the signal is to be decomposed into sources whose activity
is attributed to individual piano notes. To highlight the impact of improved
non-linear modelling of sources, we compare variants of our method to a linear
overcomplete NMF baseline. Experimental results will show that even in the
absence of additional constraints, our models produce increasingly sparse and
precise decompositions, according to two pertinent evaluation measures.","Published in the Proceedings of the 24th International Congress on
  Acoustics (ICA 2022), Gyeongju, Korea, October 24-28, 2022",
Probabilistic Modelling of Signal Mixtures with Differentiable Dictionaries,"[arxiv.Result.Author('Lukáš Samuel Marták'), arxiv.Result.Author('Rainer Kelz'), arxiv.Result.Author('Gerhard Widmer')]",2022-11-28 15:27:53+00:00,"We introduce a novel way to incorporate prior information into (semi-)
supervised non-negative matrix factorization, which we call differentiable
dictionary search. It enables general, highly flexible and principled modelling
of mixtures where non-linear sources are linearly mixed. We study its behavior
on an audio decomposition task, and conduct an extensive, highly controlled
study of its modelling capabilities.","Published in the Proceedings of the 29th European Signal Processing
  Conference (EUSIPCO 2021), Dublin, Ireland, August 23-27, 2021 (IEEE),
  441-445",
Automated Detection of Dolphin Whistles with Convolutional Networks and Transfer Learning,"[arxiv.Result.Author('Burla Nur Korkmaz'), arxiv.Result.Author('Roee Diamant'), arxiv.Result.Author('Gil Danino'), arxiv.Result.Author('Alberto Testolin')]",2022-11-28 15:06:46+00:00,"Effective conservation of maritime environments and wildlife management of
endangered species require the implementation of efficient, accurate and
scalable solutions for environmental monitoring. Ecoacoustics offers the
advantages of non-invasive, long-duration sampling of environmental sounds and
has the potential to become the reference tool for biodiversity surveying.
However, the analysis and interpretation of acoustic data is a time-consuming
process that often requires a great amount of human supervision. This issue
might be tackled by exploiting modern techniques for automatic audio signal
analysis, which have recently achieved impressive performance thanks to the
advances in deep learning research. In this paper we show that convolutional
neural networks can indeed significantly outperform traditional automatic
methods in a challenging detection task: identification of dolphin whistles
from underwater audio recordings. The proposed system can detect signals even
in the presence of ambient noise, at the same time consistently reducing the
likelihood of producing false positives and false negatives. Our results
further support the adoption of artificial intelligence technology to improve
the automatic monitoring of marine ecosystems.",,
HERDPhobia: A Dataset for Hate Speech against Fulani in Nigeria,"[arxiv.Result.Author('Saminu Mohammad Aliyu'), arxiv.Result.Author('Gregory Maksha Wajiga'), arxiv.Result.Author('Muhammad Murtala'), arxiv.Result.Author('Shamsuddeen Hassan Muhammad'), arxiv.Result.Author('Idris Abdulmumin'), arxiv.Result.Author('Ibrahim Said Ahmad')]",2022-11-28 12:30:11+00:00,"Social media platforms allow users to freely share their opinions about
issues or anything they feel like. However, they also make it easier to spread
hate and abusive content. The Fulani ethnic group has been the victim of this
unfortunate phenomenon. This paper introduces the HERDPhobia - the first
annotated hate speech dataset on Fulani herders in Nigeria - in three
languages: English, Nigerian-Pidgin, and Hausa. We present a benchmark
experiment using pre-trained languages models to classify the tweets as either
hateful or non-hateful. Our experiment shows that the XML-T model provides
better performance with 99.83% weighted F1. We released the dataset at
https://github.com/hausanlp/HERDPhobia for further research.","To appear in the Proceedings of the Sixth Workshop on Widening
  Natural Language Processing at EMNLP2022",
Learnable Front Ends Based on Temporal Modulation for Music Tagging,"[arxiv.Result.Author('Yinghao Ma'), arxiv.Result.Author('Richard M. Stern')]",2022-11-28 12:17:14+00:00,"While end-to-end systems are becoming popular in auditory signal processing
including automatic music tagging, models using raw audio as input needs a
large amount of data and computational resources without domain knowledge.
Inspired by the fact that temporal modulation is regarded as an essential
component in auditory perception, we introduce the Temporal Modulation Neural
Network (TMNN) that combines Mel-like data-driven front ends and temporal
modulation filters with a simple ResNet back end. The structure includes a set
of temporal modulation filters to capture long-term patterns in all frequency
channels. Experimental results show that the proposed front ends surpass
state-of-the-art (SOTA) methods on the MagnaTagATune dataset in automatic music
tagging, and they are also helpful for keyword spotting on speech commands.
Moreover, the model performance for each tag suggests that genre or instrument
tags with complex rhythm and mood tags can especially be improved with temporal
modulation.",Submitted to ICASSP 2023,
Automatic Transcription of Drum Strokes in Carnatic Music,"[arxiv.Result.Author('Kausthubh Chandramouli'), arxiv.Result.Author('William Sethares')]",2022-11-28 09:50:18+00:00,"The mridangam is a double-headed percussion instrument that plays a key role
in Carnatic music concerts. This paper presents a novel automatic transcription
algorithm to classify the strokes played on the mridangam. Onset detection is
first performed to segment the audio signal into individual strokes, and
feature vectors consisting of the DFT magnitude spectrum of the segmented
signal are generated. A multi-layer feedforward neural network is trained using
the feature vectors as inputs and the manual transcriptions as targets. Since
the mridangam is a tonal instrument tuned to a given tonic, tonic invariance is
an important feature of the classifier. Tonic invariance is achieved by
augmenting the dataset with pitch-shifted copies of the audio. This algorithm
consistently yields over 83% accuracy on a held-out test dataset.","7 pages, 9 figures",
Handling and extracting key entities from customer conversations using Speech recognition and Named Entity recognition,"[arxiv.Result.Author('Sharvi Endait'), arxiv.Result.Author('Ruturaj Ghatage'), arxiv.Result.Author('Prof. DD Kadam')]",2022-11-28 06:41:29+00:00,"In this modern era of technology with e-commerce developing at a rapid pace,
it is very important to understand customer requirements and details from a
business conversation. It is very crucial for customer retention and
satisfaction. Extracting key insights from these conversations is very
important when it comes to developing their product or solving their issue.
Understanding customer feedback, responses, and important details of the
product are essential and it would be done using Named entity recognition
(NER). For extracting the entities we would be converting the conversations to
text using the optimal speech-to-text model. The model would be a two-stage
network in which the conversation is converted to text. Then, suitable entities
are extracted using robust techniques using a NER BERT transformer model. This
will aid in the enrichment of customer experience when there is an issue which
is faced by them. If a customer faces a problem he will call and register his
complaint. The model will then extract the key features from this conversation
which will be necessary to look into the problem. These features would include
details like the order number, and the exact problem. All these would be
extracted directly from the conversation and this would reduce the effort of
going through the conversation again.",,
Inter-KD: Intermediate Knowledge Distillation for CTC-Based Automatic Speech Recognition,"[arxiv.Result.Author('Ji Won Yoon'), arxiv.Result.Author('Beom Jun Woo'), arxiv.Result.Author('Sunghwan Ahn'), arxiv.Result.Author('Hyeonseung Lee'), arxiv.Result.Author('Nam Soo Kim')]",2022-11-28 05:23:59+00:00,"Recently, the advance in deep learning has brought a considerable improvement
in the end-to-end speech recognition field, simplifying the traditional
pipeline while producing promising results. Among the end-to-end models, the
connectionist temporal classification (CTC)-based model has attracted research
interest due to its non-autoregressive nature. However, such CTC models require
a heavy computational cost to achieve outstanding performance. To mitigate the
computational burden, we propose a simple yet effective knowledge distillation
(KD) for the CTC framework, namely Inter-KD, that additionally transfers the
teacher's knowledge to the intermediate CTC layers of the student network. From
the experimental results on the LibriSpeech, we verify that the Inter-KD shows
better achievements compared to the conventional KD methods. Without using any
language model (LM) and data augmentation, Inter-KD improves the word error
rate (WER) performance from 8.85 % to 6.30 % on the test-clean.",Accepted by 2022 SLT Workshop,
Mix and Localize: Localizing Sound Sources in Mixtures,"[arxiv.Result.Author('Xixi Hu'), arxiv.Result.Author('Ziyang Chen'), arxiv.Result.Author('Andrew Owens')]",2022-11-28 04:30:50+00:00,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize",CVPR 2022,
A novel multimodal dynamic fusion network for disfluency detection in spoken utterances,"[arxiv.Result.Author('Sreyan Ghosh'), arxiv.Result.Author('Utkarsh Tyagi'), arxiv.Result.Author('Sonal Kumar'), arxiv.Result.Author('Manan Suri'), arxiv.Result.Author('Rajiv Ratn Shah')]",2022-11-27 01:54:22+00:00,"Disfluency, though originating from human spoken utterances, is primarily
studied as a uni-modal text-based Natural Language Processing (NLP) task. Based
on early-fusion and self-attention-based multimodal interaction between text
and acoustic modalities, in this paper, we propose a novel multimodal
architecture for disfluency detection from individual utterances. Our
architecture leverages a multimodal dynamic fusion network that adds minimal
parameters over an existing text encoder commonly used in prior art to leverage
the prosodic and acoustic cues hidden in speech. Through experiments, we show
that our proposed model achieves state-of-the-art results on the widely used
English Switchboard for disfluency detection and outperforms prior unimodal and
multimodal systems in literature by a significant margin. In addition, we make
a thorough qualitative analysis and show that, unlike text-only systems, which
suffer from spurious correlations in the data, our system overcomes this
problem through additional cues from speech signals. We make all our codes
publicly available on GitHub.","Submitted to ICASSP 2023. arXiv admin note: text overlap with
  arXiv:2203.16794",
Searching for Discriminative Words in Multidimensional Continuous Feature Space,"[arxiv.Result.Author('Marius Sajgalik'), arxiv.Result.Author('Michal Barla'), arxiv.Result.Author('Maria Bielikova')]",2022-11-26 18:05:11+00:00,"Word feature vectors have been proven to improve many NLP tasks. With recent
advances in unsupervised learning of these feature vectors, it became possible
to train it with much more data, which also resulted in better quality of
learned features. Since it learns joint probability of latent features of
words, it has the advantage that we can train it without any prior knowledge
about the goal task we want to solve. We aim to evaluate the universal
applicability property of feature vectors, which has been already proven to
hold for many standard NLP tasks like part-of-speech tagging or syntactic
parsing. In our case, we want to understand the topical focus of text documents
and design an efficient representation suitable for discriminating different
topics. The discriminativeness can be evaluated adequately on text
categorisation task. We propose a novel method to extract discriminative
keywords from documents. We utilise word feature vectors to understand the
relations between words better and also understand the latent topics which are
discussed in the text and not mentioned directly but inferred logically. We
also present a simple way to calculate document feature vectors out of
extracted discriminative words. We evaluate our method on the four most popular
datasets for text categorisation. We show how different discriminative metrics
influence the overall results. We demonstrate the effectiveness of our approach
by achieving state-of-the-art results on text categorisation task using just a
small number of extracted keywords. We prove that word feature vectors can
substantially improve the topical inference of documents' meaning. We conclude
that distributed representation of words can be used to build higher levels of
abstraction as we demonstrate and build feature vectors of documents.",,"Computer Speech & Language, Volume 53, 2019, Pages 276-301"
Where to Pay Attention in Sparse Training for Feature Selection?,"[arxiv.Result.Author('Ghada Sokar'), arxiv.Result.Author('Zahra Atashgahi'), arxiv.Result.Author('Mykola Pechenizkiy'), arxiv.Result.Author('Decebal Constantin Mocanu')]",2022-11-26 17:49:32+00:00,"A new line of research for feature selection based on neural networks has
recently emerged. Despite its superiority to classical methods, it requires
many training iterations to converge and detect informative features. The
computational time becomes prohibitively long for datasets with a large number
of samples or a very high dimensional feature space. In this paper, we present
a new efficient unsupervised method for feature selection based on sparse
autoencoders. In particular, we propose a new sparse training algorithm that
optimizes a model's sparse topology during training to pay attention to
informative features quickly. The attention-based adaptation of the sparse
topology enables fast detection of informative features after a few training
iterations. We performed extensive experiments on 10 datasets of different
types, including image, speech, text, artificial, and biological. They cover a
wide range of characteristics, such as low and high-dimensional feature spaces,
and few and large training samples. Our proposed approach outperforms the
state-of-the-art methods in terms of selecting informative features while
reducing training iterations and computational costs substantially. Moreover,
the experiments show the robustness of our method in extremely noisy
environments.",Accepted at Neural Information Processing Systems (NeurIPS) 2022,
Toward Universal Text-to-Music Retrieval,"[arxiv.Result.Author('SeungHeon Doh'), arxiv.Result.Author('Minz Won'), arxiv.Result.Author('Keunwoo Choi'), arxiv.Result.Author('Juhan Nam')]",2022-11-26 13:07:26+00:00,"This paper introduces effective design choices for text-to-music retrieval
systems. An ideal text-based retrieval system would support various input
queries such as pre-defined tags, unseen tags, and sentence-level descriptions.
In reality, most previous works mainly focused on a single query type (tag or
sentence) which may not generalize to another input type. Hence, we review
recent text-based music retrieval systems using our proposed benchmark in two
main aspects: input text representation and training objectives. Our findings
enable a universal text-to-music retrieval system that achieves comparable
retrieval performances in both tag- and sentence-level inputs. Furthermore, the
proposed multimodal representation generalizes to 9 different downstream music
classification tasks. We present the code and demo online.",,
Contextual Expressive Text-to-Speech,"[arxiv.Result.Author('Jianhong Tu'), arxiv.Result.Author('Zeyu Cui'), arxiv.Result.Author('Xiaohuan Zhou'), arxiv.Result.Author('Siqi Zheng'), arxiv.Result.Author('Kai Hu'), arxiv.Result.Author('Ju Fan'), arxiv.Result.Author('Chang Zhou')]",2022-11-26 12:06:21+00:00,"The goal of expressive Text-to-speech (TTS) is to synthesize natural speech
with desired content, prosody, emotion, or timbre, in high expressiveness. Most
of previous studies attempt to generate speech from given labels of styles and
emotions, which over-simplifies the problem by classifying styles and emotions
into a fixed number of pre-defined categories. In this paper, we introduce a
new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a
person speaks depends on the particular context she is in, where the context
can typically be represented as text. Thus, in the CTTS task, we propose to
utilize such context to guide the speech synthesis process instead of relying
on explicit labels of styles and emotions. To achieve this task, we construct a
synthetic dataset and develop an effective framework. Experiments show that our
framework can generate high-quality expressive speech based on the given
context both in synthetic datasets and real-world scenarios.",Submitted to ICASSP 2023,
Non-Polar Opposites: Analyzing the Relationship Between Echo Chambers and Hostile Intergroup Interactions on Reddit,"[arxiv.Result.Author('Alexandros Efstratiou'), arxiv.Result.Author('Jeremy Blackburn'), arxiv.Result.Author('Tristan Caulfield'), arxiv.Result.Author('Gianluca Stringhini'), arxiv.Result.Author('Savvas Zannettou'), arxiv.Result.Author('Emiliano De Cristofaro')]",2022-11-25 22:17:07+00:00,"Previous research has documented the existence of both online echo chambers
and hostile intergroup interactions. In this paper, we explore the relationship
between these two phenomena by studying the activity of 5.97M Reddit users and
421M comments posted over 13 years. We examine whether users who are more
engaged in echo chambers are more hostile when they comment on other
communities. We then create a typology of relationships between political
communities based on whether their users are toxic to each other, whether echo
chamber-like engagement with these communities is associated with polarization,
and on the communities' political leanings. We observe both the echo chamber
and hostile intergroup interaction phenomena, but neither holds universally
across communities. Contrary to popular belief, we find that polarizing and
toxic speech is more dominant between communities on the same, rather than
opposing, sides of the political spectrum, especially on the left; however,
this mainly points to the collective targeting of political outgroups.",,"17th International AAAI Conference on Web and Social Media (ICWSM
  2023). Please cite accordingly"
Stereo Speech Enhancement Using Custom Mid-Side Signals and Monaural Processing,"[arxiv.Result.Author('Aaron Master'), arxiv.Result.Author('Lie Lu'), arxiv.Result.Author('Nathan Swedlow')]",2022-11-25 21:28:02+00:00,"Speech Enhancement (SE) systems typically operate on monaural input and are
used for applications including voice communications and capture cleanup for
user generated content. Recent advancements and changes in the devices used for
these applications are likely to lead to an increase in the amount of
two-channel content for the same applications. However, SE systems are
typically designed for monaural input; stereo results produced using trivial
methods such as channel independent or mid-side processing may be
unsatisfactory, including substantial speech distortions. To address this, we
propose a system which creates a novel representation of stereo signals called
Custom Mid-Side Signals (CMSS). CMSS allow benefits of mid-side signals for
center-panned speech to be extended to a much larger class of input signals.
This in turn allows any existing monaural SE system to operate as an efficient
stereo system by processing the custom mid signal. We describe how the
parameters needed for CMSS can be efficiently estimated by a component of the
spatio-level filtering source separation system. Subjective listening using
state-of-the-art deep learning-based SE systems on stereo content with various
speech mixing styles shows that CMSS processing leads to improved speech
quality at approximately half the cost of channel-independent processing.","12 pages, 5 figures. Submitted to the Journal of the Audio
  Engineering Society",
Interpretability Analysis of Deep Models for COVID-19 Detection,"[arxiv.Result.Author('Daniel Peixoto Pinto da Silva'), arxiv.Result.Author('Edresson Casanova'), arxiv.Result.Author('Lucas Rafael Stefanel Gris'), arxiv.Result.Author('Arnaldo Candido Junior'), arxiv.Result.Author('Marcelo Finger'), arxiv.Result.Author('Flaviane Svartman'), arxiv.Result.Author('Beatriz Raposo'), arxiv.Result.Author('Marcus Vinícius Moreira Martins'), arxiv.Result.Author('Sandra Maria Aluísio'), arxiv.Result.Author('Larissa Cristina Berti'), arxiv.Result.Author('João Paulo Teixeira')]",2022-11-25 20:56:23+00:00,"During the outbreak of COVID-19 pandemic, several research areas joined
efforts to mitigate the damages caused by SARS-CoV-2. In this paper we present
an interpretability analysis of a convolutional neural network based model for
COVID-19 detection in audios. We investigate which features are important for
model decision process, investigating spectrograms, F0, F0 standard deviation,
sex and age. Following, we analyse model decisions by generating heat maps for
the trained models to capture their attention during the decision process.
Focusing on a explainable Inteligence Artificial approach, we show that studied
models can taken unbiased decisions even in the presence of spurious data in
the training set, given the adequate preprocessing steps. Our best model has
94.44% of accuracy in detection, with results indicating that models favors
spectrograms for the decision process, particularly, high energy areas in the
spectrogram related to prosodic domains, while F0 also leads to efficient
COVID-19 detection.","14 pages, 4 figures",
Puffin: pitch-synchronous neural waveform generation for fullband speech on modest devices,"[arxiv.Result.Author('Oliver Watts'), arxiv.Result.Author('Lovisa Wihlborg'), arxiv.Result.Author('Cassia Valentini-Botinhao')]",2022-11-25 14:15:21+00:00,"We present a neural vocoder designed with low-powered Alternative and
Augmentative Communication devices in mind. By combining elements of successful
modern vocoders with established ideas from an older generation of technology,
our system is able to produce high quality synthetic speech at 48kHz on devices
where neural vocoders are otherwise prohibitively complex. The system is
trained adversarially using differentiable pitch synchronous overlap add, and
reduces complexity by relying on pitch synchronous Inverse Short-Time Fourier
Transform (ISTFT) to generate speech samples. Our system achieves comparable
quality with a strong (HiFi-GAN) baseline while using only a fraction of the
compute. We present results of a perceptual evaluation as well as an analysis
of system complexity.",ICASSP 2023 submission,
Learning General Audio Representations with Large-Scale Training of Patchout Audio Transformers,"[arxiv.Result.Author('Khaled Koutini'), arxiv.Result.Author('Shahed Masoudian'), arxiv.Result.Author('Florian Schmid'), arxiv.Result.Author('Hamid Eghbal-zadeh'), arxiv.Result.Author('Jan Schlüter'), arxiv.Result.Author('Gerhard Widmer')]",2022-11-25 08:39:12+00:00,"The success of supervised deep learning methods is largely due to their
ability to learn relevant features from raw data. Deep Neural Networks (DNNs)
trained on large-scale datasets are capable of capturing a diverse set of
features, and learning a representation that can generalize onto unseen tasks
and datasets that are from the same domain. Hence, these models can be used as
powerful feature extractors, in combination with shallower models as
classifiers, for smaller tasks and datasets where the amount of training data
is insufficient for learning an end-to-end model from scratch. During the past
years, Convolutional Neural Networks (CNNs) have largely been the method of
choice for audio processing. However, recently attention-based transformer
models have demonstrated great potential in supervised settings, outperforming
CNNs. In this work, we investigate the use of audio transformers trained on
large-scale datasets to learn general-purpose representations. We study how the
different setups in these audio transformers affect the quality of their
embeddings. We experiment with the models' time resolution, extracted embedding
level, and receptive fields in order to see how they affect performance on a
variety of tasks and datasets, following the HEAR 2021 NeurIPS challenge
evaluation setup. Our results show that representations extracted by audio
transformers outperform CNN representations. Furthermore, we will show that
transformers trained on Audioset can be extremely effective representation
extractors for a wide range of downstream tasks.","will apear in HEAR: Holistic Evaluation of Audio Representations
  Proceedings of Machine Learning Research PMLR 166. Source code:
  https://github.com/kkoutini/passt_hear21",
Efficient Incremental Text-to-Speech on GPUs,"[arxiv.Result.Author('Muyang Du'), arxiv.Result.Author('Chuan Liu'), arxiv.Result.Author('Jiaxing Qi'), arxiv.Result.Author('Junjie Lai')]",2022-11-25 07:43:45+00:00,"Incremental text-to-speech, also known as streaming TTS, has been
increasingly applied to online speech applications that require ultra-low
response latency to provide an optimal user experience. However, most of the
existing speech synthesis pipelines deployed on GPU are still non-incremental,
which uncovers limitations in high-concurrency scenarios, especially when the
pipeline is built with end-to-end neural network models. To address this issue,
we present a highly efficient approach to perform real-time incremental TTS on
GPUs with Instant Request Pooling and Module-wise Dynamic Batching.
Experimental results demonstrate that the proposed method is capable of
producing high-quality speech with a first-chunk latency lower than 80ms under
100 QPS on a single NVIDIA A10 GPU and significantly outperforms the
non-incremental twin in both concurrency and latency. Our work reveals the
effectiveness of high-performance incremental TTS on GPUs.","5 pages, 4 figures",
Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?,"[arxiv.Result.Author('Xuan Shi'), arxiv.Result.Author('Erica Cooper'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Junichi Yamagishi'), arxiv.Result.Author('Shrikanth Narayanan')]",2022-11-25 02:54:33+00:00,"With the similarity between music and speech synthesis from symbolic input
and the rapid development of text-to-speech (TTS) techniques, it is worthwhile
to explore ways to improve the MIDI-to-audio performance by borrowing from TTS
techniques. In this study, we analyze the shortcomings of a TTS-based
MIDI-to-audio system and improve it in terms of feature computation, model
selection, and training strategy, aiming to synthesize highly natural-sounding
audio. Moreover, we conducted an extensive model evaluation through listening
tests, pitch measurement, and spectrogram analysis. This work demonstrates not
only synthesis of highly natural music but offers a thorough analytical
approach and useful outcomes for the community. Our code and pre-trained models
are open sourced at https://github.com/nii-yamagishilab/midi-to-audio.",Submitted to ICASSP 2023,
Improving Multi-task Learning via Seeking Task-based Flat Regions,"[arxiv.Result.Author('Hoang Phan'), arxiv.Result.Author('Lam Tran'), arxiv.Result.Author('Ngoc N. Tran'), arxiv.Result.Author('Nhat Ho'), arxiv.Result.Author('Dinh Phung'), arxiv.Result.Author('Trung Le')]",2022-11-24 17:19:30+00:00,"Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for
training deep neural networks that allows learning more than one objective by a
single backbone. Compared to training tasks separately, MTL significantly
reduces computational costs, improves data efficiency, and potentially enhances
model performance by leveraging knowledge across tasks. Hence, it has been
adopted in a variety of applications, ranging from computer vision to natural
language processing and speech recognition. Among them, there is an emerging
line of work in MTL that focuses on manipulating the task gradient to derive an
ultimate gradient descent direction to benefit all tasks. Despite achieving
impressive results on many benchmarks, directly applying these approaches
without using appropriate regularization techniques might lead to suboptimal
solutions on real-world problems. In particular, standard training that
minimizes the empirical loss on the training data can easily suffer from
overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which
can cause negative transfer between tasks and overall performance drop. To
alleviate such problems, we propose to leverage a recently introduced training
method, named Sharpness-aware Minimization, which can enhance model
generalization ability on single-task learning. Accordingly, we present a novel
MTL training methodology, encouraging the model to find task-based flat minima
for coherently improving its generalization capability on all tasks. Finally,
we conduct comprehensive experiments on a variety of applications to
demonstrate the merit of our proposed approach to existing gradient-based MTL
methods, as suggested by our developed theory.","29 pages, 11 figures, 6 tables",
Bidirectional Representations for Low Resource Spoken Language Understanding,"[arxiv.Result.Author('Quentin Meeus'), arxiv.Result.Author('Marie-Francine Moens'), arxiv.Result.Author('Hugo Van hamme')]",2022-11-24 17:05:16+00:00,"Most spoken language understanding systems use a pipeline approach composed
of an automatic speech recognition interface and a natural language
understanding module. This approach forces hard decisions when converting
continuous inputs into discrete language symbols. Instead, we propose a
representation model to encode speech in rich bidirectional encodings that can
be used for downstream tasks such as intent prediction. The approach uses a
masked language modelling objective to learn the representations, and thus
benefits from both the left and right contexts. We show that the performance of
the resulting encodings before fine-tuning is better than comparable models on
multiple datasets, and that fine-tuning the top layers of the representation
model improves the current state of the art on the Fluent Speech Command
dataset, also in a low-data regime, when a limited amount of labelled data is
used for training. Furthermore, we propose class attention as a spoken language
understanding module, efficient both in terms of speed and number of
parameters. Class attention can be used to visually explain the predictions of
our model, which goes a long way in understanding how the model makes
predictions. We perform experiments in English and in Dutch.",,
Multitask Learning for Low Resource Spoken Language Understanding,"[arxiv.Result.Author('Quentin Meeus'), arxiv.Result.Author('Marie-Francine Moens'), arxiv.Result.Author('Hugo Van hamme')]",2022-11-24 16:38:17+00:00,"We explore the benefits that multitask learning offer to speech processing as
we train models on dual objectives with automatic speech recognition and intent
classification or sentiment classification. Our models, although being of
modest size, show improvements over models trained end-to-end on intent
classification. We compare different settings to find the optimal disposition
of each task module compared to one another. Finally, we study the performance
of the models in low-resource scenario by training the models with as few as
one example per class. We show that multitask learning in these scenarios
compete with a baseline model trained on text features and performs
considerably better than a pipeline model. On sentiment classification, we
match the performance of an end-to-end model with ten times as many parameters.
We consider 4 tasks and 4 datasets in Dutch and English.",,
"How ""open"" are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation","[arxiv.Result.Author('A. Seza Doğruöz'), arxiv.Result.Author('Gabriel Skantze')]",2022-11-24 12:23:20+00:00,"Open-domain chatbots are supposed to converse freely with humans without
being restricted to a topic, task or domain. However, the boundaries and/or
contents of open-domain conversations are not clear. To clarify the boundaries
of ""openness"", we conduct two studies: First, we classify the types of ""speech
events"" encountered in a chatbot evaluation data set (i.e., Meena by Google)
and find that these conversations mainly cover the ""small talk"" category and
exclude the other speech event categories encountered in real life human-human
communication. Second, we conduct a small-scale pilot study to generate online
conversations covering a wider range of speech event categories between two
humans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook).
A human evaluation of these generated conversations indicates a preference for
human-human conversations, since the human-chatbot conversations lack coherence
in most speech event categories. Based on these results, we suggest (a) using
the term ""small talk"" instead of ""open-domain"" for the current chatbots which
are not that ""open"" in terms of conversational abilities yet, and (b) revising
the evaluation methods to test the chatbot conversations against other speech
events.",,"In Proceedings of the 22nd Annual Meeting of the Special Interest
  Group on Discourse and Dialogue (SIGDIAL 2021), pages 392-402, Singapore"
Prosody-controllable spontaneous TTS with neural HMMs,"[arxiv.Result.Author('Harm Lameris'), arxiv.Result.Author('Shivam Mehta'), arxiv.Result.Author('Gustav Eje Henter'), arxiv.Result.Author('Joakim Gustafson'), arxiv.Result.Author('Éva Székely')]",2022-11-24 11:06:11+00:00,"Spontaneous speech has many affective and pragmatic functions that are
interesting and challenging to model in TTS (text-to-speech). However, the
presence of reduced articulation, fillers, repetitions, and other disfluencies
mean that text and acoustics are less well aligned than in read speech. This is
problematic for attention-based TTS. We propose a TTS architecture that is
particularly suited for rapidly learning to speak from irregular and small
datasets while also reproducing the diversity of expressive phenomena present
in spontaneous speech. Specifically, we modify an existing neural HMM-based TTS
system, which is capable of stable, monotonic alignments for spontaneous
speech, and add utterance-level prosody control, so that the system can
represent the wide range of natural variability in a spontaneous speech corpus.
We objectively evaluate control accuracy and perform a subjective listening
test to compare to a system without prosody control. To exemplify the power of
combining mid-level prosody control and ecologically valid data for reproducing
intricate spontaneous speech phenomena, we evaluate the system's capability of
synthesizing two types of creaky phonation. Audio samples are available at
https://hfkml.github.io/pc_nhmm_tts/","5 pages, 3 figures, Submitted to ICASSP 2023",
TESSP: Text-Enhanced Self-Supervised Speech Pre-training,"[arxiv.Result.Author('Zhuoyuan Yao'), arxiv.Result.Author('Shuo Ren'), arxiv.Result.Author('Sanyuan Chen'), arxiv.Result.Author('Ziyang Ma'), arxiv.Result.Author('Pengcheng Guo'), arxiv.Result.Author('Lei Xie')]",2022-11-24 07:08:51+00:00,"Self-supervised speech pre-training empowers the model with the contextual
structure inherent in the speech signal while self-supervised text pre-training
empowers the model with linguistic information. Both of them are beneficial for
downstream speech tasks such as ASR. However, the distinct pre-training
objectives make it challenging to jointly optimize the speech and text
representation in the same model. To solve this problem, we propose
Text-Enhanced Self-Supervised Speech Pre-training (TESSP), aiming to
incorporate the linguistic information into speech pre-training. Our model
consists of three parts, i.e., a speech encoder, a text encoder and a shared
encoder. The model takes unsupervised speech and text data as the input and
leverages the common HuBERT and MLM losses respectively. We also propose
phoneme up-sampling and representation swapping to enable joint modeling of the
speech and text information. Specifically, to fix the length mismatching
problem between speech and text data, we phonemize the text sequence and
up-sample the phonemes with the alignment information extracted from a small
set of supervised data. Moreover, to close the gap between the learned speech
and text representations, we swap the text representation with the speech
representation extracted by the respective private encoders according to the
alignment information. Experiments on the Librispeech dataset shows the
proposed TESSP model achieves more than 10% improvement compared with WavLM on
the test-clean and test-other sets. We also evaluate our model on the SUPERB
benchmark, showing our model has better performance on Phoneme Recognition,
Acoustic Speech Recognition and Speech Translation compared with WavLM.","9 pages, 4 figures",
A new Speech Feature Fusion method with cross gate parallel CNN for Speaker Recognition,"[arxiv.Result.Author('Jiacheng Zhang'), arxiv.Result.Author('Wenyi Yan'), arxiv.Result.Author('Ye Zhang')]",2022-11-24 02:17:43+00:00,"In this paper, a new speech feature fusion method is proposed for speaker
recognition on the basis of the cross gate parallel convolutional neural
network (CG-PCNN). The Mel filter bank features (MFBFs) of different frequency
resolutions can be extracted from each speech frame of a speaker's speech by
several Mel filter banks, where the numbers of the triangular filters in the
Mel filter banks are different. Due to the frequency resolutions of these MFBFs
are different, there are some complementaries for these MFBFs. The CG-PCNN is
utilized to extract the deep features from these MFBFs, which applies a cross
gate mechanism to capture the complementaries for improving the performance of
the speaker recognition system. Then, the fusion feature can be obtained by
concatenating these deep features for speaker recognition. The experimental
results show that the speaker recognition system with the proposed speech
feature fusion method is effective, and marginally outperforms the existing
state-of-the-art systems.",,
Proceedings of the 4th International Workshop on Reading Music Systems,"[arxiv.Result.Author('Jorge Calvo-Zaragoza'), arxiv.Result.Author('Alexander Pacha'), arxiv.Result.Author('Elona Shatri')]",2022-11-23 20:16:45+00:00,"The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 4th International Workshop on Reading Music
Systems, held online on Nov. 18th 2022.","Proceedings edited by Jorge Calvo-Zaragoza, Alexander Pacha and Elona
  Shatri",
Voice-preserving Zero-shot Multiple Accent Conversion,"[arxiv.Result.Author('Mumin Jin'), arxiv.Result.Author('Prashant Serai'), arxiv.Result.Author('Jilong Wu'), arxiv.Result.Author('Andros Tjandra'), arxiv.Result.Author('Vimal Manohar'), arxiv.Result.Author('Qing He')]",2022-11-23 19:51:16+00:00,"Most people who have tried to learn a foreign language would have experienced
difficulties understanding or speaking with a native speaker's accent. For
native speakers, understanding or speaking a new accent is likewise a difficult
task. An accent conversion system that changes a speaker's accent but preserves
that speaker's voice identity, such as timbre and pitch, has the potential for
a range of applications, such as communication, language learning, and
entertainment. Existing accent conversion models tend to change the speaker
identity and accent at the same time. Here, we use adversarial learning to
disentangle accent dependent features while retaining other acoustic
characteristics. What sets our work apart from existing accent conversion
models is the capability to convert an unseen speaker's utterance to multiple
accents while preserving its original voice identity. Subjective evaluations
show that our model generates audio that sound closer to the target accent and
like the original speaker.",Submitted to IEEE ICASSP 2023,
Device Directedness with Contextual Cues for Spoken Dialog Systems,"[arxiv.Result.Author('Dhanush Bekal'), arxiv.Result.Author('Sundararajan Srinivasan'), arxiv.Result.Author('Sravan Bodapati'), arxiv.Result.Author('Srikanth Ronanki'), arxiv.Result.Author('Katrin Kirchhoff')]",2022-11-23 19:49:11+00:00,"In this work, we define barge-in verification as a supervised learning task
where audio-only information is used to classify user spoken dialogue into true
and false barge-ins. Following the success of pre-trained models, we use
low-level speech representations from a self-supervised representation learning
model for our downstream classification task. Further, we propose a novel
technique to infuse lexical information directly into speech representations to
improve the domain-specific language information implicitly learned during
pre-training. Experiments conducted on spoken dialog data show that our
proposed model trained to validate barge-in entirely from speech
representations is faster by 38% relative and achieves 4.5% relative F1 score
improvement over a baseline LSTM model that uses both audio and Automatic
Speech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed
model with lexically infused representations along with contextual features
provides a further relative improvement of 5.7% in the F1 score but only 22%
faster than the baseline.",,
Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction,"[arxiv.Result.Author('Kai Shen'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Siliang Tang'), arxiv.Result.Author('Yuan Zhang'), arxiv.Result.Author('Wenjie Liu'), arxiv.Result.Author('Edward Lin')]",2022-11-23 19:05:48+00:00,"Text error correction aims to correct the errors in text sequences such as
those typed by humans or generated by speech recognition models. Previous error
correction methods usually take the source (incorrect) sentence as encoder
input and generate the target (correct) sentence through the decoder. Since the
error rate of the incorrect sentence is usually low (e.g., 10\%), the
correction model can only learn to correct on limited error tokens but
trivially copy on most tokens (correct tokens), which harms the effective
training of error correction. In this paper, we argue that the correct tokens
should be better utilized to facilitate effective training and then propose a
simple yet effective masking strategy to achieve this goal. Specifically, we
randomly mask out a part of the correct tokens in the source sentence and let
the model learn to not only correct the original error tokens but also predict
the masked tokens based on their context information. Our method enjoys several
advantages: 1) it alleviates trivial copy; 2) it leverages effective training
signals from correct tokens; 3) it is a plug-and-play module and can be applied
to different models and tasks. Experiments on spelling error correction and
speech recognition error correction on Mandarin datasets and grammar error
correction on English datasets with both autoregressive and non-autoregressive
generation models show that our method improves the correction accuracy
consistently.",main track of EMNLP 2022,
ASiT: Audio Spectrogram vIsion Transformer for General Audio Representation,"[arxiv.Result.Author('Sara Atito'), arxiv.Result.Author('Muhammad Awais'), arxiv.Result.Author('Wenwu Wang'), arxiv.Result.Author('Mark D Plumbley'), arxiv.Result.Author('Josef Kittler')]",2022-11-23 18:21:09+00:00,"Vision transformers, which were originally developed for natural language
processing, have recently generated significant interest in the computer vision
and audio communities due to their flexibility in learning long-range
relationships. Constrained by data hungry nature of transformers and limited
labelled data most transformer-based models for audio tasks are finetuned from
ImageNet pretrained models, despite the huge gap between the natural images
domain and audio domain. This has motivated the research in self-supervised
pretraining of audio transformers, which reduces the dependency on large
amounts of labeled data and focuses on extracting concise representation of the
audio spectrograms. In this paper, we propose ASiT, a novel self-supervised
transformer for general audio representations that captures local and global
contextual information employing group masked model learning and
self-distillation. We evaluate our pretrained models on both audio and speech
classification tasks including audio event classification, keyword spotting,
and speaker identification. We further conduct comprehensive ablation studies,
including evaluations of different pretraining strategies. The proposed ASiT
framework significantly boosts the performance on all tasks and sets a new
state-of-the-art performance on five audio and speech classification tasks,
outperforming recent methods, including the approaches that use additional
datasets for pretraining. The code and pretrained weights will be made publicly
available for the scientific community.",,
On the Typicality of Musical Sequences,"[arxiv.Result.Author('Mathias Rose Bjare'), arxiv.Result.Author('Stefan Lattner')]",2022-11-23 16:05:28+00:00,"It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how ""typical
sampling"" influences the distribution of information around the entropy for
single events and sequences.","2 pages, 1 figure, Accepted at the Extended Abstracts for the
  Late-Breaking Demo Session of the 23rd Int. Society for Music Information
  Retrieval Conf., Bengaluru, India, 2022",
Ignorance is Bliss? The Effect of Explanations on Perceptions of Voice Assistants,"[arxiv.Result.Author('William Seymour'), arxiv.Result.Author('Jose Such')]",2022-11-23 12:04:31+00:00,"Voice assistants offer a convenient and hands-free way of accessing computing
in the home, but a key problem with speech as an interaction modality is how to
scaffold accurate mental models of voice assistants, a task complicated by
privacy and security concerns. We present the results of a survey of voice
assistant users (n=1314) measuring trust, security, and privacy perceptions of
voice assistants with varying levels of online functionality explained in
different ways. We then asked participants to re-explain how these voice
assistants worked, showing that while privacy explanations relieved privacy
concerns, trust concerns were exacerbated by trust explanations. Participants'
trust, privacy, and security perceptions also distinguished between first party
online functionality from the voice assistant vendor and third party online
functionality from other developers, and trust in vendors appeared to operate
independently from device explanations. Our findings point to the use of
analogies to guide users, targeting trust and privacy concerns, key
improvements required from manufacturers, and implications for competition in
the sector.","To appear in the Proceedings of the ACM on Human-Computer
  Interaction, CSCW1, April 2023 issue. To be presented at CSCW 2023",
Whose Emotion Matters? Speaker Detection without Prior Knowledge,"[arxiv.Result.Author('Hugo Carneiro'), arxiv.Result.Author('Cornelius Weber'), arxiv.Result.Author('Stefan Wermter')]",2022-11-23 09:57:17+00:00,"The task of emotion recognition in conversations (ERC) benefits from the
availability of multiple modalities, as offered, for example, in the
video-based MELD dataset. However, only a few research approaches use both
acoustic and visual information from the MELD videos. There are two reasons for
this: First, label-to-video alignments in MELD are noisy, making those videos
an unreliable source of emotional speech data. Second, conversations can
involve several people in the same scene, which requires the detection of the
person speaking the utterance. In this paper we demonstrate that by using
recent automatic speech recognition and active speaker detection models, we are
able to realign the videos of MELD, and capture the facial expressions from
uttering speakers in 96.92% of the utterances provided in MELD. Experiments
with a self-supervised voice recognition model indicate that the realigned MELD
videos more closely match the corresponding utterances offered in the dataset.
Finally, we devise a model for emotion recognition in conversations trained on
the face and audio information of the MELD realigned videos, which outperforms
state-of-the-art models for ERC based on vision alone. This indicates that
active speaker detection is indeed effective for extracting facial expressions
from the uttering speakers, and that faces provide more informative visual cues
than the visual features state-of-the-art models have been using so far.","22 pages, 8 figures, 6 tables",
IMaSC -- ICFOSS Malayalam Speech Corpus,"[arxiv.Result.Author('Deepa P Gopinath'), arxiv.Result.Author('Thennal D K'), arxiv.Result.Author('Vrinda V Nair'), arxiv.Result.Author('Swaraj K S'), arxiv.Result.Author('Sachin G')]",2022-11-23 09:21:01+00:00,"Modern text-to-speech (TTS) systems use deep learning to synthesize speech
increasingly approaching human quality, but they require a database of high
quality audio-text sentence pairs for training. Malayalam, the official
language of the Indian state of Kerala and spoken by 35+ million people, is a
low resource language in terms of available corpora for TTS systems. In this
paper, we present IMaSC, a Malayalam text and speech corpora containing
approximately 50 hours of recorded speech. With 8 speakers and a total of
34,473 text-audio pairs, IMaSC is larger than every other publicly available
alternative. We evaluated the database by using it to train TTS models for each
speaker based on a modern deep learning architecture. Via subjective
evaluation, we show that our models perform significantly better in terms of
naturalness compared to previous studies and publicly available models, with an
average mean opinion score of 4.50, indicating that the synthesized speech is
close to human quality.","18 pages, 8 figures",
Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation,"[arxiv.Result.Author('Vinay Kothapally'), arxiv.Result.Author('John H. L. Hansen')]",2022-11-22 23:38:10+00:00,"Several speech processing systems have demonstrated considerable performance
improvements when deep complex neural networks (DCNN) are coupled with
self-attention (SA) networks. However, the majority of DCNN-based studies on
speech dereverberation that employ self-attention do not explicitly account for
the inter-dependencies between real and imaginary features when computing
attention. In this study, we propose a complex-valued T-F attention (TFA)
module that models spectral and temporal dependencies by computing
two-dimensional attention maps across time and frequency dimensions. We
validate the effectiveness of our proposed complex-valued TFA module with the
deep complex convolutional recurrent network (DCCRN) using the REVERB challenge
corpus. Experimental findings indicate that integrating our complex-TFA module
with DCCRN improves overall speech quality and performance of back-end speech
applications, such as automatic speech recognition, compared to earlier
approaches for self-attention.",Interspeech 2022: ISCA Best Student Paper Award Finalist,
SkipConvGAN: Monaural Speech Dereverberation using Generative Adversarial Networks via Complex Time-Frequency Masking,"[arxiv.Result.Author('Vinay Kothapally'), arxiv.Result.Author('J. H. L. Hansen')]",2022-11-22 23:02:49+00:00,"With the advancements in deep learning approaches, the performance of speech
enhancing systems in the presence of background noise have shown significant
improvements. However, improving the system's robustness against reverberation
is still a work in progress, as reverberation tends to cause loss of formant
structure due to smearing effects in time and frequency. A wide range of deep
learning-based systems either enhance the magnitude response and reuse the
distorted phase or enhance complex spectrogram using a complex time-frequency
mask. Though these approaches have demonstrated satisfactory performance, they
do not directly address the lost formant structure caused by reverberation. We
believe that retrieving the formant structure can help improve the efficiency
of existing systems. In this study, we propose SkipConvGAN - an extension of
our prior work SkipConvNet. The proposed system's generator network tries to
estimate an efficient complex time-frequency mask, while the discriminator
network aids in driving the generator to restore the lost formant structure. We
evaluate the performance of our proposed system on simulated and real
recordings of reverberant speech from the single-channel task of the REVERB
challenge corpus. The proposed system shows a consistent improvement across
multiple room configurations over other deep learning-based generative
adversarial frameworks.","Published in: IEEE/ACM Transactions on Audio, Speech, and Language
  Processing ( Volume: 30)",
Deep Neural Mel-Subband Beamformer for In-car Speech Separation,"[arxiv.Result.Author('Vinay Kothapally'), arxiv.Result.Author('Yong Xu'), arxiv.Result.Author('Meng Yu'), arxiv.Result.Author('Shi-Xiong Zhang'), arxiv.Result.Author('Dong Yu')]",2022-11-22 21:11:26+00:00,"While current deep learning (DL)-based beamforming techniques have been
proved effective in speech separation, they are often designed to process
narrow-band (NB) frequencies independently which results in higher
computational costs and inference times, making them unsuitable for real-world
use. In this paper, we propose DL-based mel-subband spatio-temporal beamformer
to perform speech separation in a car environment with reduced computation cost
and inference time. As opposed to conventional subband (SB) approaches, our
framework uses a mel-scale based subband selection strategy which ensures a
fine-grained processing for lower frequencies where most speech formant
structure is present, and coarse-grained processing for higher frequencies. In
a recursive way, robust frame-level beamforming weights are determined for each
speaker location/zone in a car from the estimated subband speech and noise
covariance matrices. Furthermore, proposed framework also estimates and
suppresses any echoes from the loudspeaker(s) by using the echo reference
signals. We compare the performance of our proposed framework to several NB,
SB, and full-band (FB) processing techniques in terms of speech quality and
recognition metrics. Based on experimental evaluations on simulated and
real-world recordings, we find that our proposed framework achieves better
separation performance over all SB and FB approaches and achieves performance
closer to NB processing techniques while requiring lower computing cost.",Submitted to ICASSP 2023,
TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation,"[arxiv.Result.Author('Zhong-Qiu Wang'), arxiv.Result.Author('Samuele Cornell'), arxiv.Result.Author('Shukjae Choi'), arxiv.Result.Author('Younglo Lee'), arxiv.Result.Author('Byeong-Yeol Kim'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-22 17:43:12+00:00,"We propose TF-GridNet for speech separation. The model is a novel multi-path
deep neural network (DNN) integrating full- and sub-band modeling in the
time-frequency (T-F) domain. It stacks several multi-path blocks, each
consisting of an intra-frame full-band module, a sub-band temporal module, and
a cross-frame self-attention module. It is trained to perform complex spectral
mapping, where the real and imaginary (RI) components of input signals are
stacked as features to predict target RI components. We first evaluate it on
monaural anechoic speaker separation. Without using data augmentation and
dynamic mixing, it obtains a state-of-the-art 23.5 dB improvement in
scale-invariant signal-to-distortion ratio (SI-SDR) on WSJ0-2mix, a standard
dataset for two-speaker separation. To show its robustness to noise and
reverberation, we evaluate it on monaural reverberant speaker separation using
the SMS-WSJ dataset and on noisy-reverberant speaker separation using WHAMR!,
and obtain state-of-the-art performance on both datasets. We then extend
TF-GridNet to multi-microphone conditions through multi-microphone complex
spectral mapping, and integrate it into a two-DNN system with a beamformer in
between (named as MISO-BF-MISO in earlier studies), where the beamformer
proposed in this paper is a novel multi-frame Wiener filter computed based on
the outputs of the first DNN. State-of-the-art performance is obtained on the
multi-channel tasks of SMS-WSJ and WHAMR!. Besides speaker separation, we apply
the proposed algorithms to speech dereverberation and noisy-reverberant speech
enhancement. State-of-the-art performance is obtained on a dereverberation
dataset and on the dataset of the recent L3DAS22 multi-channel speech
enhancement challenge.","In submission. A sound demo is available at
  https://zqwang7.github.io/demos/TF-GridNet-demo/index.html",
On Narrative Information and the Distillation of Stories,"[arxiv.Result.Author('Dylan R. Ashley'), arxiv.Result.Author('Vincent Herrmann'), arxiv.Result.Author('Zachary Friggstad'), arxiv.Result.Author('Jürgen Schmidhuber')]",2022-11-22 17:30:36+00:00,"The act of telling stories is a fundamental part of what it means to be
human. This work introduces the concept of narrative information, which we
define to be the overlap in information space between a story and the items
that compose the story. Using contrastive learning methods, we show how modern
artificial neural networks can be leveraged to distill stories and extract a
representation of the narrative information. We then demonstrate how
evolutionary algorithms can leverage this to extract a set of narrative
templates and how these templates -- in tandem with a novel curve-fitting
algorithm we introduce -- can reorder music albums to automatically induce
stories in them. In the process of doing so, we give strong statistical
evidence that these narrative information templates are present in existing
albums. While we experiment only with music albums here, the premises of our
work extend to any form of (largely) independent media.","presented in the Information-Theoretic Principles in Cognitive
  Systems Workshop at the 36th Conference on Neural Information Processing
  Systems; 4 pages in main text + 2 pages of references + 8 pages of
  appendices, 2 figures in main text + 3 in appendices, 1 table in main text, 2
  algorithms in appendices; source code available at
  https://github.com/dylanashley/story-distiller",
AERO: Audio Super Resolution in the Spectral Domain,"[arxiv.Result.Author('Moshe Mandel'), arxiv.Result.Author('Or Tal'), arxiv.Result.Author('Yossi Adi')]",2022-11-22 12:37:01+00:00,"We present AERO, a audio super-resolution model that processes speech and
music signals in the spectral domain. AERO is based on an encoder-decoder
architecture with U-Net like skip connections. We optimize the model using both
time and frequency domain loss functions. Specifically, we consider a set of
reconstruction losses together with perceptual ones in the form of adversarial
and feature discriminator loss functions. To better handle phase information
the proposed method operates over the complex-valued spectrogram using two
separate channels. Unlike prior work which mainly considers low and high
frequency concatenation for audio super-resolution, the proposed method
directly predicts the full frequency range. We demonstrate high performance
across a wide range of sample rates considering both speech and music. AERO
outperforms the evaluated baselines considering Log-Spectral Distance, ViSQOL,
and the subjective MUSHRA test. Audio samples and code are available at
https://pages.cs.huji.ac.il/adiyoss-lab/aero",,
A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding,"[arxiv.Result.Author('Lizhi Cheng'), arxiv.Result.Author('Wenmian Yang'), arxiv.Result.Author('Weijia Jia')]",2022-11-22 12:24:22+00:00,"Multi-Intent Spoken Language Understanding (SLU), a novel and more complex
scenario of SLU, is attracting increasing attention. Unlike traditional SLU,
each intent in this scenario has its specific scope. Semantic information
outside the scope even hinders the prediction, which tremendously increases the
difficulty of intent detection. More seriously, guiding slot filling with these
inaccurate intent labels suffers error propagation problems, resulting in
unsatisfied overall performance. To solve these challenges, in this paper, we
propose a novel Scope-Sensitive Result Attention Network (SSRAN) based on
Transformer, which contains a Scope Recognizer (SR) and a Result Attention
Network (RAN). Scope Recognizer assignments scope information to each token,
reducing the distraction of out-of-scope tokens. Result Attention Network
effectively utilizes the bidirectional interaction between results of slot
filling and intent detection, mitigating the error propagation problem.
Experiments on two public datasets indicate that our model significantly
improves SLU performance (5.4\% and 2.1\% on Overall accuracy) over the
state-of-the-art baseline.",,
Ontology-aware Learning and Evaluation for Audio Tagging,"[arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Qiuqiang Kong'), arxiv.Result.Author('Xubo Liu'), arxiv.Result.Author('Xinhao Mei'), arxiv.Result.Author('Wenwu Wang'), arxiv.Result.Author('Mark D. Plumbley')]",2022-11-22 11:35:14+00:00,"This study defines a new evaluation metric for audio tagging tasks to
overcome the limitation of the conventional mean average precision (mAP)
metric, which treats different kinds of sound as independent classes without
considering their relations. Also, due to the ambiguities in sound labeling,
the labels in the training and evaluation set are not guaranteed to be accurate
and exhaustive, which poses challenges for robust evaluation with mAP. The
proposed metric, ontology-aware mean average precision (OmAP) addresses the
weaknesses of mAP by utilizing the AudioSet ontology information during the
evaluation. Specifically, we reweight the false positive events in the model
prediction based on the ontology graph distance to the target classes. The OmAP
measure also provides more insights into model performance by evaluations with
different coarse-grained levels in the ontology graph. We conduct human
evaluations and demonstrate that OmAP is more consistent with human perception
than mAP. To further verify the importance of utilizing the ontology
information, we also propose a novel loss function (OBCE) that reweights binary
cross entropy (BCE) loss based on the ontology distance. Our experiment shows
that OBCE can improve both mAP and OmAP metrics on the AudioSet tagging task.","Submitted to ICASSP 2023. The code is open-sourced at
  https://github.com/haoheliu/ontology-aware-audio-tagging",
SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation,"[arxiv.Result.Author('Wenxuan Zhang'), arxiv.Result.Author('Xiaodong Cun'), arxiv.Result.Author('Xuan Wang'), arxiv.Result.Author('Yong Zhang'), arxiv.Result.Author('Xi Shen'), arxiv.Result.Author('Yu Guo'), arxiv.Result.Author('Ying Shan'), arxiv.Result.Author('Fei Wang')]",2022-11-22 11:35:07+00:00,"Generating talking head videos through a face image and a piece of speech
audio still contains many challenges. ie, unnatural head movement, distorted
expression, and identity modification. We argue that these issues are mainly
because of learning from the coupled 2D motion fields. On the other hand,
explicitly using 3D information also suffers problems of stiff expression and
incoherent video. We present SadTalker, which generates 3D motion coefficients
(head pose, expression) of the 3DMM from audio and implicitly modulates a novel
3D-aware face render for talking head generation. To learn the realistic motion
coefficients, we explicitly model the connections between audio and different
types of motion coefficients individually. Precisely, we present ExpNet to
learn the accurate facial expression from audio by distilling both coefficients
and 3D-rendered faces. As for the head pose, we design PoseVAE via a
conditional VAE to synthesize head motion in different styles. Finally, the
generated 3D motion coefficients are mapped to the unsupervised 3D keypoints
space of the proposed face render, and synthesize the final video. We conduct
extensive experiments to show the superior of our method in terms of motion and
video quality.",Project page: https://sadtalker.github.io,
PromptTTS: Controllable Text-to-Speech with Text Descriptions,"[arxiv.Result.Author('Zhifang Guo'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Yihan Wu'), arxiv.Result.Author('Sheng Zhao'), arxiv.Result.Author('Xu Tan')]",2022-11-22 10:58:38+00:00,"Using a text description as prompt to guide the generation of text or images
(e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and
image generation, in this work, we explore the possibility of utilizing text
descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS)
system (dubbed as PromptTTS) that takes a prompt with both style and content
descriptions as input to synthesize the corresponding speech. Specifically,
PromptTTS consists of a style encoder and a content encoder to extract the
corresponding representations from the prompt, and a speech decoder to
synthesize speech according to the extracted style and content representations.
Compared with previous works in controllable TTS that require users to have
acoustic knowledge to understand style factors such as prosody and pitch,
PromptTTS is more user-friendly since text descriptions are a more natural way
to express speech style (e.g., ''A lady whispers to her friend slowly''). Given
that there is no TTS dataset with prompts, to benchmark the task of PromptTTS,
we construct and release a dataset containing prompts with style and content
information and the corresponding speech. Experiments show that PromptTTS can
generate speech with precise style control and high speech quality. Audio
samples and our dataset are publicly available.",Submitted to ICASSP 2023,
Dynamic Acoustic Compensation and Adaptive Focal Training for Personalized Speech Enhancement,"[arxiv.Result.Author('Xiaofeng Ge'), arxiv.Result.Author('Jiangyu Han'), arxiv.Result.Author('Haixin Guan'), arxiv.Result.Author('Yanhua Long')]",2022-11-22 08:58:23+00:00,"Recently, more and more personalized speech enhancement systems (PSE) with
excellent performance have been proposed. However, two critical issues still
limit the performance and generalization ability of the model: 1) Acoustic
environment mismatch between the test noisy speech and target speaker
enrollment speech; 2) Hard sample mining and learning. In this paper, dynamic
acoustic compensation (DAC) is proposed to alleviate the environment mismatch,
by intercepting the noise or environmental acoustic segments from noisy speech
and mixing it with the clean enrollment speech. To well exploit the hard
samples in training data, we propose an adaptive focal training (AFT) strategy
by assigning adaptive loss weights to hard and non-hard samples during
training. A time-frequency multi-loss training is further introduced to improve
and generalize our previous work sDPCCN for PSE. The effectiveness of proposed
methods are examined on the DNS4 Challenge dataset. Results show that, the DAC
brings large improvements in terms of multiple evaluation metrics, and AFT
reduces the hard sample rate significantly and produces obvious MOS score
improvement.",,
OR-Gate: A Noisy Label Filtering Method for Speaker Verification,"[arxiv.Result.Author('Zhihua Fang'), arxiv.Result.Author('Hanhan Ma'), arxiv.Result.Author('Lin Li'), arxiv.Result.Author('Liang He')]",2022-11-22 08:23:33+00:00,"The deep learning models used for speaker verification are heavily dependent
on large-scale data and correct labels. However, noisy (wrong) labels often
occur, which deteriorates the system's performance. Unfortunately, there are
relatively few studies in this area. In this paper, we propose a method to
gradually filter noisy labels out at the training stage. We compare the network
predictions at different training epochs with ground-truth labels, and select
reliable (considered correct) labels by using the OR gate mechanism like that
in logic circuits. Therefore, our proposed method is named as OR-Gate. We
experimentally demonstrated that the OR-Gate can effectively filter noisy
labels out and has excellent performance.","Submitted to 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP 2023)",
Benchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition,"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Amir Hussein'), arxiv.Result.Author('Oumnia Chellah'), arxiv.Result.Author('Shammur Chowdhury'), arxiv.Result.Author('Hamdy Mubarak'), arxiv.Result.Author('Sunayana Sitaram'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Ahmed Ali')]",2022-11-22 08:14:07+00:00,"Code-switching poses a number of challenges and opportunities for
multilingual automatic speech recognition. In this paper, we focus on the
question of robust and fair evaluation metrics. To that end, we develop a
reference benchmark data set of code-switching speech recognition hypotheses
with human judgments. We define clear guidelines for minimal editing of
automatic hypotheses. We validate the guidelines using 4-way inter-annotator
agreement. We evaluate a large number of metrics in terms of correlation with
human judgments. The metrics we consider vary in terms of representation
(orthographic, phonological, semantic), directness (intrinsic vs extrinsic),
granularity (e.g. word, character), and similarity computation method. The
highest correlation to human judgment is achieved using transliteration
followed by text normalization. We release the first corpus for human
acceptance of code-switching speech recognition results in dialectal
Arabic/English conversation speech.",Accepted to SLT 2022,
TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective,"[arxiv.Result.Author('Andong Li'), arxiv.Result.Author('Guochen Yu'), arxiv.Result.Author('Wenzhe Liu'), arxiv.Result.Author('Xiaodong Li'), arxiv.Result.Author('Chengshi Zheng')]",2022-11-22 05:39:02+00:00,"Despite the promising performance of existing frame-wise all-neural
beamformers in the speech enhancement field, it remains unclear what the
underlying mechanism exists. In this paper, we revisit the beamforming behavior
from the beam-space dictionary perspective and formulate it into the learning
and mixing of different beam-space components. Based on that, we propose an
all-neural beamformer called TaylorBM to simulate Taylor's series expansion
operation in which the 0th-order term serves as a spatial filter to conduct the
beam mixing, and several high-order terms are tasked with residual noise
cancellation for post-processing. The whole system is devised to work in an
end-to-end manner. Experiments are conducted on the spatialized LibriSpeech
corpus and results show that the proposed approach outperforms existing
advanced baselines in terms of evaluation metrics.","In submission to ICASSP 2023, 5 pages",
ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English,"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]",2022-11-22 04:37:14+00:00,"We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -
English Speech Translation Corpus. This corpus is an extension of the ArzEn
speech corpus, which was collected through informal interviews with bilingual
speakers. In this work, we collect translations in both directions, monolingual
Egyptian Arabic and monolingual English, forming a three-way speech translation
corpus. We make the translation guidelines and corpus publicly available. We
also report results for baseline systems for machine translation and speech
translation tasks. We believe this is a valuable resource that can motivate and
facilitate further research studying the code-switching phenomenon from a
linguistic perspective and can be used to train and evaluate NLP systems.","Accepted to the Seventh Arabic Natural Language Processing Workshop
  (WANLP 2022)",
Disentangled Feature Learning for Real-Time Neural Speech Coding,"[arxiv.Result.Author('Xue Jiang'), arxiv.Result.Author('Xiulian Peng'), arxiv.Result.Author('Yuan Zhang'), arxiv.Result.Author('Yan Lu')]",2022-11-22 02:50:12+00:00,"Recently end-to-end neural audio/speech coding has shown its great potential
to outperform traditional signal analysis based audio codecs. This is mostly
achieved by following the VQ-VAE paradigm where blind features are learned,
vector-quantized and coded. In this paper, instead of blind end-to-end
learning, we propose to learn disentangled features for real-time neural speech
coding. Specifically, more global-like speaker identity and local content
features are learned with disentanglement to represent speech. Such a compact
feature decomposition not only achieves better coding efficiency by exploiting
bit allocation among different features but also provides the flexibility to do
audio editing in embedding space, such as voice conversion in real-time
communications. Both subjective and objective results demonstrate its coding
efficiency and we find that the learned disentangled features show comparable
performance on any-to-any voice conversion with modern self-supervised speech
representation learning models with far less parameters and low latency,
showing the potential of our neural coding framework.",Submitted to ICASSP2023,
COVID-Net Assistant: A Deep Learning-Driven Virtual Assistant for COVID-19 Symptom Prediction and Recommendation,"[arxiv.Result.Author('Pengyuan Shi'), arxiv.Result.Author('Yuetong Wang'), arxiv.Result.Author('Saad Abbasi'), arxiv.Result.Author('Alexander Wong')]",2022-11-22 01:41:48+00:00,"As the COVID-19 pandemic continues to put a significant burden on healthcare
systems worldwide, there has been growing interest in finding inexpensive
symptom pre-screening and recommendation methods to assist in efficiently using
available medical resources such as PCR tests. In this study, we introduce the
design of COVID-Net Assistant, an efficient virtual assistant designed to
provide symptom prediction and recommendations for COVID-19 by analyzing users'
cough recordings through deep convolutional neural networks. We explore a
variety of highly customized, lightweight convolutional neural network
architectures generated via machine-driven design exploration (which we refer
to as COVID-Net Assistant neural networks) on the Covid19-Cough benchmark
dataset. The Covid19-Cough dataset comprises 682 cough recordings from a
COVID-19 positive cohort and 642 from a COVID-19 negative cohort. Among the 682
cough recordings labeled positive, 382 recordings were verified by PCR test.
Our experimental results show promising, with the COVID-Net Assistant neural
networks demonstrating robust predictive performance, achieving AUC scores of
over 0.93, with the best score over 0.95 while being fast and efficient in
inference. The COVID-Net Assistant models are made available in an open source
manner through the COVID-Net open initiative and, while not a production-ready
solution, we hope their availability acts as a good resource for clinical
scientists, machine learning researchers, as well as citizen scientists to
develop innovative solutions.",,
Latent Iterative Refinement for Modular Source Separation,"[arxiv.Result.Author('Dimitrios Bralios'), arxiv.Result.Author('Efthymios Tzinis'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Paris Smaragdis'), arxiv.Result.Author('Jonathan Le Roux')]",2022-11-22 00:02:57+00:00,"Traditional source separation approaches train deep neural network models
end-to-end with all the data available at once by minimizing the empirical risk
on the whole training set. On the inference side, after training the model, the
user fetches a static computation graph and runs the full model on some
specified observed mixture signal to get the estimated source signals.
Additionally, many of those models consist of several basic processing blocks
which are applied sequentially. We argue that we can significantly increase
resource efficiency during both training and inference stages by reformulating
a model's training and inference procedures as iterative mappings of latent
signal representations. First, we can apply the same processing block more than
once on its output to refine the input signal and consequently improve
parameter efficiency. During training, we can follow a block-wise procedure
which enables a reduction on memory requirements. Thus, one can train a very
complicated network structure using significantly less computation compared to
end-to-end training. During inference, we can dynamically adjust how many
processing blocks and iterations of a specific block an input signal needs
using a gating module.",,
"SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale","[arxiv.Result.Author('Raphael Tang'), arxiv.Result.Author('Karun Kumar'), arxiv.Result.Author('Gefei Yang'), arxiv.Result.Author('Akshat Pandey'), arxiv.Result.Author('Yajie Mao'), arxiv.Result.Author('Vladislav Belyaev'), arxiv.Result.Author('Madhuri Emmadi'), arxiv.Result.Author('Craig Murray'), arxiv.Result.Author('Ferhan Ture'), arxiv.Result.Author('Jimmy Lin')]",2022-11-21 18:58:36+00:00,"End-to-end automatic speech recognition systems represent the state of the
art, but they rely on thousands of hours of manually annotated speech for
training, as well as heavyweight computation for inference. Of course, this
impedes commercialization since most companies lack vast human and
computational resources. In this paper, we explore training and deploying an
ASR system in the label-scarce, compute-limited setting. To reduce human labor,
we use a third-party ASR system as a weak supervision source, supplemented with
labeling functions derived from implicit user feedback. To accelerate
inference, we propose to route production-time queries across a pool of CUDA
graphs of varying input lengths, the distribution of which best matches the
traffic's. Compared to our third-party ASR, we achieve a relative improvement
in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet,
currently serves 12 million queries per day on our voice-enabled smart
television. To our knowledge, this is the first time a large-scale,
Wav2vec-based deployment has been described in the academic literature.","Accepted to EMNLP 2022 Industry Track; 9 pages, 7 figures",
Towards continually learning new languages,"[arxiv.Result.Author('Ngoc-Quan Pham'), arxiv.Result.Author('Jan Niehues'), arxiv.Result.Author('Alexander Waibel')]",2022-11-21 18:24:34+00:00,"Multilingual speech recognition with neural networks is often implemented
with batch-learning, when all of the languages are available before training.
An ability to add new languages after the prior training sessions can be
economically beneficial, but the main challenge is catastrophic forgetting. In
this work, we combine the qualities of weight factorization and elastic weight
consolidation in order to counter catastrophic forgetting and facilitate
learning new languages quickly. Such combination allowed us to eliminate
catastrophic forgetting while still achieving performance for the new languages
comparable with having all languages at once, in experiments of learning from
an initial 10 languages to achieve 26 languages without catastrophic forgetting
and a reasonable performance compared to training all languages from scratch.",Submitted to ICCASP 2023 - Revision 1.0,
Constructing Effective Machine Learning Models for the Sciences: A Multidisciplinary Perspective,"[arxiv.Result.Author('Alice E. A. Allen'), arxiv.Result.Author('Alexandre Tkatchenko')]",2022-11-21 17:48:44+00:00,"Learning from data has led to substantial advances in a multitude of
disciplines, including text and multimedia search, speech recognition, and
autonomous-vehicle navigation. Can machine learning enable similar leaps in the
natural and social sciences? This is certainly the expectation in many
scientific fields and recent years have seen a plethora of applications of
non-linear models to a wide range of datasets. However, flexible non-linear
solutions will not always improve upon manually adding transforms and
interactions between variables to linear regression models. We discuss how to
recognize this before constructing a data-driven model and how such analysis
can help us move to intrinsically interpretable regression models. Furthermore,
for a variety of applications in the natural and social sciences we demonstrate
why improvements may be seen with more complex regression models and why they
may not.",,
A Dataset for Greek Traditional and Folk Music: Lyra,"[arxiv.Result.Author('Charilaos Papaioannou'), arxiv.Result.Author('Ioannis Valiantzas'), arxiv.Result.Author('Theodoros Giannakopoulos'), arxiv.Result.Author('Maximos Kaliakatsos-Papakostas'), arxiv.Result.Author('Alexandros Potamianos')]",2022-11-21 14:15:43+00:00,"Studying under-represented music traditions under the MIR scope is crucial,
not only for developing novel analysis tools, but also for unveiling musical
functions that might prove useful in studying world musics. This paper presents
a dataset for Greek Traditional and Folk music that includes 1570 pieces,
summing in around 80 hours of data. The dataset incorporates YouTube
timestamped links for retrieving audio and video, along with rich metadata
information with regards to instrumentation, geography and genre, among others.
The content has been collected from a Greek documentary series that is
available online, where academics present music traditions of Greece with live
music and dance performance during the show, along with discussions about
social, cultural and musicological aspects of the presented music. Therefore,
this procedure has resulted in a significant wealth of descriptions regarding a
variety of aspects, such as musical genre, places of origin and musical
instruments. In addition, the audio recordings were performed under strict
production-level specifications, in terms of recording equipment, leading to
very clean and homogeneous audio content. In this work, apart from presenting
the dataset in detail, we propose a baseline deep-learning classification
approach to recognize the involved musicological attributes. The dataset, the
baseline classification methods and the models are provided in public
repositories. Future directions for further refining the dataset are also
discussed.",,
Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR,"[arxiv.Result.Author('Fangyuan Wang'), arxiv.Result.Author('Xiyuan Wang'), arxiv.Result.Author('Bo Xu')]",2022-11-21 13:04:37+00:00,"This paper presents an in-depth study on a Sequentially Sampled Chunk
Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer
first demonstrates the significant performance gains from using the
sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the
Conformer encoder by allowing efficient cross-chunk interactions while keeping
linear complexities. Furthermore, it explores taking advantage of chunked
convolution to make use of the chunk-wise future context and integrates with
casual convolution in the convolution layers to further reduce CER. We verify
the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results
show that a state-of-the-art performance for streaming E2E ASR is achieved with
CER 5.33% without LM rescoring. And, owing to its linear complexity, the
SSC-Conformer can train with large batch sizes and infer more efficiently.",This paper has been submitted to ICASSP 2023,
LISA: Localized Image Stylization with Audio via Implicit Neural Representation,"[arxiv.Result.Author('Seung Hyun Lee'), arxiv.Result.Author('Chanyoung Kim'), arxiv.Result.Author('Wonmin Byeon'), arxiv.Result.Author('Sang Ho Yoon'), arxiv.Result.Author('Jinkyu Kim'), arxiv.Result.Author('Sangpil Kim')]",2022-11-21 11:51:48+00:00,"We present a novel framework, Localized Image Stylization with Audio (LISA)
which performs audio-driven localized image stylization. Sound often provides
information about the specific context of the scene and is closely related to a
certain part of the scene or object. However, existing image stylization works
have focused on stylizing the entire image using an image or text input.
Stylizing a particular part of the image based on audio input is natural but
challenging. In this work, we propose a framework that a user provides an audio
input to localize the sound source in the input image and another for locally
stylizing the target object or scene. LISA first produces a delicate
localization map with an audio-visual localization network by leveraging CLIP
embedding space. We then utilize implicit neural representation (INR) along
with the predicted localization map to stylize the target object or scene based
on sound information. The proposed INR can manipulate the localized pixel
values to be semantically consistent with the provided audio input. Through a
series of experiments, we show that the proposed framework outperforms the
other audio-guided stylization methods. Moreover, LISA constructs concise
localization maps and naturally manipulates the target object or scene in
accordance with the given audio input.",,
VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning,"[arxiv.Result.Author('Qiushi Zhu'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Ziqiang Zhang'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Binxing Jiao'), arxiv.Result.Author('Jie Zhang'), arxiv.Result.Author('Lirong Dai'), arxiv.Result.Author('Daxin Jiang'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Furu Wei')]",2022-11-21 09:10:10+00:00,"Although speech is a simple and effective way for humans to communicate with
the outside world, a more realistic speech interaction contains multimodal
information, e.g., vision, text. How to design a unified framework to integrate
different modal information and leverage different resources (e.g.,
visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to
facilitate speech representation learning was not well explored. In this paper,
we propose a unified cross-modal representation learning framework VATLM
(Visual-Audio-Text Language Model). The proposed VATLM employs a unified
backbone network to model the modality-independent information and utilizes
three simple modality-dependent modules to preprocess visual, speech, and text
inputs. In order to integrate these three modalities into one shared semantic
space, VATLM is optimized with a masked prediction task of unified tokens,
given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on
audio-visual related downstream tasks, including audio-visual speech
recognition (AVSR), visual speech recognition (VSR) tasks. Results show that
the proposed VATLM outperforms previous the state-of-the-art models, such as
audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that
VATLM is capable of aligning different modalities into the same space. To
facilitate future research, we release the code and pre-trained models at
https://aka.ms/vatlm.",10 pages,
The applicability of transperceptual and deep learning approaches to the study and mimicry of complex cartilaginous tissues,"[arxiv.Result.Author('J. Waghorne'), arxiv.Result.Author('C. Howard'), arxiv.Result.Author('H. Hu'), arxiv.Result.Author('J. Pang'), arxiv.Result.Author('W. J. Peveler'), arxiv.Result.Author('L. Harris'), arxiv.Result.Author('O. Barrera')]",2022-11-21 08:51:52+00:00,"Complex soft tissues, for example the knee meniscus, play a crucial role in
mobility and joint health, but when damaged are incredibly difficult to repair
and replace. This is due to their highly hierarchical and porous nature which
in turn leads to their unique mechanical properties. In order to design tissue
substitutes, the internal architecture of the native tissue needs to be
understood and replicated. Here we explore a combined audio-visual approach -
so called transperceptual - to generate artificial architectures mimicking the
native ones. The proposed method uses both traditional imagery, and sound
generated from each image as a method of rapidly comparing and contrasting the
porosity and pore size within the samples. We have trained and tested a
generative adversarial network (GAN) on the 2D image stacks. The impact of the
training set of images on the similarity of the artificial to the original
dataset was assessed by analyzing two samples. The first consisting of n=478
pairs of audio and image files for which the images were downsampled to 64
$\times$ 64 pixels, the second one consisting of n=7640 pairs of audio and
image files for which the full resolution 256 $\times$ 256 pixels is retained
but each image is divided into 16 squares to maintain the limit of 64 $\times$
64 pixels required by the GAN. We reconstruct the 2D stacks of artificially
generated datasets into 3D objects and run image analysis algorithms to
characterize statistically the architectural parameters - pore size, tortuosity
and pore connectivity - and compare them with the original dataset. Results
show that the artificially generated dataset that undergoes downsampling
performs better in terms of parameter matching. Our audiovisual approach has
the potential to be extended to larger data sets to explore both how
similarities and differences can be audibly recognized across multiple samples.",,
"Video Background Music Generation: Dataset, Method and Evaluation","[arxiv.Result.Author('Le Zhuo'), arxiv.Result.Author('Zhaokai Wang'), arxiv.Result.Author('Baisen Wang'), arxiv.Result.Author('Yue Liao'), arxiv.Result.Author('Stanley Peng'), arxiv.Result.Author('Chenxi Bao'), arxiv.Result.Author('Miao Lu'), arxiv.Result.Author('Xiaobo Li'), arxiv.Result.Author('Si Liu')]",2022-11-21 08:39:48+00:00,"Music is essential when editing videos, but selecting music manually is
difficult and time-consuming. Thus, we seek to automatically generate
background music tracks given video input. This is a challenging task since it
requires plenty of paired videos and music to learn their correspondence.
Unfortunately, there exist no such datasets. To close this gap, we introduce a
dataset, benchmark model, and evaluation metric for video background music
generation. We introduce SymMV, a video and symbolic music dataset, along with
chord, rhythm, melody, and accompaniment annotations. To the best of our
knowledge, it is the first video-music dataset with high-quality symbolic music
and detailed annotations. We also propose a benchmark video background music
generation framework named V-MusProd, which utilizes music priors of chords,
melody, and accompaniment along with video-music relations of semantic, color,
and motion features. To address the lack of objective metrics for video-music
correspondence, we propose a retrieval-based metric VMCP built upon a powerful
video-music representation learning model. Experiments show that with our
dataset, V-MusProd outperforms the state-of-the-art method in both music
quality and correspondence with videos. We believe our dataset, benchmark
model, and evaluation metric will boost the development of video background
music generation.",,
TimbreCLIP: Connecting Timbre to Text and Images,"[arxiv.Result.Author('Nicolas Jonason'), arxiv.Result.Author('Bob L. T. Sturm')]",2022-11-21 07:40:01+00:00,"We present work in progress on TimbreCLIP, an audio-text cross modal
embedding trained on single instrument notes. We evaluate the models with a
cross-modal retrieval task on synth patches. Finally, we demonstrate the
application of TimbreCLIP on two tasks: text-driven audio equalization and
timbre to image generation.",Submitted to AAAI workshop on creative AI across modalities,
Embedding a Differentiable Mel-cepstral Synthesis Filter to a Neural Speech Synthesis System,"[arxiv.Result.Author('Takenori Yoshimura'), arxiv.Result.Author('Shinji Takaki'), arxiv.Result.Author('Kazuhiro Nakamura'), arxiv.Result.Author('Keiichiro Oura'), arxiv.Result.Author('Yukiya Hono'), arxiv.Result.Author('Kei Hashimoto'), arxiv.Result.Author('Yoshihiko Nankaku'), arxiv.Result.Author('Keiichi Tokuda')]",2022-11-21 07:35:21+00:00,"This paper integrates a classic mel-cepstral synthesis filter into a modern
neural speech synthesis system towards end-to-end controllable speech
synthesis. Since the mel-cepstral synthesis filter is explicitly embedded in
neural waveform models in the proposed system, both voice characteristics and
the pitch of synthesized speech are highly controlled via a frequency warping
parameter and fundamental frequency, respectively. We implement the
mel-cepstral synthesis filter as a differentiable and GPU-friendly module to
enable the acoustic and waveform models in the proposed system to be
simultaneously optimized in an end-to-end manner. Experiments show that the
proposed system improves speech quality from a baseline system maintaining
controllability. The core PyTorch modules used in the experiments will be
publicly available on GitHub.",Submitted to ICASSP 2023,
Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task,"[arxiv.Result.Author('Shangda Wu'), arxiv.Result.Author('Maosong Sun')]",2022-11-21 07:19:17+00:00,"Benefiting from large-scale datasets and pre-trained models, the field of
generative models has recently gained significant momentum. However, most
datasets for symbolic music are very small, which potentially limits the
performance of data-driven multimodal models. An intuitive solution to this
problem is to leverage pre-trained models from other modalities (e.g., natural
language) to improve the performance of symbolic music-related multimodal
tasks. In this paper, we carry out the first study of generating complete and
semantically consistent symbolic music scores from text descriptions, and
explore the efficacy of using publicly available checkpoints (i.e., BERT,
GPT-2, and BART) for natural language processing in the task of text-to-music
generation. Our experimental results show that the improvement from using
pre-trained checkpoints is statistically significant in terms of BLEU score and
edit distance similarity. We analyse the capabilities and limitations of our
model to better understand the potential of language-music models.",Accepted by the Creative AI Across Modalities workshop at AAAI 2023,
Simultaneously Learning Robust Audio Embeddings and balanced Hash codes for Query-by-Example,"[arxiv.Result.Author('Anup Singh'), arxiv.Result.Author('Kris Demuynck'), arxiv.Result.Author('Vipul Arora')]",2022-11-20 19:22:44+00:00,"Audio fingerprinting systems must efficiently and robustly identify query
snippets in an extensive database. To this end, state-of-the-art systems use
deep learning to generate compact audio fingerprints. These systems deploy
indexing methods, which quantize fingerprints to hash codes in an unsupervised
manner to expedite the search. However, these methods generate imbalanced hash
codes, leading to their suboptimal performance. Therefore, we propose a
self-supervised learning framework to compute fingerprints and balanced hash
codes in an end-to-end manner to achieve both fast and accurate retrieval
performance. We model hash codes as a balanced clustering process, which we
regard as an instance of the optimal transport problem. Experimental results
indicate that the proposed approach improves retrieval efficiency while
preserving high accuracy, particularly at high distortion levels, compared to
the competing methods. Moreover, our system is efficient and scalable in
computational load and memory storage.","We need to rewrite the subsection 'Efficiency' section under section
  4 to make it more easy to follow for the readers and appreciate our results",
PointResNet: Residual Network for 3D Point Cloud Segmentation and Classification,"[arxiv.Result.Author('Aadesh Desai'), arxiv.Result.Author('Saagar Parikh'), arxiv.Result.Author('Seema Kumari'), arxiv.Result.Author('Shanmuganathan Raman')]",2022-11-20 17:39:48+00:00,"Point cloud segmentation and classification are some of the primary tasks in
3D computer vision with applications ranging from augmented reality to
robotics. However, processing point clouds using deep learning-based algorithms
is quite challenging due to the irregular point formats. Voxelization or 3D
grid-based representation are different ways of applying deep neural networks
to this problem. In this paper, we propose PointResNet, a residual block-based
approach. Our model directly processes the 3D points, using a deep neural
network for the segmentation and classification tasks. The main components of
the architecture are: 1) residual blocks and 2) multi-layered perceptron (MLP).
We show that it preserves profound features and structural information, which
are useful for segmentation and classification tasks. The experimental
evaluations demonstrate that the proposed model produces the best results for
segmentation and comparable results for classification in comparison to the
conventional baselines.","Paper Under Review at IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP) 2023",
LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders,"[arxiv.Result.Author('Rodrigo Mira'), arxiv.Result.Author('Buye Xu'), arxiv.Result.Author('Jacob Donley'), arxiv.Result.Author('Anurag Kumar'), arxiv.Result.Author('Stavros Petridis'), arxiv.Result.Author('Vamsi Krishna Ithapu'), arxiv.Result.Author('Maja Pantic')]",2022-11-20 15:27:55+00:00,"Audio-visual speech enhancement aims to extract clean speech from a noisy
environment by leveraging not only the audio itself but also the target
speaker's lip movements. This approach has been shown to yield improvements
over audio-only speech enhancement, particularly for the removal of interfering
speech. Despite recent advances in speech synthesis, most audio-visual
approaches continue to use spectral mapping/masking to reproduce the clean
audio, often resulting in visual backbones added to existing speech enhancement
architectures. In this work, we propose LA-VocE, a new two-stage approach that
predicts mel-spectrograms from noisy audio-visual speech via a
transformer-based architecture, and then converts them into waveform audio
using a neural vocoder (HiFi-GAN). We train and evaluate our framework on
thousands of speakers and 11+ different languages, and study our model's
ability to adapt to different levels of background noise and speech
interference. Our experiments show that LA-VocE outperforms existing methods
according to multiple metrics, particularly under very noisy scenarios.",Submitted to ICASSP 2023,
Contrastive Regularization for Multimodal Emotion Recognition Using Audio and Text,"[arxiv.Result.Author('Fan Qian'), arxiv.Result.Author('Jiqing Han')]",2022-11-20 06:56:26+00:00,"Speech emotion recognition is a challenge and an important step towards more
natural human-computer interaction (HCI). The popular approach is multimodal
emotion recognition based on model-level fusion, which means that the
multimodal signals can be encoded to acquire embeddings, and then the
embeddings are concatenated together for the final classification. However, due
to the influence of noise or other factors, each modality does not always tend
to the same emotional category, which affects the generalization of a model. In
this paper, we propose a novel regularization method via contrastive learning
for multimodal emotion recognition using audio and text. By introducing a
discriminator to distinguish the difference between the same and different
emotional pairs, we explicitly restrict the latent code of each modality to
contain the same emotional information, so as to reduce the noise interference
and get more discriminative representation. Experiments are performed on the
standard IEMOCAP dataset for 4-class emotion recognition. The results show a
significant improvement of 1.44\% and 1.53\% in terms of weighted accuracy (WA)
and unweighted accuracy (UA) compared to the baseline system.",Completed in October 2020 and submitted to ICASSP2021,
Audio-visual video face hallucination with frequency supervision and cross modality support by speech based lip reading loss,"[arxiv.Result.Author('Shailza Sharma'), arxiv.Result.Author('Abhinav Dhall'), arxiv.Result.Author('Vinay Kumar'), arxiv.Result.Author('Vivek Singh Bawa')]",2022-11-20 06:44:11+00:00,"Recently, there has been numerous breakthroughs in face hallucination tasks.
However, the task remains rather challenging in videos in comparison to the
images due to inherent consistency issues. The presence of extra temporal
dimension in video face hallucination makes it non-trivial to learn the facial
motion through out the sequence. In order to learn these fine spatio-temporal
motion details, we propose a novel cross-modal audio-visual Video Face
Hallucination Generative Adversarial Network (VFH-GAN). The architecture
exploits the semantic correlation of between the movement of the facial
structure and the associated speech signal. Another major issue in present
video based approaches is the presence of blurriness around the key facial
regions such as mouth and lips - where spatial displacement is much higher in
comparison to other areas. The proposed approach explicitly defines a lip
reading loss to learn the fine grain motion in these facial areas. During
training, GANs have potential to fit frequencies from low to high, which leads
to miss the hard to synthesize frequencies. Therefore, to add salient frequency
features to the network we add a frequency based loss function. The visual and
the quantitative comparison with state-of-the-art shows a significant
improvement in performance and efficacy.",,
Entity-Assisted Language Models for Identifying Check-worthy Sentences,"[arxiv.Result.Author('Ting Su'), arxiv.Result.Author('Craig Macdonald'), arxiv.Result.Author('Iadh Ounis')]",2022-11-19 12:03:30+00:00,"We propose a new uniform framework for text classification and ranking that
can automate the process of identifying check-worthy sentences in political
debates and speech transcripts. Our framework combines the semantic analysis of
the sentences, with additional entity embeddings obtained through the
identified entities within the sentences. In particular, we analyse the
semantic meaning of each sentence using state-of-the-art neural language models
such as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained
from knowledge graph (KG) embedding models. Specifically, we instantiate our
framework using five different language models, entity embeddings obtained from
six different KG embedding models, as well as two combination methods leading
to several Entity-Assisted neural language models. We extensively evaluate the
effectiveness of our framework using two publicly available datasets from the
CLEF' 2019 & 2020 CheckThat! Labs. Our results show that the neural language
models significantly outperform traditional TF.IDF and LSTM methods. In
addition, we show that the ALBERT model is consistently the most effective
model among all the tested neural language models. Our entity embeddings
significantly outperform other existing approaches from the literature that are
based on similarity and relatedness scores between the entities in a sentence,
when used alongside a KG embedding.","22 pages, 15 tables, 3 figures",
VarietySound: Timbre-Controllable Video to Sound Generation via Unsupervised Information Disentanglement,"[arxiv.Result.Author('Chenye Cui'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Zhou Zhao')]",2022-11-19 11:12:01+00:00,"Video to sound generation aims to generate realistic and natural sound given
a video input. However, previous video-to-sound generation methods can only
generate a random or average timbre without any controls or specializations of
the generated sound timbre, leading to the problem that people cannot obtain
the desired timbre under these methods sometimes. In this paper, we pose the
task of generating sound with a specific timbre given a video input and a
reference audio sample. To solve this task, we disentangle each target sound
audio into three components: temporal information, acoustic information, and
background information. We first use three encoders to encode these components
respectively: 1) a temporal encoder to encode temporal information, which is
fed with video frames since the input video shares the same temporal
information as the original audio; 2) an acoustic encoder to encode timbre
information, which takes the original audio as input and discards its temporal
information by a temporal-corrupting operation; and 3) a background encoder to
encode the residual or background sound, which uses the background part of the
original audio as input. To make the generated result achieve better quality
and temporal alignment, we also adopt a mel discriminator and a temporal
discriminator for the adversarial training. Our experimental results on the VAS
dataset demonstrate that our method can generate high-quality audio samples
with good synchronization with events in video and high timbre similarity with
the reference audio.",,
Phonemic Adversarial Attack against Audio Recognition in Real World,"[arxiv.Result.Author('Jiakai Wang'), arxiv.Result.Author('Zhendong Chen'), arxiv.Result.Author('Zixin Yin'), arxiv.Result.Author('Qinghong Yang'), arxiv.Result.Author('Xianglong Liu')]",2022-11-19 11:01:21+00:00,"Recently, adversarial attacks for audio recognition have attracted much
attention. However, most of the existing studies mainly rely on the
coarse-grain audio features at the instance level to generate adversarial
noises, which leads to expensive generation time costs and weak universal
attacking ability. Motivated by the observations that all audio speech consists
of fundamental phonemes, this paper proposes a phonemic adversarial tack (PAT)
paradigm, which attacks the fine-grain audio features at the phoneme level
commonly shared across audio instances, to generate phonemic adversarial
noises, enjoying the more general attacking ability with fast generation speed.
Specifically, for accelerating the generation, a phoneme density balanced
sampling strategy is introduced to sample quantity less but phonemic features
abundant audio instances as the training data via estimating the phoneme
density, which substantially alleviates the heavy dependency on the large
training dataset. Moreover, for promoting universal attacking ability, the
phonemic noise is optimized in an asynchronous way with a sliding window, which
enhances the phoneme diversity and thus well captures the critical fundamental
phonemic patterns. By conducting extensive experiments, we comprehensively
investigate the proposed PAT framework and demonstrate that it outperforms the
SOTA baselines by large margins (i.e., at least 11X speed up and 78% attacking
ability improvement).",,
EDGE: Editable Dance Generation From Music,"[arxiv.Result.Author('Jonathan Tseng'), arxiv.Result.Author('Rodrigo Castellon'), arxiv.Result.Author('C. Karen Liu')]",2022-11-19 10:41:38+00:00,"Dance is an important human art form, but creating new dances can be
difficult and time-consuming. In this work, we introduce Editable Dance
GEneration (EDGE), a state-of-the-art method for editable dance generation that
is capable of creating realistic, physically-plausible dances while remaining
faithful to the input music. EDGE uses a transformer-based diffusion model
paired with Jukebox, a strong music feature extractor, and confers powerful
editing capabilities well-suited to dance, including joint-wise conditioning,
and in-betweening. We introduce a new metric for physical plausibility, and
evaluate dance quality generated by our method extensively through (1) multiple
quantitative metrics on physical plausibility, beat alignment, and diversity
benchmarks, and more importantly, (2) a large-scale user study, demonstrating a
significant improvement over previous state-of-the-art methods. Qualitative
samples from our model can be found at our website.",Project website: https://edge-dance.github.io,
Multi-Speaker Expressive Speech Synthesis via Multiple Factors Decoupling,"[arxiv.Result.Author('Xinfa Zhu'), arxiv.Result.Author('Yi Lei'), arxiv.Result.Author('Kun Song'), arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Tao Li'), arxiv.Result.Author('Lei Xie')]",2022-11-19 02:44:51+00:00,"This paper aims to synthesize target speaker's speech with desired speaking
style and emotion by transferring the style and emotion from reference speech
recorded by other speakers. Specifically, we address this challenging problem
with a two-stage framework composed of a text-to-style-and-emotion (Text2SE)
module and a style-and-emotion-to-wave (SE2Wave) module, bridging by neural
bottleneck (BN) features. To further solve the multi-factor (speaker timbre,
speaking style and emotion) decoupling problem, we adopt the multi-label binary
vector (MBV) and mutual information (MI) minimization to respectively
discretize the extracted embeddings and disentangle these highly entangled
factors in both Text2SE and SE2Wave modules. Moreover, we introduce a
semi-supervised training strategy to leverage data from multiple speakers,
including emotion-labelled data, style-labelled data, and unlabeled data. To
better transfer the fine-grained expressiveness from references to the target
speaker in the non-parallel transfer, we introduce a reference-candidate pool
and propose an attention based reference selection approach. Extensive
experiments demonstrate the good design of our model.",Submitted to ICASSP2023,
Filterbank Learning for Small-Footprint Keyword Spotting Robust to Noise,"[arxiv.Result.Author('Iván López-Espejo'), arxiv.Result.Author('Ram C. M. C. Shekar'), arxiv.Result.Author('Zheng-Hua Tan'), arxiv.Result.Author('Jesper Jensen'), arxiv.Result.Author('John H. L. Hansen')]",2022-11-19 02:20:14+00:00,"In the context of keyword spotting (KWS), the replacement of handcrafted
speech features by learnable features has not yielded superior KWS performance.
In this study, we demonstrate that filterbank learning outperforms handcrafted
speech features for KWS whenever the number of filterbank channels is severely
decreased. Reducing the number of channels might yield certain KWS performance
drop, but also a substantial energy consumption reduction, which is key when
deploying common always-on KWS on low-resource devices. Experimental results on
a noisy version of the Google Speech Commands Dataset show that filterbank
learning adapts to noise characteristics to provide a higher degree of
robustness to noise, especially when dropout is integrated. Thus, switching
from typically used 40-channel log-Mel features to 8-channel learned features
leads to a relative KWS accuracy loss of only 3.5% while simultaneously
achieving a 6.3x energy consumption reduction.",,
Impact of visual assistance for automated audio captioning,"[arxiv.Result.Author('Wim Boes'), arxiv.Result.Author('Hugo Van hamme')]",2022-11-18 23:55:13+00:00,"We study the impact of visual assistance for automated audio captioning.
Utilizing multi-encoder transformer architectures, which have previously been
employed to introduce vision-related information in the context of sound event
detection, we analyze the usefulness of incorporating a variety of pretrained
features.
  We perform experiments on a YouTube-based audiovisual data set and
investigate the effect of applying the considered transfer learning technique
in terms of a variety of captioning metrics.
  We find that only one of the considered kinds of pretrained features provides
consistent improvements, while the others do not provide any noteworthy gains
at all. Interestingly, the outcomes of prior research efforts indicate that the
exact opposite is true in the case of sound event detection, leading us to
conclude that the optimal choice of visual embeddings is strongly dependent on
the task at hand.
  More specifically, visual features focusing on semantics appear appropriate
in the context of automated audio captioning, while for sound event detection,
time information seems to be more important.","Submitted to 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)",
Dialogs Re-enacted Across Languages,"[arxiv.Result.Author('Nigel G. Ward'), arxiv.Result.Author('Jonathan E. Avila'), arxiv.Result.Author('Emilia Rivas')]",2022-11-18 17:08:12+00:00,"To support machine learning of cross-language prosodic mappings and other
ways to improve speech-to-speech translation, we present a protocol for
collecting closely matched pairs of utterances across languages, a description
of the resulting data collection, and some observations and musings. This
report is intended for 1) people using the corpus, 2) people extending the
corpus, and 3) people designing similar collections of bilingual dialog data.",,
Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis,"[arxiv.Result.Author('Zhihao Du'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Siqi Zheng'), arxiv.Result.Author('Zhijie Yan')]",2022-11-18 14:03:26+00:00,"Recently, hybrid systems of clustering and neural diarization models have
been successfully applied in multi-party meeting analysis. However, current
models always treat overlapped speaker diarization as a multi-label
classification problem, where speaker dependency and overlaps are not well
considered. To overcome the disadvantages, we reformulate overlapped speaker
diarization task as a single-label prediction problem via the proposed power
set encoding (PSE). Through this formulation, speaker dependency and overlaps
can be explicitly modeled. To fully leverage this formulation, we further
propose the speaker overlap-aware neural diarization (SOND) model, which
consists of a context-independent (CI) scorer to model global speaker
discriminability, a context-dependent scorer (CD) to model local
discriminability, and a speaker combining network (SCN) to combine and reassign
speaker activities. Experimental results show that using the proposed
formulation can outperform the state-of-the-art methods based on target speaker
voice activity detection, and the performance can be further improved with
SOND, resulting in a 6.30% relative diarization error reduction.",Accepted by EMNLP 2022,
Corpus non alignés et ADT. Essai de comparaison entre les présidents français et brésiliens de l'ère contemporaine,"[arxiv.Result.Author('Carlos Maciel'), arxiv.Result.Author('Damon Mayaffre'), arxiv.Result.Author('Laurent Vanni')]",2022-11-18 12:40:47+00:00,"Is there an ADT method that can deal with non-aligned bilingual corpora? Does
the textual genre exert a sufficiently strong constraint on the discourse that
would make texts written in different languages comparable, provided they are
of identical genre? To answer these two questions, one methodological, the
other linguistic, this contribution gathers in a single corpus French and
Brazilian presidential speeches of the contemporary era (1950-2020), from de
Gaulle to Macron, from Kubitschek to Lula, i.e. 15 million words. A
methodological path is proposed from the simple frequency dictionary to the
factorial treatment of the cooccurrencial profiles of words, in order to
establish a generic transnational presidential speech.",in French language,"JADT2022, Jul 2022, Naples, France. pp.568-575"
Self-Remixing: Unsupervised Speech Separation via Separation and Remixing,"[arxiv.Result.Author('Kohei Saijo'), arxiv.Result.Author('Tetsuji Ogawa')]",2022-11-18 12:37:32+00:00,"We present Self-Remixing, a novel self-supervised speech separation method,
which refines a pre-trained separation model in an unsupervised manner. The
proposed method consists of a shuffler module and a solver module, and they
grow together through separation and remixing processes. Specifically, the
shuffler first separates observed mixtures and makes pseudo-mixtures by
shuffling and remixing the separated signals. The solver then separates the
pseudo-mixtures and remixes the separated signals back to the observed
mixtures. The solver is trained using the observed mixtures as supervision,
while the shuffler's weights are updated by taking the moving average with the
solver's, generating the pseudo-mixtures with fewer distortions. Our
experiments demonstrate that Self-Remixing gives better performance over
existing remixing-based self-supervised methods with the same or less training
costs under unsupervised setup. Self-Remixing also outperforms baselines in
semi-supervised domain adaptation, showing effectiveness in multiple setups.","Submitted to ICASSP2023, 5pages, 2figures, 2tables",
Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,"[arxiv.Result.Author('Tharindu Ranasinghe'), arxiv.Result.Author('Kai North'), arxiv.Result.Author('Damith Premasiri'), arxiv.Result.Author('Marcos Zampieri')]",2022-11-18 11:17:15+00:00,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches.",,
Self-Transriber: Few-shot Lyrics Transcription with Self-training,"[arxiv.Result.Author('Xiaoxue Gao'), arxiv.Result.Author('Xianghu Yue'), arxiv.Result.Author('Haizhou Li')]",2022-11-18 10:58:27+00:00,"The current lyrics transcription approaches heavily rely on supervised
learning with labeled data, but such data are scarce and manual labeling of
singing is expensive. How to benefit from unlabeled data and alleviate limited
data problem have not been explored for lyrics transcription. We propose the
first semi-supervised lyrics transcription paradigm, Self-Transcriber, by
leveraging on unlabeled data using self-training with noisy student
augmentation. We attempt to demonstrate the possibility of lyrics transcription
with a few amount of labeled data. Self-Transcriber generates pseudo labels of
the unlabeled singing using teacher model, and augments pseudo-labels to the
labeled data for student model update with both self-training and supervised
training losses. This work closes the gap between supervised and
semi-supervised learning as well as opens doors for few-shot learning of lyrics
transcription. Our experiments show that our approach using only 12.7 hours of
labeled data achieves competitive performance compared with the supervised
approaches trained on 149.1 hours of labeled data for lyrics transcription.",5 pages,
A Persian ASR-based SER: Modification of Sharif Emotional Speech Database and Investigation of Persian Text Corpora,"[arxiv.Result.Author('Ali Yazdani'), arxiv.Result.Author('Yasser Shekofteh')]",2022-11-18 10:33:20+00:00,"Speech Emotion Recognition (SER) is one of the essential perceptual methods
of humans in understanding the situation and how to interact with others,
therefore, in recent years, it has been tried to add the ability to recognize
emotions to human-machine communication systems. Since the SER process relies
on labeled data, databases are essential for it. Incomplete, low-quality or
defective data may lead to inaccurate predictions. In this paper, we fixed the
inconsistencies in Sharif Emotional Speech Database (ShEMO), as a Persian
database, by using an Automatic Speech Recognition (ASR) system and
investigating the effect of Farsi language models obtained from accessible
Persian text corpora. We also introduced a Persian/Farsi ASR-based SER system
that uses linguistic features of the ASR outputs and Deep Learning-based
models.","7 pages, 4 figures, 8 tables",
Scaling Native Language Identification with Transformer Adapters,"[arxiv.Result.Author('Ahmet Yavuz Uluslu'), arxiv.Result.Author('Gerold Schneider')]",2022-11-18 09:40:16+00:00,"Native language identification (NLI) is the task of automatically identifying
the native language (L1) of an individual based on their language production in
a learned language. It is useful for a variety of purposes including marketing,
security and educational applications. NLI is usually framed as a multi-label
classification task, where numerous designed features are combined to achieve
state-of-the-art results. Recently deep generative approach based on
transformer decoders (GPT-2) outperformed its counterparts and achieved the
best results on the NLI benchmark datasets. We investigate this approach to
determine the practical implications compared to traditional state-of-the-art
NLI systems. We introduce transformer adapters to address memory limitations
and improve training/inference speed to scale NLI applications for production.","Paper accepted to International Conference on Natural Language and
  Speech Processing 2022 (ICNLSP 2022)",
Exploring WavLM on Speech Enhancement,"[arxiv.Result.Author('Hyungchan Song'), arxiv.Result.Author('Sanyuan Chen'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Takuya Yoshioka'), arxiv.Result.Author('Min Tang'), arxiv.Result.Author('Jong Won Shin'), arxiv.Result.Author('Shujie Liu')]",2022-11-18 02:23:16+00:00,"There is a surge in interest in self-supervised learning approaches for
end-to-end speech encoding in recent years as they have achieved great success.
Especially, WavLM showed state-of-the-art performance on various speech
processing tasks. To better understand the efficacy of self-supervised learning
models for speech enhancement, in this work, we design and conduct a series of
experiments with three resource conditions by combining WavLM and two
high-quality speech enhancement systems. Also, we propose a regression-based
WavLM training objective and a noise-mixing data configuration to further boost
the downstream enhancement performance. The experiments on the DNS challenge
dataset and a simulation dataset show that the WavLM benefits the speech
enhancement task in terms of both speech quality and speech recognition
accuracy, especially for low fine-tuning resources. For the high fine-tuning
resource condition, only the word error rate is substantially improved.",Accepted by IEEE SLT 2022,
AVATAR submission to the Ego4D AV Transcription Challenge,"[arxiv.Result.Author('Paul Hongsuck Seo'), arxiv.Result.Author('Arsha Nagrani'), arxiv.Result.Author('Cordelia Schmid')]",2022-11-18 01:03:30+00:00,"In this report, we describe our submission to the Ego4D AudioVisual (AV)
Speech Transcription Challenge 2022. Our pipeline is based on AVATAR, a state
of the art encoder-decoder model for AV-ASR that performs early fusion of
spectrograms and RGB images. We describe the datasets, experimental settings
and ablations. Our final method achieves a WER of 68.40 on the challenge test
set, outperforming the baseline by 43.7%, and winning the challenge.",,
Compressing Transformer-based self-supervised models for speech processing,"[arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Tsung-Huan Yang'), arxiv.Result.Author('Chun-Yao Chang'), arxiv.Result.Author('Kuang-Ming Chen'), arxiv.Result.Author('Tzu-hsun Feng'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Hao Tang')]",2022-11-17 23:53:52+00:00,"Despite the success of Transformers in self-supervised learning with
applications to various downstream tasks, the computational cost of training
and inference remains a major challenge for applying these models to a wide
spectrum of devices. Several isolated attempts have been made to compress
Transformers, prior to applying them to downstream tasks. In this work, we aim
to provide context for the isolated results, studying several commonly used
compression techniques, including weight pruning, head pruning, low-rank
approximation, and knowledge distillation. We report wall-clock time, the
number of parameters, and the number of multiply-accumulate operations for
these techniques, charting the landscape of compressing Transformer-based
self-supervised models.",Submitted to ICASSP 2023,
MelHuBERT: A simplified HuBERT on Mel spectrogram,"[arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Hao Tang')]",2022-11-17 23:38:29+00:00,"Self-supervised models have had great success in learning speech
representations that can generalize to various downstream tasks. HuBERT, in
particular, achieves strong performance while being relatively simple in
training compared to others. The original experimental setting is
computationally extensive, hindering the reproducibility of the models. It is
also unclear why certain design decisions are made, such as the ad-hoc loss
function, and whether these decisions have an impact on the learned
representations. We propose MelHuBERT, a simplified version of HuBERT that
takes Mel spectrograms as input, significantly reducing computation and memory
consumption. We study several aspects of training, including the loss function,
multi-stage training, and streaming options. Our result is a efficient yet
performant model that can be trained on a single GPU.",Submitted to ICASSP 2023,
Multi-source Domain Adaptation for Text-independent Forensic Speaker Recognition,"[arxiv.Result.Author('Zhenyu Wang'), arxiv.Result.Author('John H. L. Hansen')]",2022-11-17 22:11:25+00:00,"Adapting speaker recognition systems to new environments is a widely-used
technique to improve a well-performing model learned from large-scale data
towards a task-specific small-scale data scenarios. However, previous studies
focus on single domain adaptation, which neglects a more practical scenario
where training data are collected from multiple acoustic domains needed in
forensic scenarios. Audio analysis for forensic speaker recognition offers
unique challenges in model training with multi-domain training data due to
location/scenario uncertainty and diversity mismatch between reference and
naturalistic field recordings. It is also difficult to directly employ
small-scale domain-specific data to train complex neural network architectures
due to domain mismatch and performance loss. Fine-tuning is a commonly-used
method for adaptation in order to retrain the model with weights initialized
from a well-trained model. Alternatively, in this study, three novel adaptation
methods based on domain adversarial training, discrepancy minimization, and
moment-matching approaches are proposed to further promote adaptation
performance across multiple acoustic domains. A comprehensive set of
experiments are conducted to demonstrate that: 1) diverse acoustic environments
do impact speaker recognition performance, which could advance research in
audio forensics, 2) domain adversarial training learns the discriminative
features which are also invariant to shifts between domains, 3)
discrepancy-minimizing adaptation achieves effective performance simultaneously
across multiple acoustic domains, and 4) moment-matching adaptation along with
dynamic distribution alignment also significantly promotes speaker recognition
performance on each domain, especially for the LENA-field domain with noise
compared to all other systems.","IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING",
Audio Anti-spoofing Using a Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning,"[arxiv.Result.Author('Zhenyu Wang'), arxiv.Result.Author('John H. L. Hansen')]",2022-11-17 21:25:29+00:00,"Automatic speaker verification systems are vulnerable to a variety of access
threats, prompting research into the formulation of effective spoofing
detection systems to act as a gate to filter out such spoofing attacks. This
study introduces a simple attention module to infer 3-dim attention weights for
the feature map in a convolutional layer, which then optimizes an energy
function to determine each neuron's importance. With the advancement of both
voice conversion and speech synthesis technologies, unseen spoofing attacks are
constantly emerging to limit spoofing detection system performance. Here, we
propose a joint optimization approach based on the weighted additive angular
margin loss for binary classification, with a meta-learning training framework
to develop an efficient system that is robust to a wide range of spoofing
attacks for model generalization enhancement. As a result, when compared to
current state-of-the-art systems, our proposed approach delivers a competitive
result with a pooled EER of 0.99% and min t-DCF of 0.0289.",Interspeech 2022,
Robust Vocal Quality Feature Embeddings for Dysphonic Voice Detection,"[arxiv.Result.Author('Jianwei Zhang'), arxiv.Result.Author('Julie Liss'), arxiv.Result.Author('Suren Jayasuriya'), arxiv.Result.Author('Visar Berisha')]",2022-11-17 19:34:59+00:00,"Approximately 1.2% of the world's population has impaired voice production.
As a result, automatic dysphonic voice detection has attracted considerable
academic and clinical interest. However, existing methods for automated voice
assessment often fail to generalize outside the training conditions or to other
related applications. In this paper, we propose a deep learning framework for
generating acoustic feature embeddings sensitive to vocal quality and robust
across different corpora. A contrastive loss is combined with a classification
loss to train our deep learning model jointly. Data warping methods are used on
input voice samples to improve the robustness of our method. Empirical results
demonstrate that our method not only achieves high in-corpus and cross-corpus
classification accuracy but also generates good embeddings sensitive to voice
quality and robust across different corpora. We also compare our results
against three baseline methods on clean and three variations of deteriorated
in-corpus and cross-corpus datasets and demonstrate that the proposed model
consistently outperforms the baseline methods.","This manuscript is submitted on July 06, 2022 to IEEE/ACM
  Transactions on Audio, Speech, and Language Processing for peer-review",
SPACE: Speech-driven Portrait Animation with Controllable Expression,"[arxiv.Result.Author('Siddharth Gururani'), arxiv.Result.Author('Arun Mallya'), arxiv.Result.Author('Ting-Chun Wang'), arxiv.Result.Author('Rafael Valle'), arxiv.Result.Author('Ming-Yu Liu')]",2022-11-17 18:59:56+00:00,"Animating portraits using speech has received growing attention in recent
years, with various creative and practical use cases. An ideal generated video
should have good lip sync with the audio, natural facial expressions and head
motions, and high frame quality. In this work, we present SPACE, which uses
speech and a single image to generate high-resolution, and expressive videos
with realistic head pose, without requiring a driving video. It uses a
multi-stage approach, combining the controllability of facial landmarks with
the high-quality synthesis power of a pretrained face generator. SPACE also
allows for the control of emotions and their intensities. Our method
outperforms prior methods in objective metrics for image quality and facial
motions and is strongly preferred by users in pair-wise comparisons. The
project website is available at https://deepimagination.cc/SPACE/",,
Heart Abnormality Detection from Heart Sound Signals using MFCC Feature and Dual Stream Attention Based Network,"[arxiv.Result.Author('Nayeeb Rashid'), arxiv.Result.Author('Swapnil Saha'), arxiv.Result.Author('Mohseu Rashid Subah'), arxiv.Result.Author('Rizwan Ahmed Robin'), arxiv.Result.Author('Syed Mortuza Hasan Fahim'), arxiv.Result.Author('Shahed Ahmed'), arxiv.Result.Author('Talha Ibn Mahmud')]",2022-11-17 18:20:46+00:00,"Cardiovascular diseases are one of the leading cause of death in today's
world and early screening of heart condition plays a crucial role in preventing
them. The heart sound signal is one of the primary indicator of heart condition
and can be used to detect abnormality in the heart. The acquisition of heart
sound signal is non-invasive, cost effective and requires minimum equipment.
But currently the detection of heart abnormality from heart sound signal
depends largely on the expertise and experience of the physician. As such an
automatic detection system for heart abnormality detection from heart sound
signal can be a great asset for the people living in underdeveloped areas. In
this paper we propose a novel deep learning based dual stream network with
attention mechanism that uses both the raw heart sound signal and the MFCC
features to detect abnormality in heart condition of a patient. The deep neural
network has a convolutional stream that uses the raw heart sound signal and a
recurrent stream that uses the MFCC features of the signal. The features from
these two streams are merged together using a novel attention network and
passed through the classification network. The model is trained on the largest
publicly available dataset of PCG signal and achieves an accuracy of 87.11,
sensitivity of 82.41, specificty of 91.8 and a MACC of 87.12.",,
"Listen, denoise, action! Audio-driven motion synthesis with diffusion models","[arxiv.Result.Author('Simon Alexanderson'), arxiv.Result.Author('Rajmund Nagy'), arxiv.Result.Author('Jonas Beskow'), arxiv.Result.Author('Gustav Eje Henter')]",2022-11-17 17:41:00+00:00,"Diffusion models have experienced a surge of interest as highly expressive
yet efficiently trainable probabilistic models. We show that these models are
an excellent fit for synthesising human motion that co-occurs with audio, for
example co-speech gesticulation, since motion is complex and highly ambiguous
given audio, calling for a probabilistic description. Specifically, we adapt
the DiffWave architecture to model 3D pose sequences, putting Conformers in
place of dilated convolutions for improved accuracy. We also demonstrate
control over motion style, using classifier-free guidance to adjust the
strength of the stylistic expression. Gesture-generation experiments on the
Trinity Speech-Gesture and ZeroEGGS datasets confirm that the proposed method
achieves top-of-the-line motion quality, with distinctive styles whose
expression can be made more or less pronounced. We also synthesise dance motion
and path-driven locomotion using the same model architecture. Finally, we
extend the guidance procedure to perform style interpolation in a manner that
is appealing for synthesis tasks and has connections to product-of-experts
models, a contribution we believe is of independent interest. Video examples
are available at https://www.speech.kth.se/research/listen-denoise-action/","15 pages, 6 figures",
Towards Building Text-To-Speech Systems for the Next Billion Users,"[arxiv.Result.Author('Gokul Karthik Kumar'), arxiv.Result.Author('Praveen S V'), arxiv.Result.Author('Pratyush Kumar'), arxiv.Result.Author('Mitesh M. Khapra'), arxiv.Result.Author('Karthik Nandakumar')]",2022-11-17 13:59:34+00:00,"Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.",Under review at ICASSP 2023. Gokul and Praveen contributed equally,
Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review,"[arxiv.Result.Author('Mikel K. Ngueajio'), arxiv.Result.Author('Gloria Washington')]",2022-11-17 13:15:58+00:00,"Speech is the fundamental means of communication between humans. The advent
of AI and sophisticated speech technologies have led to the rapid proliferation
of human-to-computer-based interactions, fueled primarily by Automatic Speech
Recognition (ASR) systems. ASR systems normally take human speech in the form
of audio and convert it into words, but for some users, it cannot decode the
speech, and any output text is filled with errors that are incomprehensible to
the human reader. These systems do not work equally for everyone and actually
hinder the productivity of some users. In this paper, we present research that
addresses ASR biases against gender, race, and the sick and disabled, while
exploring studies that propose ASR debiasing techniques for mitigating these
discriminations. We also discuss techniques for designing a more accessible and
inclusive ASR technology. For each approach surveyed, we also provide a summary
of the investigation and methods applied, the ASR systems and corpora used, and
the research findings, and highlight their strengths and/or weaknesses.
Finally, we propose future opportunities for Natural Language Processing
researchers to explore in the next level creation of ASR technologies.","In press at HCI International 2022 - Late Breaking Papers:
  Interacting with eXtended Reality and Artificial Intelligence, LNCS 13518","HCI InternationaLate Breaking Papers: Interacting with eXtended
  Reality and Artificial Intelligence. HCII 2022. Lecture Notes in Computer
  Science, vol 13518. Springer, Cham"
Adaptive Representations of Sound for Automatic Insect Recognition,[arxiv.Result.Author('Marius Faiß')],2022-11-17 12:52:22+00:00,"Insects are an integral part of our ecosystem. These often small and evasive
animals have a big impact on their surroundings, providing a large part of the
present biodiversity and pollination duties, forming the foundation of the food
chain and many biological and ecological processes. Due to factors of human
influence, population numbers and biodiversity have been rapidly declining with
time. Monitoring this decline has become increasingly important for
conservation measures to be effectively implemented. But monitoring methods are
often invasive, time and resource intense, and prone to various biases. Many
insect species produce characteristic mating sounds that can easily be detected
and recorded without large cost or effort. Using deep learning methods, insect
sounds from field recordings could be automatically detected and classified to
monitor biodiversity and species distribution ranges. In this project, I
implement this using existing datasets of insect sounds (Orthoptera and
Cicadidae) and machine learning methods and evaluate their potential for
acoustic insect monitoring. I compare the performance of the conventional
spectrogram-based deep learning method against the new adaptive and
waveform-based approach LEAF. The waveform-based frontend achieved
significantly better classification performance than the Mel-spectrogram
frontend by adapting its feature extraction parameters during training. This
result is encouraging for future implementations of deep learning technology
for automatic insect sound recognition, especially if larger datasets become
available.","30 pages, 9 figures Dataset: https://doi.org/10.5281/zenodo.7072196",
EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance,"[arxiv.Result.Author('Yiwei Guo'), arxiv.Result.Author('Chenpeng Du'), arxiv.Result.Author('Xie Chen'), arxiv.Result.Author('Kai Yu')]",2022-11-17 12:37:48+00:00,"Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.",Submitted to ICASSP2023,
Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation,"[arxiv.Result.Author('Chunyu Qiang'), arxiv.Result.Author('Peng Yang'), arxiv.Result.Author('Hao Che'), arxiv.Result.Author('Jinba Xiao'), arxiv.Result.Author('Xiaorui Wang'), arxiv.Result.Author('Zhongyuan Wang')]",2022-11-17 12:37:41+00:00,"Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.",Published to APSIPA ASC 2022,
LongFNT: Long-form Speech Recognition with Factorized Neural Transducer,"[arxiv.Result.Author('Xun Gong'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Rui Zhao'), arxiv.Result.Author('Xie Chen'), arxiv.Result.Author('Yanmin Qian')]",2022-11-17 08:48:27+00:00,"Traditional automatic speech recognition~(ASR) systems usually focus on
individual utterances, without considering long-form speech with useful
historical information, which is more practical in real scenarios. Simply
attending longer transcription history for a vanilla neural transducer model
shows no much gain in our preliminary experiments, since the prediction network
is not a pure language model. This motivates us to leverage the factorized
neural transducer structure, containing a real language model, the vocabulary
predictor. We propose the {LongFNT-Text} architecture, which fuses the
sentence-level long-form features directly with the output of the vocabulary
predictor and then embeds token-level long-form features inside the vocabulary
predictor, with a pre-trained contextual encoder RoBERTa to further boost the
performance. Moreover, we propose the {LongFNT} architecture by extending the
long-form speech to the original speech input and achieve the best performance.
The effectiveness of our LongFNT approach is validated on LibriSpeech and
GigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,
respectively.",Submitted to ICASSP2023,
NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis,"[arxiv.Result.Author('Hyeong-Seok Choi'), arxiv.Result.Author('Jinhyeok Yang'), arxiv.Result.Author('Juheon Lee'), arxiv.Result.Author('Hyeongju Kim')]",2022-11-17 08:29:57+00:00,"Various applications of voice synthesis have been developed independently
despite the fact that they generate ""voice"" as output in common. In addition,
most of the voice synthesis models still require a large number of audio data
paired with annotated labels (e.g., text transcription and music score) for
training. To this end, we propose a unified framework of synthesizing and
manipulating voice signals from analysis features, dubbed NANSY++. The backbone
network of NANSY++ is trained in a self-supervised manner that does not require
any annotations paired with audio. After training the backbone network, we
efficiently tackle four voice applications - i.e. voice conversion,
text-to-speech, singing voice synthesis, and voice designing - by partially
modeling the analysis features required for each task. Extensive experiments
show that the proposed framework offers competitive advantages such as
controllability, data efficiency, and fast training convergence, while
providing high quality synthesis. Audio samples: tinyurl.com/8tnsy3uc.",Submitted to ICLR 2023,
ComMU: Dataset for Combinatorial Music Generation,"[arxiv.Result.Author('Lee Hyun'), arxiv.Result.Author('Taehyun Kim'), arxiv.Result.Author('Hyolim Kang'), arxiv.Result.Author('Minjoo Ki'), arxiv.Result.Author('Hyeonchan Hwang'), arxiv.Result.Author('Kwanho Park'), arxiv.Result.Author('Sharang Han'), arxiv.Result.Author('Seon Joo Kim')]",2022-11-17 07:25:09+00:00,"Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (https://pozalabs.github.io/ComMU).","19 pages, 12 figures",
Any-speaker Adaptive Text-To-Speech Synthesis with Diffusion Models,"[arxiv.Result.Author('Minki Kang'), arxiv.Result.Author('Dongchan Min'), arxiv.Result.Author('Sung Ju Hwang')]",2022-11-17 07:17:24+00:00,"There has been a significant progress in Text-To-Speech (TTS) synthesis
technology in recent years, thanks to the advancement in neural generative
modeling. However, existing methods on any-speaker adaptive TTS have achieved
unsatisfactory performance, due to their suboptimal accuracy in mimicking the
target speakers' styles. In this work, we present Grad-StyleSpeech, which is an
any-speaker adaptive TTS framework that is based on a diffusion model that can
generate highly natural speech with extremely high similarity to target
speakers' voice, given a few seconds of reference speech. Grad-StyleSpeech
significantly outperforms recent speaker-adaptive TTS baselines on English
benchmarks. Audio samples are available at
https://nardien.github.io/grad-stylespeech-demo.",Under Review,
Token-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire,"[arxiv.Result.Author('Zhiyun Fan'), arxiv.Result.Author('Zhenlin Liang'), arxiv.Result.Author('Linhao Dong'), arxiv.Result.Author('Yi Liu'), arxiv.Result.Author('Shiyu Zhou'), arxiv.Result.Author('Meng Cai'), arxiv.Result.Author('Jun Zhang'), arxiv.Result.Author('Zejun Ma'), arxiv.Result.Author('Bo Xu')]",2022-11-17 07:16:17+00:00,"In multi-talker scenarios such as meetings and conversations, speech
processing systems are usually required to segment the audio and then
transcribe each segmentation. These two stages are addressed separately by
speaker change detection (SCD) and automatic speech recognition (ASR). Most
previous SCD systems rely solely on speaker information and ignore the
importance of speech content. In this paper, we propose a novel SCD system that
considers both cues of speaker difference and speech content. These two cues
are converted into token-level representations by the continuous
integrate-and-fire (CIF) mechanism and then combined for detecting speaker
changes on the token acoustic boundaries. We evaluate the performance of our
approach on a public real-recorded meeting dataset, AISHELL-4. The experiment
results show that our method outperforms a competitive frame-level baseline
system by 2.45% equal coverage-purity (ECP). In addition, we demonstrate the
importance of speech content and speaker difference to the SCD task, and the
advantages of conducting SCD on the token acoustic boundaries compared with
conducting SCD frame by frame.",,
Advanced Audio Aid for Blind People,"[arxiv.Result.Author('Savera Sarwar'), arxiv.Result.Author('Muhammad Turab'), arxiv.Result.Author('Danish Channa'), arxiv.Result.Author('Aisha Chandio'), arxiv.Result.Author('M. Uzair Sohu'), arxiv.Result.Author('Vikram Kumar')]",2022-11-17 07:13:14+00:00,"One of the most important senses in human life is vision, without it life is
totally filled with darkness. According to WHO globally millions of people are
visually impaired estimated there are 285 million, of whom some millions are
blind. Unfortunately, there are around 2.4 million people are blind in our
beloved country Pakistan. Human are a crucial part of society and the blind
community is a main part of society. The technologies are grown so far to make
the life of humans easier more comfortable and more reliable for. However, this
disability of the blind community would reduce their chance of using such
innovative products. Therefore, the visually impaired community believe that
they are burden to other societies and they do not capture in normal activities
separates the blind people from society and because of this believe did not
participate in the normally tasks of society . The visual impair people mainly
face most of the problems in this real-time The aim of this work is to turn the
real time world into an audio world by telling blind person about the objects
in their way and can read printed text. This will enable blind persons to
identify the things and read the text without any external help just by using
the object detection and reading system in real time. Objective of this work:
i) Object detection ii) Read printed text, using state-of-the-art (SOTA)
technology.","Under revision. Submitted to International Conference On Emerging
  Technologies In Electronics, Computing And Communication (ICETECC) 2022",
Balanced Deep CCA for Bird Vocalization Detection,"[arxiv.Result.Author('Sumit Kumar'), arxiv.Result.Author('B. Anshuman'), arxiv.Result.Author('Linus Ruettimann'), arxiv.Result.Author('Richard H. R. Hahnloser'), arxiv.Result.Author('Vipul Arora')]",2022-11-17 07:09:07+00:00,"Event detection improves when events are captured by two different modalities
rather than just one. But to train detection systems on multiple modalities is
challenging, in particular when there is abundance of unlabelled data but
limited amounts of labeled data. We develop a novel self-supervised learning
technique for multi-modal data that learns (hidden) correlations between
simultaneously recorded microphone (sound) signals and accelerometer (body
vibration) signals. The key objective of this work is to learn useful
embeddings associated with high performance in downstream event detection tasks
when labeled data is scarce and the audio events of interest (songbird
vocalizations) are sparse. We base our approach on deep canonical correlation
analysis (DCCA) that suffers from event sparseness. We overcome the sparseness
of positive labels by first learning a data sampling model from the labelled
data and by applying DCCA on the output it produces. This method that we term
balanced DCCA (b-DCCA) improves the performance of the unsupervised embeddings
on the downstream supervised audio detection task compared to classsical DCCA.
Because data labels are frequently imbalanced, our method might be of broad
utility in low-resource scenarios.",,
Low-Resource Mongolian Speech Synthesis Based on Automatic Prosody Annotation,"[arxiv.Result.Author('Xin Yuan'), arxiv.Result.Author('Robin Feng'), arxiv.Result.Author('Mingming Ye')]",2022-11-17 06:33:55+00:00,"While deep learning-based text-to-speech (TTS) models such as VITS have shown
excellent results, they typically require a sizable set of high-quality <text,
audio> pairs to train, which is expensive to collect. So far, most languages in
the world still lack the training data needed to develop TTS systems. This
paper proposes two improvement methods for the two problems faced by
low-resource Mongolian speech synthesis: a) In view of the lack of high-quality
<text, audio> pairs of data, it is difficult to model the mapping problem from
linguistic features to acoustic features. Improvements are made using
pre-trained VITS model and transfer learning methods. b) In view of the problem
of less labeled information, this paper proposes to use an automatic prosodic
annotation method to label the prosodic information of text and corresponding
speech, thereby improving the naturalness and intelligibility of low-resource
Mongolian language. Through empirical research, the N-MOS of the method
proposed in this paper is 4.195, and the I-MOS is 4.228.",Accepted by NCMMSC 2022,
SpectNet : End-to-End Audio Signal Classification Using Learnable Spectrograms,"[arxiv.Result.Author('Md. Istiaq Ansari'), arxiv.Result.Author('Taufiq Hasan')]",2022-11-17 05:29:03+00:00,"Pattern recognition from audio signals is an active research topic
encompassing audio tagging, acoustic scene classification, music
classification, and other areas. Spectrogram and mel-frequency cepstral
coefficients (MFCC) are among the most commonly used features for audio signal
analysis and classification. Recently, deep convolutional neural networks (CNN)
have been successfully used for audio classification problems using
spectrogram-based 2D features. In this paper, we present SpectNet, an
integrated front-end layer that extracts spectrogram features within a CNN
architecture that can be used for audio pattern recognition tasks. The
front-end layer utilizes learnable gammatone filters that are initialized using
mel-scale filters. The proposed layer outputs a 2D spectrogram image which can
be fed into a 2D CNN for classification. The parameters of the entire network,
including the front-end filterbank, can be updated via back-propagation. This
training scheme allows for fine-tuning the spectrogram-image features according
to the target audio dataset. The proposed method is evaluated in two different
audio signal classification tasks: heart sound anomaly detection and acoustic
scene classification. The proposed method shows a significant 1.02\%
improvement in MACC for the heart sound classification task and 2.11\%
improvement in accuracy for the acoustic scene classification task compared to
the classical spectrogram image features. The source code of our experiments
can be found at \url{https://github.com/mHealthBuet/SpectNet}",,
Unsupervised Model-based speaker adaptation of end-to-end lattice-free MMI model for speech recognition,"[arxiv.Result.Author('Xurong Xie'), arxiv.Result.Author('Xunying Liu'), arxiv.Result.Author('Hui Chen'), arxiv.Result.Author('Hongan Wang')]",2022-11-17 03:04:16+00:00,"Modeling the speaker variability is a key challenge for automatic speech
recognition (ASR) systems. In this paper, the learning hidden unit
contributions (LHUC) based adaptation techniques with compact speaker dependent
(SD) parameters are used to facilitate both speaker adaptive training (SAT) and
unsupervised test-time speaker adaptation for end-to-end (E2E) lattice-free MMI
(LF-MMI) models. An unsupervised model-based adaptation framework is proposed
to estimate the SD parameters in E2E paradigm using LF-MMI and cross entropy
(CE) criterions. Various regularization methods of the standard LHUC
adaptation, e.g., the Bayesian LHUC (BLHUC) adaptation, are systematically
investigated to mitigate the risk of overfitting, on E2E LF-MMI CNN-TDNN and
CNN-TDNN-BLSTM models. Lattice-based confidence score estimation is used for
adaptation data selection to reduce the supervision label uncertainty.
Experiments on the 300-hour Switchboard task suggest that applying BLHUC in the
proposed unsupervised E2E adaptation framework to byte pair encoding (BPE)
based E2E LF-MMI systems consistently outperformed the baseline systems by
relative word error rate (WER) reductions up to 10.5% and 14.7% on the NIST
Hub5'00 and RT03 evaluation sets, and achieved the best performance in WERs of
9.0% and 9.7%, respectively. These results are comparable to the results of
state-of-the-art adapted LF-MMI hybrid systems and adapted Conformer-based E2E
systems.","6 pages, 2 figures, submitted to ICASSP 2023",
Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning,"[arxiv.Result.Author('Brian Testa'), arxiv.Result.Author('Yi Xiao'), arxiv.Result.Author('Avery Gump'), arxiv.Result.Author('Asif Salekin')]",2022-11-17 00:25:05+00:00,"Emotional Surveillance is an emerging area with wide-reaching privacy
concerns. These concerns are exacerbated by ubiquitous IoT devices with
multiple sensors that can support these surveillance use cases. The work
presented here considers one such use case: the use of a speech emotion
recognition (SER) classifier tied to a smart speaker. This work demonstrates
the ability to evade black-box SER classifiers tied to a smart speaker without
compromising the utility of the smart speaker. This privacy concern is
considered through the lens of adversarial evasion of machine learning. Our
solution, Defeating Acoustic Recognition of Emotion via Genetic Programming
(DARE-GP), uses genetic programming to generate non-invasive additive audio
perturbations (AAPs). By constraining the evolution of these AAPs,
transcription accuracy can be protected while simultaneously degrading SER
classifier performance. The additive nature of these AAPs, along with an
approach that generates these AAPs for a fixed set of users in an utterance and
user location-independent manner, supports real-time, real-world evasion of SER
classifiers. DARE-GP's use of spectral features, which underlay the emotional
content of speech, allows the transferability of AAPs to previously unseen
black-box SER classifiers. Further, DARE-GP outperforms state-of-the-art SER
evasion techniques and is robust against defenses employed by a knowledgeable
adversary. The evaluations in this work culminate with acoustic evaluations
against two off-the-shelf commercial smart speakers, where a single AAP could
evade a black box classifier over 70% of the time. The final evaluation
deployed AAP playback on a small-form-factor system (raspberry pi) integrated
with a wake-word system to evaluate the efficacy of a real-world, real-time
deployment where DARE-GP is automatically invoked with the smart speaker's wake
word.",,
A Graph-Based Context-Aware Model to Understand Online Conversations,"[arxiv.Result.Author('Vibhor Agarwal'), arxiv.Result.Author('Anthony P. Young'), arxiv.Result.Author('Sagar Joglekar'), arxiv.Result.Author('Nishanth Sastry')]",2022-11-16 20:51:45+00:00,"Online forums that allow for participatory engagement between users have been
transformative for the public discussion of many important issues. However,
such conversations can sometimes escalate into full-blown exchanges of hate and
misinformation. Existing approaches in natural language processing (NLP), such
as deep learning models for classification tasks, use as inputs only a single
comment or a pair of comments depending upon whether the task concerns the
inference of properties of the individual comments or the replies between pairs
of comments, respectively. But in online conversations, comments and replies
may be based on external context beyond the immediately relevant information
that is input to the model. Therefore, being aware of the conversations'
surrounding contexts should improve the model's performance for the inference
task at hand.
  We propose GraphNLI, a novel graph-based deep learning architecture that uses
graph walks to incorporate the wider context of a conversation in a principled
manner. Specifically, a graph walk starts from a given comment and samples
""nearby"" comments in the same or parallel conversation threads, which results
in additional embeddings that are aggregated together with the initial
comment's embedding. We then use these enriched embeddings for downstream NLP
prediction tasks that are important for online conversations. We evaluate
GraphNLI on two such tasks - polarity prediction and misogynistic hate speech
detection - and found that our model consistently outperforms all relevant
baselines for both tasks. Specifically, GraphNLI with a biased root-seeking
random walk performs with a macro-F1 score of 3 and 6 percentage points better
than the best-performing BERT-based baselines for the polarity prediction and
hate speech detection tasks, respectively.","25 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:2202.08175",
A Two-Stage Deep Representation Learning-Based Speech Enhancement Method Using Variational Autoencoder and Adversarial Training,"[arxiv.Result.Author('Yang Xiang'), arxiv.Result.Author('Jesper Lisby Højvang'), arxiv.Result.Author('Morten Højfeldt Rasmussen'), arxiv.Result.Author('Mads Græsbøll Christensen')]",2022-11-16 19:31:46+00:00,"This paper focuses on leveraging deep representation learning (DRL) for
speech enhancement (SE). In general, the performance of the deep neural network
(DNN) is heavily dependent on the learning of data representation. However, the
DRL's importance is often ignored in many DNN-based SE algorithms. To obtain a
higher quality enhanced speech, we propose a two-stage DRL-based SE method
through adversarial training. In the first stage, we disentangle different
latent variables because disentangled representations can help DNN generate a
better enhanced speech. Specifically, we use the $\beta$-variational
autoencoder (VAE) algorithm to obtain the speech and noise posterior
estimations and related representations from the observed signal. However,
since the posteriors and representations are intractable and we can only apply
a conditional assumption to estimate them, it is difficult to ensure that these
estimations are always pretty accurate, which may potentially degrade the final
accuracy of the signal estimation. To further improve the quality of enhanced
speech, in the second stage, we introduce adversarial training to reduce the
effect of the inaccurate posterior towards signal reconstruction and improve
the signal estimation accuracy, making our algorithm more robust for the
potentially inaccurate posterior estimations. As a result, better SE
performance can be achieved. The experimental results indicate that the
proposed strategy can help similar DNN-based SE algorithms achieve higher
short-time objective intelligibility (STOI), perceptual evaluation of speech
quality (PESQ), and scale-invariant signal-to-distortion ratio (SI-SDR) scores.
Moreover, the proposed algorithm can also outperform recent competitive SE
algorithms.","Submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing",
Psychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter,"[arxiv.Result.Author('Yi Xiao'), arxiv.Result.Author('Harshit Sharma'), arxiv.Result.Author('Victoria Tumanova'), arxiv.Result.Author('Asif Salekin')]",2022-11-16 18:12:15+00:00,"This first-of-its-kind paper presents a novel approach named PASAD that
detects changes in perceptually fluent speech acoustics of young children.
Particularly, analysis of perceptually fluent speech enables identifying the
speech-motor-control factors that are considered as the underlying cause of
stuttering disfluencies. Recent studies indicate that the speech production of
young children, especially those who stutter, may get adversely affected by
situational physiological arousal. A major contribution of this paper is
leveraging the speaker's situational physiological responses in real-time to
analyze the speech signal effectively. The presented PASAD approach adapts a
Hyper-Network structure to extract temporal speech importance information
leveraging physiological parameters. In addition, a novel non-local acoustic
spectrogram feature extraction network identifies meaningful acoustic
attributes. Finally, a sequential network utilizes the acoustic attributes and
the extracted temporal speech importance for effective classification. We
collected speech and physiological sensing data from 73 preschool-age children
who stutter (CWS) and who don't stutter (CWNS) in different conditions. PASAD's
unique architecture enables visualizing speech attributes distinct to a CWS's
fluent speech and mapping them to the speaker's respective speech-motor-control
factors (i.e., speech articulators). Extracted knowledge can enhance
understanding of children's fluent speech, speech-motor-control (SMC), and
stuttering development. Our comprehensive evaluation shows that PASAD
outperforms state-of-the-art multi-modal baseline approaches in different
conditions, is expressive and adaptive to the speaker's speech and physiology,
generalizable, robust, and is real-time executable on mobile and scalable
devices.","20 pages, 5 figures",
A Review of Intelligent Music Generation Systems,"[arxiv.Result.Author('Ziyi Zhao'), arxiv.Result.Author('Hanwei Liu'), arxiv.Result.Author('Song Li'), arxiv.Result.Author('Junwei Pang'), arxiv.Result.Author('Maoqing Zhang'), arxiv.Result.Author('Yi Qin'), arxiv.Result.Author('Lei Wang'), arxiv.Result.Author('Qidi Wu')]",2022-11-16 13:43:16+00:00,"Intelligent music generation, one of the most popular subfields of computer
creativity, can lower the creative threshold for non-specialists and increase
the efficiency of music creation. In the last five years, the quality of
algorithm-based automatic music generation has increased significantly,
motivated by the use of modern generative algorithms to learn the patterns
implicit within a piece of music based on rule constraints or a musical corpus,
thus generating music samples in various styles. Some of the available
literature reviews lack a systematic benchmark of generative models and are
traditional and conservative in their perspective, resulting in a vision of the
future development of the field that is not deeply integrated with the current
rapid scientific progress. In this paper, we conduct a comprehensive survey and
analysis of recent intelligent music generation techniques,provide a critical
discussion, explicitly identify their respective characteristics, and present
them in a general table. We first introduce how music as a stream of
information is encoded and the relevant datasets, then compare different types
of generation algorithms, summarize their strengths and weaknesses, and discuss
existing methods for evaluation. Finally, the development of artificial
intelligence in composition is studied, especially by comparing the different
characteristics of music generation techniques in the East and West and
analyzing the development prospects in this field.","Overall 24 Pages, 11 Figures, 2 Tables, 96 References items",
McNet: Fuse Multiple Cues for Multichannel Speech Enhancement,"[arxiv.Result.Author('Yujie Yang'), arxiv.Result.Author('Changsheng Quan'), arxiv.Result.Author('Xiaofei Li')]",2022-11-16 12:25:54+00:00,"In multichannel speech enhancement, both spectral and spatial information are
vital for discriminating between speech and noise. How to fully exploit these
two types of information and their temporal dynamics remains an interesting
research problem. As a solution to this problem, this paper proposes a
multi-cue fusion network named McNet, which cascades four modules to
respectively exploit the full-band spatial, narrow-band spatial, sub-band
spectral, and full-band spectral information. Experiments show that each module
in the proposed network has its unique contribution and, as a whole, notably
outperforms other state-of-the-art methods.",submitted to icassp 2023,
Delivering Speaking Style in Low-resource Voice Conversion with Multi-factor Constraints,"[arxiv.Result.Author('Zhichao Wang'), arxiv.Result.Author('Xinsheng Wang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Yuanzhe Chen'), arxiv.Result.Author('Qiao Tian'), arxiv.Result.Author('Yuping Wang')]",2022-11-16 12:06:12+00:00,"Conveying the linguistic content and maintaining the source speech's speaking
style, such as intonation and emotion, is essential in voice conversion (VC).
However, in a low-resource situation, where only limited utterances from the
target speaker are accessible, existing VC methods are hard to meet this
requirement and capture the target speaker's timber. In this work, a novel VC
model, referred to as MFC-StyleVC, is proposed for the low-resource VC task.
Specifically, speaker timbre constraint generated by clustering method is newly
proposed to guide target speaker timbre learning in different stages.
Meanwhile, to prevent over-fitting to the target speaker's limited data,
perceptual regularization constraints explicitly maintain model performance on
specific aspects, including speaking style, linguistic content, and speech
quality. Besides, a simulation mode is introduced to simulate the inference
process to alleviate the mismatch between training and inference. Extensive
experiments performed on highly expressive speech demonstrate the superiority
of the proposed method in low-resource VC.",Submitted to ICASSP 2023,
L2 proficiency assessment using self-supervised speech representations,"[arxiv.Result.Author('Stefano Bannò'), arxiv.Result.Author('Kate M. Knill'), arxiv.Result.Author('Marco Matassoni'), arxiv.Result.Author('Vyas Raina'), arxiv.Result.Author('Mark J. F. Gales')]",2022-11-16 11:47:20+00:00,"There has been a growing demand for automated spoken language assessment
systems in recent years. A standard pipeline for this process is to start with
a speech recognition system and derive features, either hand-crafted or based
on deep-learning, that exploit the transcription and audio. Though these
approaches can yield high performance systems, they require speech recognition
systems that can be used for L2 speakers, and preferably tuned to the specific
form of test being deployed. Recently a self-supervised speech representation
based scheme, requiring no speech recognition, was proposed. This work extends
the initial analysis conducted on this approach to a large scale proficiency
test, Linguaskill, that comprises multiple parts, each designed to assess
different attributes of a candidate's speaking proficiency. The performance of
the self-supervised, wav2vec 2.0, system is compared to a high performance
hand-crafted assessment system and a BERT-based text system both of which use
speech transcriptions. Though the wav2vec 2.0 based system is found to be
sensitive to the nature of the response, it can be configured to yield
comparable performance to systems requiring a speech transcription, and yields
gains when appropriately combined with standard approaches.",,
Annotation of Soft Onsets in String Ensemble Recordings,"[arxiv.Result.Author('Maciej Tomczak'), arxiv.Result.Author('Min Susan Li'), arxiv.Result.Author('Adrian Bradbury'), arxiv.Result.Author('Mark Elliott'), arxiv.Result.Author('Ryan Stables'), arxiv.Result.Author('Maria Witek'), arxiv.Result.Author('Tom Goodman'), arxiv.Result.Author('Diar Abdlkarim'), arxiv.Result.Author('Massimiliano Di Luca'), arxiv.Result.Author('Alan Wing'), arxiv.Result.Author('Jason Hockman')]",2022-11-16 11:46:34+00:00,"Onset detection is the process of identifying the start points of musical
note events within an audio recording. While the detection of percussive onsets
is often considered a solved problem, soft onsets-as found in string instrument
recordings-still pose a significant challenge for state-of-the-art algorithms.
The problem is further exacerbated by a paucity of data containing expert
annotations and research related to best practices for curating soft onset
annotations for string instruments. To this end, we investigate inter-annotator
agreement between 24 participants, extend an algorithm for determining the most
consistent annotator, and compare the performance of human annotators and
state-of-the-art onset detection algorithms. Experimental results reveal a
positive trend between musical experience and both inter-annotator agreement
and performance in comparison with automated systems. Additionally, onsets
produced by changes in fingering as well as those from the cello were found to
be particularly challenging for both human annotators and automatic approaches.
To promote research in best practices for annotation of soft onsets, we have
made all experimental data associated with this study publicly available. In
addition, we publish the ARME Virtuoso Strings dataset, consisting of over 144
recordings of professional performances of an excerpt from Haydn's string
quartet Op. 74 No. 1 Finale, each with corresponding individual instrumental
onset annotations.",,
Data Augmentation with Unsupervised Speaking Style Transfer for Speech Emotion Recognition,"[arxiv.Result.Author('Leyuan Qu'), arxiv.Result.Author('Wei Wang'), arxiv.Result.Author('Taihao Li'), arxiv.Result.Author('Cornelius Weber'), arxiv.Result.Author('Stefan Wermter'), arxiv.Result.Author('Fuji Ren')]",2022-11-16 11:43:25+00:00,"Currently, the performance of Speech Emotion Recognition (SER) systems is
mainly constrained by the absence of large-scale labelled corpora. Data
augmentation is regarded as a promising approach, which borrows methods from
Automatic Speech Recognition (ASR), for instance, perturbation on speed and
pitch, or generating emotional speech utilizing generative adversarial
networks. In this paper, we propose EmoAug, a novel style transfer model to
augment emotion expressions, in which a semantic encoder and a paralinguistic
encoder represent verbal and non-verbal information respectively. Additionally,
a decoder reconstructs speech signals by conditioning on the aforementioned two
information flows in an unsupervised fashion. Once training is completed,
EmoAug enriches expressions of emotional speech in different prosodic
attributes, such as stress, rhythm and intensity, by feeding different styles
into the paralinguistic encoder. In addition, we can also generate similar
numbers of samples for each class to tackle the data imbalance issue.
Experimental results on the IEMOCAP dataset demonstrate that EmoAug can
successfully transfer different speaking styles while retaining the speaker
identity and semantic content. Furthermore, we train a SER model with data
augmented by EmoAug and show that it not only surpasses the state-of-the-art
supervised and self-supervised methods but also overcomes overfitting problems
caused by data imbalance. Some audio samples can be found on our demo website.",,
On using the UA-Speech and TORGO databases to validate automatic dysarthric speech classification approaches,"[arxiv.Result.Author('Guilherme Schu'), arxiv.Result.Author('Parvaneh Janbakhshi'), arxiv.Result.Author('Ina Kodrasi')]",2022-11-16 11:16:42+00:00,"Although the UA-Speech and TORGO databases of control and dysarthric speech
are invaluable resources made available to the research community with the
objective of developing robust automatic speech recognition systems, they have
also been used to validate a considerable number of automatic dysarthric speech
classification approaches. Such approaches typically rely on the underlying
assumption that recordings from control and dysarthric speakers are collected
in the same noiseless environment using the same recording setup. In this
paper, we show that this assumption is violated for the UA-Speech and TORGO
databases. Using voice activity detection to extract speech and non-speech
segments, we show that the majority of state-of-the-art dysarthria
classification approaches achieve the same or a considerably better performance
when using the non-speech segments of these databases than when using the
speech segments. These results demonstrate that such approaches trained and
validated on the UA-Speech and TORGO databases are potentially learning
characteristics of the recording environment or setup rather than dysarthric
speech characteristics. We hope that these results raise awareness in the
research community about the importance of the quality of recordings when
developing and evaluating automatic dysarthria classification approaches.",Submitted to ICASSP 2023,
Codebook-Based User Tracking in IRS-Assisted mmWave Communication Networks,"[arxiv.Result.Author('Moritz Garkisch'), arxiv.Result.Author('Vahid Jamali'), arxiv.Result.Author('Robert Schober')]",2022-11-16 10:40:42+00:00,"In this paper, we present a novel mobile user tracking (UT) scheme for
codebook-based intelligent reflecting surface (IRS) aided millimeter wave
(mmWave) systems. The proposed UT scheme exploits the temporal correlation of
the direction from the IRS to the mobile user for selecting IRS phase shifts
that provide reflection towards the user. To this end, the user's direction is
periodically estimated based on a generalized likelihood ratio test (GLRT) and
the user's movement trajectory is extrapolated from several past direction
estimates. The efficiency of the proposed UT scheme is evaluated in terms of
the average effective rate, which accounts for both the required signaling
overhead and the achieved signal to noise ratio (SNR). Our results show that
for medium to high SNR, the proposed codebook based UT scheme achieves a higher
effective rate than two reference approaches based on full codebook search and
optimization of the individual IRS unit cells, respectively.","Has been submitted to 2023 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP 2023)",
Structural Segmentation and Labeling of Tabla Solo Performances,"[arxiv.Result.Author('Gowriprasad R'), arxiv.Result.Author('R Aravind'), arxiv.Result.Author('Hema A Murthy')]",2022-11-16 09:34:04+00:00,"Tabla is a North Indian percussion instrument used as an accompaniment and an
exclusive instrument for solo performances. Tabla solo is intricate and
elaborate, exhibiting rhythmic evolution through a sequence of homogeneous
sections marked by shared rhythmic characteristics. Each section has a specific
structure and name associated with it. Tabla learning and performance in the
Indian subcontinent is based on stylistic schools called gharana-s. Several
compositions by various composers from different gharana-s are played in each
section. This paper addresses the task of segmenting the tabla solo concert
into musically meaningful sections. We then assign suitable section labels and
recognize gharana-s from the sections. We present a diverse collection of over
38 hours of solo tabla recordings for the task. We motivate the problem and
present different challenges and facets of the tasks. Inspired by the distinct
musical properties of tabla solo, we compute several rhythmic and timbral
features for the segmentation task. This work explores the approach of
automatically locating the significant changes in the rhythmic structure by
analyzing local self-similarity in an unsupervised manner. We also explore
supervised random forest and a convolutional neural network trained on
hand-crafted features. Both supervised and unsupervised approaches are also
tested on a set of held-out recordings. Segmentation of an audio piece into its
structural components and labeling is crucial to many music information
retrieval applications like repetitive structure finding, audio summarization,
and fast music navigation. This work helps us obtain a comprehensive musical
description of the tabla solo concert.","35 pages, 11 figures",
Speaker Adaptation for End-To-End Speech Recognition Systems in Noisy Environments,"[arxiv.Result.Author('Dominik Wagner'), arxiv.Result.Author('Ilja Baumann'), arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Korbinian Riedhammer'), arxiv.Result.Author('Tobias Bocklet')]",2022-11-16 09:02:41+00:00,"We analyze the impact of speaker adaptation in end-to-end architectures based
on transformers and wav2vec 2.0 under different noise conditions. We
demonstrate that the proven method of concatenating speaker vectors to the
acoustic features and supplying them as an auxiliary model input remains a
viable option to increase the robustness of end-to-end architectures. By
including speaker embeddings obtained from x-vector and ECAPA-TDNN models, we
achieve relative word error rate improvements of up to 9.6% on LibriSpeech and
up to 14.5% on Switchboard. The effect on transformer-based architectures is
approximately inversely proportional to the signal-to-noise ratio (SNR) and is
strongest in heavily noised environments ($SNR=0$). The most substantial
benefit of speaker adaption in systems based on wav2vec 2.0 can be achieved
under moderate noise conditions ($SNR\geq18$). We also find that x-vectors tend
to yield larger improvements than ECAPA-TDNN embeddings.",Submitted to ICASSP 2023,
Array Configuration-Agnostic Personalized Speech Enhancement using Long-Short-Term Spatial Coherence,"[arxiv.Result.Author('Yicheng Hsu'), arxiv.Result.Author('Yonghan Lee'), arxiv.Result.Author('Mingsian R. Bai')]",2022-11-16 08:15:56+00:00,"Personalized speech enhancement has been a field of active research for
suppression of speechlike interferers such as competing speakers or TV
dialogues. Compared with single channel approaches, multichannel PSE systems
can be more effective in adverse acoustic conditions by leveraging the spatial
information in microphone signals. However, the implementation of multichannel
PSEs to accommodate a wide range of array topology in household applications
can be challenging. To develop an array configuration agnostic PSE system, we
define a spatial feature termed the long short term spatial coherence as the
input feature to a convolutional recurrent network to monitor the voice
activity of the target speaker. As another refinement, an equivalent
rectangular bandwidth scaled LSTSC feature can be used to reduce the
computational cost. Experiments were conducted to compare the proposed PSE
systems, including the complete and the simplified versions with two baselines
using unseen room responses and array configurations in the presence of TV
noise and competing speakers. The results demonstrated that the proposed
multichannel PSE network trained with the LSTSC feature achieved superior
enhancement performance without precise knowledge of the array configurations
and room responses.",,
Comparing Subjective Perceptions of Robot-to-Human Handover Trajectories,"[arxiv.Result.Author('Alexander Calvert'), arxiv.Result.Author('Wesley Chan'), arxiv.Result.Author('Tin Tran'), arxiv.Result.Author('Sara Sheikholeslami'), arxiv.Result.Author('Rhys Newbury'), arxiv.Result.Author('Akansel Cosgun'), arxiv.Result.Author('Elizabeth Croft')]",2022-11-16 07:48:23+00:00,"Robots must move legibly around people for safety reasons, especially for
tasks where physical contact is possible. One such task is handovers, which
requires implicit communication on where and when physical contact (object
transfer) occurs. In this work, we study whether the trajectory model used by a
robot during the reaching phase affects the subjective perceptions of receivers
for robot-to-human handovers. We conducted a user study where 32 participants
were handed over three objects with four trajectory models: three were versions
of a minimum jerk trajectory, and one was an ellipse-fitting-based trajectory.
The start position of the handover was fixed for all trajectories, and the end
position was allowed to vary randomly around a fixed position by $\pm$3 cm in
all axis. The user study found no significant differences among the handover
trajectories in survey questions relating to safety, predictability,
naturalness, and other subjective metrics. While these results seemingly reject
the hypothesis that the trajectory affects human perceptions of a handover, it
prompts future research to investigate the effect of other variables, such as
robot speed, object transfer position, object orientation at the transfer
point, and explicit communication signals such as gaze and speech.","Submitted to Australasian Conference on Robotics and Automation 2022.
  9 pages, 4 figures",
Streaming Joint Speech Recognition and Disfluency Detection,"[arxiv.Result.Author('Hayato Futami'), arxiv.Result.Author('Emiru Tsunoo'), arxiv.Result.Author('Kentaro Shibata'), arxiv.Result.Author('Yosuke Kashiwagi'), arxiv.Result.Author('Takao Okuda'), arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-16 07:34:20+00:00,"Disfluency detection has mainly been solved in a pipeline approach, as
post-processing of speech recognition. In this study, we propose
Transformer-based encoder-decoder models that jointly solve speech recognition
and disfluency detection, which work in a streaming manner. Compared to
pipeline approaches, the joint models can leverage acoustic information that
makes disfluency detection robust to recognition errors and provide non-verbal
clues. Moreover, joint modeling results in low-latency and lightweight
inference. We investigate two joint model variants for streaming disfluency
detection: a transcript-enriched model and a multi-task model. The
transcript-enriched model is trained on text with special tags indicating the
starting and ending points of the disfluent part. However, it has problems with
latency and standard language model adaptation, which arise from the additional
disfluency tags. We propose a multi-task model to solve such problems, which
has two output layers at the Transformer decoder; one for speech recognition
and the other for disfluency detection. It is modeled to be conditioned on the
currently recognized token with an additional token-dependency mechanism. We
show that the proposed joint models outperformed a BERT-based pipeline approach
in both accuracy and latency, on both the Switchboard and the corpus of
spontaneous Japanese.",,
Conditional variational autoencoder to improve neural audio synthesis for polyphonic music sound,"[arxiv.Result.Author('Seokjin Lee'), arxiv.Result.Author('Minhan Kim'), arxiv.Result.Author('Seunghyeon Shin'), arxiv.Result.Author('Daeho Lee'), arxiv.Result.Author('Inseon Jang'), arxiv.Result.Author('Wootaek Lim')]",2022-11-16 07:11:56+00:00,"Deep generative models for audio synthesis have recently been significantly
improved. However, the task of modeling raw-waveforms remains a difficult
problem, especially for audio waveforms and music signals. Recently, the
realtime audio variational autoencoder (RAVE) method was developed for
high-quality audio waveform synthesis. The RAVE method is based on the
variational autoencoder and utilizes the two-stage training strategy.
Unfortunately, the RAVE model is limited in reproducing wide-pitch polyphonic
music sound. Therefore, to enhance the reconstruction performance, we adopt the
pitch activation data as an auxiliary information to the RAVE model. To handle
the auxiliary information, we propose an enhanced RAVE model with a conditional
variational autoencoder structure and an additional fully-connected layer. To
evaluate the proposed structure, we conducted a listening experiment based on
multiple stimulus tests with hidden references and an anchor (MUSHRA) with the
MAESTRO. The obtained results indicate that the proposed model exhibits a more
significant performance and stability improvement than the conventional RAVE
model.","5 pages, 6 figures",
Exploring Detection-based Method For Speaker Diarization @ Ego4D Audio-only Diarization Challenge 2022,"[arxiv.Result.Author('Jiahao Wang'), arxiv.Result.Author('Guo Chen'), arxiv.Result.Author('Yin-Dong Zheng'), arxiv.Result.Author('Tong Lu')]",2022-11-16 06:48:08+00:00,"We provide the technical report for Ego4D audio-only diarization challenge in
ECCV 2022. Speaker diarization takes the audio streams as input and outputs the
homogeneous segments according to the speaker's identity. It aims to solve the
problem of ""Who spoke when."" In this paper, we explore a Detection-based method
to tackle the audio-only speaker diarization task. Our method first extracts
audio features by audio backbone and then feeds the feature to a
detection-generate network to get the speaker proposals. Finally, after
postprocessing, we can get the diarization results. The validation dataset
validates this method, and our method achieves 53.85 DER on the test dataset.
These results rank 3rd on the leaderboard of Ego4D audio-only diarization
challenge 2022.",2 pages,
PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking,"[arxiv.Result.Author('Hanbo Cai'), arxiv.Result.Author('Pengcheng Zhang'), arxiv.Result.Author('Hai Dong'), arxiv.Result.Author('Yan Xiao'), arxiv.Result.Author('Shunhui Ji')]",2022-11-16 06:20:47+00:00,"Keyword spotting (KWS) has been widely used in various speech control
scenarios. The training of KWS is usually based on deep neural networks and
requires a large amount of data. Manufacturers often use third-party data to
train KWS. However, deep neural networks are not sufficiently interpretable to
manufacturers, and attackers can manipulate third-party training data to plant
backdoors during the model training. An effective backdoor attack can force the
model to make specified judgments under certain conditions, i.e., triggers. In
this paper, we design a backdoor attack scheme based on Pitch Boosting and
Sound Masking for KWS, called PBSM. Experimental results demonstrated that PBSM
is feasible to achieve an average attack success rate close to 90% in three
victim models when poisoning less than 1% of the training data.","5 pages, 4 figures",
MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation,"[arxiv.Result.Author('Dominik Macháček'), arxiv.Result.Author('Ondřej Bojar'), arxiv.Result.Author('Raj Dabre')]",2022-11-16 03:03:56+00:00,"There have been several studies on the correlation between human ratings and
metrics such as BLEU, chrF2 and COMET in machine translation. Most, if not all
consider full-sentence translation. It is unclear whether human ratings of
simultaneous speech translation Continuous Rating (CR) correlate with these
metrics or not. Therefore, we conduct an extensive correlation analysis of CR
and the aforementioned automatic metrics on evaluations of candidate systems at
English-German simultaneous speech translation task at IWSLT 2022. Our studies
reveal that the offline MT metrics correlate with CR and can be reliably used
for evaluating machine translation in the simultaneous mode, with some
limitations on the test set size. This implies that automatic metrics can be
used as proxies for CR, thereby alleviating the need for human evaluation.",Technical Report,
Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral Mapping for Single-channel Speech Enhancement,"[arxiv.Result.Author('Kuan-Lin Chen'), arxiv.Result.Author('Daniel D. E. Wong'), arxiv.Result.Author('Ke Tan'), arxiv.Result.Author('Buye Xu'), arxiv.Result.Author('Anurag Kumar'), arxiv.Result.Author('Vamsi Krishna Ithapu')]",2022-11-16 02:29:05+00:00,"Most speech enhancement (SE) models learn a point estimate, and do not make
use of uncertainty estimation in the learning process. In this paper, we show
that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian
negative log-likelihood (NLL) improves SE performance at no extra cost. During
training, our approach augments a model learning complex spectral mapping with
a temporary submodel to predict the covariance of the enhancement error at each
time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the
covariance introduces an undersampling effect, detrimental to SE performance.
To mitigate undersampling, our approach inflates the uncertainty lower bound
and weights each loss component with their uncertainty, effectively
compensating severely undersampled components with more penalties. Our
multivariate setting reveals common covariance assumptions such as scalar and
diagonal matrices. By weakening these assumptions, we show that the NLL
achieves superior performance compared to popular losses including the mean
squared error (MSE), mean absolute error (MAE), and scale-invariant
signal-to-distortion ratio (SI-SDR).",5 pages. Submitted to ICASSP 2023,
Hybrid Transformers for Music Source Separation,"[arxiv.Result.Author('Simon Rouard'), arxiv.Result.Author('Francisco Massa'), arxiv.Result.Author('Alexandre Défossez')]",2022-11-15 22:48:16+00:00,"A natural question arising in Music Source Separation (MSS) is whether long
range contextual information is useful, or whether local acoustic features are
sufficient. In other fields, attention based Transformers have shown their
ability to integrate information over long sequences. In this work, we
introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral
bi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a
cross-domain Transformer Encoder, using self-attention within one domain, and
cross-attention across domains. While it performs poorly when trained only on
MUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by
0.45 dB of SDR when using 800 extra training songs. Using sparse attention
kernels to extend its receptive field, and per source fine-tuning, we achieve
state-of-the-art results on MUSDB with extra training data, with 9.20 dB of
SDR.",,
Alzheimer's Dementia Detection through Spontaneous Dialogue with Proactive Robotic Listeners,"[arxiv.Result.Author('Yuanchao Li'), arxiv.Result.Author('Catherine Lai'), arxiv.Result.Author('Divesh Lala'), arxiv.Result.Author('Koji Inoue'), arxiv.Result.Author('Tatsuya Kawahara')]",2022-11-15 21:52:41+00:00,"As the aging of society continues to accelerate, Alzheimer's Disease (AD) has
received more and more attention from not only medical but also other fields,
such as computer science, over the past decade. Since speech is considered one
of the effective ways to diagnose cognitive decline, AD detection from speech
has emerged as a hot topic. Nevertheless, such approaches fail to tackle
several key issues: 1) AD is a complex neurocognitive disorder which means it
is inappropriate to conduct AD detection using utterance information alone
while ignoring dialogue information; 2) Utterances of AD patients contain many
disfluencies that affect speech recognition yet are helpful to diagnosis; 3) AD
patients tend to speak less, causing dialogue breakdown as the disease
progresses. This fact leads to a small number of utterances, which may cause
detection bias. Therefore, in this paper, we propose a novel AD detection
architecture consisting of two major modules: an ensemble AD detector and a
proactive listener. This architecture can be embedded in the dialogue system of
conversational robots for healthcare.",Accepted for HRI2022 Late-Breaking Report,
Introducing Semantics into Speech Encoders,"[arxiv.Result.Author('Derek Xu'), arxiv.Result.Author('Shuyan Dong'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Suyoun Kim'), arxiv.Result.Author('Zhaojiang Lin'), arxiv.Result.Author('Akshat Shrivastava'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Liang-Hsuan Tseng'), arxiv.Result.Author('Alexei Baevski'), arxiv.Result.Author('Guan-Ting Lin'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Yizhou Sun'), arxiv.Result.Author('Wei Wang')]",2022-11-15 18:44:28+00:00,"Recent studies find existing self-supervised speech encoders contain
primarily acoustic rather than semantic information. As a result, pipelined
supervised automatic speech recognition (ASR) to large language model (LLM)
systems achieve state-of-the-art results on semantic spoken language tasks by
utilizing rich semantic representations from the LLM. These systems come at the
cost of labeled audio transcriptions, which is expensive and time-consuming to
obtain. We propose a task-agnostic unsupervised way of incorporating semantic
information from LLMs into self-supervised speech encoders without labeled
audio transcriptions. By introducing semantics, we improve existing speech
encoder spoken language understanding performance by over 10\% on intent
classification, with modest gains in named entity resolution and slot filling,
and spoken question answering FF1 score by over 2\%. Our unsupervised approach
achieves similar performance as supervised methods trained on over 100 hours of
labeled audio transcripts, demonstrating the feasibility of unsupervised
semantic augmentations to existing speech encoders.","11 pages, 3 figures",
Music Instrument Classification Reprogrammed,"[arxiv.Result.Author('Hsin-Hung Chen'), arxiv.Result.Author('Alexander Lerch')]",2022-11-15 18:26:01+00:00,"The performance of approaches to Music Instrument Classification, a popular
task in Music Information Retrieval, is often impacted and limited by the lack
of availability of annotated data for training. We propose to address this
issue with ""reprogramming,"" a technique that utilizes pre-trained deep and
complex neural networks originally targeting a different task by modifying and
mapping both the input and output of the pre-trained model. We demonstrate that
reprogramming can effectively leverage the power of the representation learned
for a different task and that the resulting reprogrammed system can perform on
par or even outperform state-of-the-art systems at a fraction of training
parameters. Our results, therefore, indicate that reprogramming is a promising
technique potentially applicable to other tasks impeded by data scarcity.","Accepted at 29th International Conference on Multimedia Modeling
  (MMM23)",
FlowGrad: Using Motion for Visual Sound Source Localization,"[arxiv.Result.Author('Rajsuryan Singh'), arxiv.Result.Author('Pablo Zinemanas'), arxiv.Result.Author('Xavier Serra'), arxiv.Result.Author('Juan Pablo Bello'), arxiv.Result.Author('Magdalena Fuentes')]",2022-11-15 18:12:10+00:00,"Most recent work in visual sound source localization relies on semantic
audio-visual representations learned in a self-supervised manner, and by design
excludes temporal information present in videos. While it proves to be
effective for widely used benchmark datasets, the method falls short for
challenging scenarios like urban traffic. This work introduces temporal context
into the state-of-the-art methods for sound source localization in urban scenes
using optical flow as a means to encode motion information. An analysis of the
strengths and weaknesses of our methods helps us better understand the problem
of visual sound source localization and sheds light on open challenges for
audio-visual scene understanding.",Submitted to ICASSP 2023,
Reverberation as Supervision for Speech Separation,"[arxiv.Result.Author('Rohith Aralikatti'), arxiv.Result.Author('Christoph Boeddeker'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Aswin Shanmugam Subramanian'), arxiv.Result.Author('Jonathan Le Roux')]",2022-11-15 17:06:50+00:00,"This paper proposes reverberation as supervision (RAS), a novel unsupervised
loss function for single-channel reverberant speech separation. Prior methods
for unsupervised separation required the synthesis of mixtures of mixtures or
assumed the existence of a teacher model, making them difficult to consider as
potential methods explaining the emergence of separation abilities in an
animal's auditory system. We assume the availability of two-channel mixtures at
training time, and train a neural network to separate the sources given one of
the channels as input such that the other channel may be predicted from the
separated sources. As the relationship between the room impulse responses
(RIRs) of each channel depends on the locations of the sources, which are
unknown to the network, the network cannot rely on learning that relationship.
Instead, our proposed loss function fits each of the separated sources to the
mixture in the target channel via Wiener filtering, and compares the resulting
mixture to the ground-truth one. We show that minimizing the scale-invariant
signal-to-distortion ratio (SI-SDR) of the predicted right-channel mixture with
respect to the ground truth implicitly guides the network towards separating
the left-channel sources. On a semi-supervised reverberant speech separation
task based on the WHAMR! dataset, using training data where just 5% (resp.,
10%) of the mixtures are labeled with associated isolated sources, we achieve
70% (resp., 78%) of the SI-SDR improvement obtained when training with
supervision on the full training set, while a model trained only on the labeled
data obtains 43% (resp., 45%).","5 pages, 2 figures, 4 tables. Submitted to ICASSP 2023",
Is Style All You Need? Dependencies Between Emotion and GST-based Speaker Recognition,"[arxiv.Result.Author('Morgan Sandler'), arxiv.Result.Author('Arun Ross')]",2022-11-15 15:29:47+00:00,"In this work, we study the hypothesis that speaker identity embeddings
extracted from speech samples may be used for detection and classification of
emotion. In particular, we show that emotions can be effectively identified by
learning speaker identities by use of a 1-D Triplet Convolutional Neural
Network (CNN) & Global Style Token (GST) scheme (e.g., DeepTalk Network) and
reusing the trained speaker recognition model weights to generate features in
the emotion classification domain. The automatic speaker recognition (ASR)
network is trained with VoxCeleb1, VoxCeleb2, and Librispeech datasets with a
triplet training loss function using speaker identity labels. Using an Support
Vector Machine (SVM) classifier, we map speaker identity embeddings into
discrete emotion categories from the CREMA-D, IEMOCAP, and MSP-Podcast
datasets. On the task of speech emotion detection, we obtain 80.8% ACC with
acted emotion samples from CREMA-D, 81.2% ACC with semi-natural emotion samples
in IEMOCAP, and 66.9% ACC with natural emotion samples in MSP-Podcast. We also
propose a novel two-stage hierarchical classifier (HC) approach which
demonstrates +2% ACC improvement on CREMA-D emotion samples. Through this work,
we seek to convey the importance of holistically modeling intra-user variation
within audio samples",Submitted to ICASSP 2023,
Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder,"[arxiv.Result.Author('Yuying Xie'), arxiv.Result.Author('Thomas Arildsen'), arxiv.Result.Author('Zheng-Hua Tan')]",2022-11-15 14:55:28+00:00,"By utilizing the fact that speaker identity and content vary on different
time scales, \acrlong{fhvae} (\acrshort{fhvae}) uses a sequential latent
variable and a segmental latent variable to symbolize these two attributes.
Disentanglement is carried out by assuming the latent variables representing
speaker and content follow sequence-dependent and sequence-independent priors.
For the sequence-dependent prior, \acrshort{fhvae} assumes a Gaussian
distribution with an utterance-scale varying mean and a fixed small variance.
The training process promotes sequential variables getting close to the mean of
its prior with small variance. However, this constraint is relatively weak.
Therefore, we introduce contrastive learning in the \acrshort{fhvae} framework.
The proposed method aims to make the sequential variables clustering when
representing the same speaker, while distancing themselves as far as possible
from those of other speakers. The structure of the framework has not been
changed in the proposed method but only the training process, thus no more cost
is needed during test. Voice conversion has been chosen as the application in
this paper. Latent variable evaluations include speakerincrease verification
and identification for the sequential latent variable, and speech recognition
for the segmental latent variable. Furthermore, assessments of voice conversion
performance are on the grounds of speaker verification and speech recognition
experiments. Experiment results show that the proposed method improves both
sequential and segmental feature extraction compared with \acrshort{fhvae}, and
moderately improved voice conversion performance.",submitted to ICASSP 2023,
Type Information Utilized Event Detection via Multi-Channel GNNs in Electrical Power Systems,"[arxiv.Result.Author('Qian Li'), arxiv.Result.Author('Jianxin Li'), arxiv.Result.Author('Lihong Wang'), arxiv.Result.Author('Cheng Ji'), arxiv.Result.Author('Yiming Hei'), arxiv.Result.Author('Jiawei Sheng'), arxiv.Result.Author('Qingyun Sun'), arxiv.Result.Author('Shan Xue'), arxiv.Result.Author('Pengtao Xie')]",2022-11-15 14:22:27+00:00,"Event detection in power systems aims to identify triggers and event types,
which helps relevant personnel respond to emergencies promptly and facilitates
the optimization of power supply strategies. However, the limited length of
short electrical record texts causes severe information sparsity, and numerous
domain-specific terminologies of power systems makes it difficult to transfer
knowledge from language models pre-trained on general-domain texts. Traditional
event detection approaches primarily focus on the general domain and ignore
these two problems in the power system domain. To address the above issues, we
propose a Multi-Channel graph neural network utilizing Type information for
Event Detection in power systems, named MC-TED, leveraging a semantic channel
and a topological channel to enrich information interaction from short texts.
Concretely, the semantic channel refines textual representations with semantic
similarity, building the semantic information interaction among potential
event-related words. The topological channel generates a relation-type-aware
graph modeling word dependencies, and a word-type-aware graph integrating
part-of-speech tags. To further reduce errors worsened by professional
terminologies in type analysis, a type learning mechanism is designed for
updating the representations of both the word type and relation type in the
topological channel. In this way, the information sparsity and professional
term occurrence problems can be alleviated by enabling interaction between
topological and semantic information. Furthermore, to address the lack of
labeled data in power systems, we built a Chinese event detection dataset based
on electrical Power Event texts, named PoE. In experiments, our model achieves
compelling results not only on the PoE dataset, but on general-domain event
detection datasets including ACE 2005 and MAVEN.",,
Exploring the Joint Use of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding,"[arxiv.Result.Author('Umberto Cappellazzo'), arxiv.Result.Author('Daniele Falavigna'), arxiv.Result.Author('Alessio Brutti')]",2022-11-15 14:15:22+00:00,"Continual learning refers to a dynamical framework in which a model or agent
receives a stream of non-stationary data over time and must adapt to new data
while preserving previously acquired knowledge. Unfortunately, deep neural
networks fail to meet these two desiderata, incurring the so-called
catastrophic forgetting phenomenon. Whereas a vast array of strategies have
been proposed to attenuate forgetting in the computer vision domain, for
speech-related tasks, on the other hand, there is a dearth of works. In this
paper, we turn our attention toward the joint use of rehearsal and knowledge
distillation (KD) approaches for spoken language understanding under a
class-incremental learning scenario. We report on multiple KD combinations at
different levels in the network, showing that combining feature-level and
predictions-level KDs leads to the best results. Finally, we provide an
ablation study on the effect of the size of the rehearsal memory that
corroborates the appropriateness of our approach for low-resource devices.",Submitted to ICASSP 2023,
SSM-Net: feature learning for Music Structure Analysis using a Self-Similarity-Matrix based loss,"[arxiv.Result.Author('Geoffroy Peeters'), arxiv.Result.Author('Florian Angulo')]",2022-11-15 13:48:11+00:00,"In this paper, we propose a new paradigm to learn audio features for Music
Structure Analysis (MSA). We train a deep encoder to learn features such that
the Self-Similarity-Matrix (SSM) resulting from those approximates a
ground-truth SSM. This is done by minimizing a loss between both SSMs. Since
this loss is differentiable w.r.t. its input features we can train the encoder
in a straightforward way. We successfully demonstrate the use of this training
paradigm using the Area Under the Curve ROC (AUC) on the RWC-Pop dataset.","Extended Abstracts for the Late-Breaking Demo Session of the 23rd
  Int. Society for Music Information Retrieval Conf., Bengaluru, India, 2022",
Hierarchical Pronunciation Assessment with Multi-Aspect Attention,"[arxiv.Result.Author('Heejin Do'), arxiv.Result.Author('Yunsu Kim'), arxiv.Result.Author('Gary Geunbae Lee')]",2022-11-15 12:49:35+00:00,"Automatic pronunciation assessment is a major component of a
computer-assisted pronunciation training system. To provide in-depth feedback,
scoring pronunciation at various levels of granularity such as phoneme, word,
and utterance, with diverse aspects such as accuracy, fluency, and
completeness, is essential. However, existing multi-aspect multi-granularity
methods simultaneously predict all aspects at all granularity levels;
therefore, they have difficulty in capturing the linguistic hierarchy of
phoneme, word, and utterance. This limitation further leads to neglecting
intimate cross-aspect relations at the same linguistic unit. In this paper, we
propose a Hierarchical Pronunciation Assessment with Multi-aspect Attention
(HiPAMA) model, which hierarchically represents the granularity levels to
directly capture their linguistic structures and introduces multi-aspect
attention that reflects associations across aspects at the same level to create
more connotative representations. By obtaining relational information from both
the granularity- and aspect-side, HiPAMA can take full advantage of multi-task
learning. Remarkable improvements in the experimental results on the
speachocean762 datasets demonstrate the robustness of HiPAMA, particularly in
the difficult-to-assess aspects.",Submitted to ICASSP 2023,
Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches,"[arxiv.Result.Author('Amirhossein Abaskohi'), arxiv.Result.Author('Nazanin Sabri'), arxiv.Result.Author('Behnam Bahrak')]",2022-11-15 10:22:49+00:00,"Emotion recognition is one of the machine learning applications which can be
done using text, speech, or image data gathered from social media spaces.
Detecting emotion can help us in different fields, including opinion mining.
With the spread of social media, different platforms like Twitter have become
data sources, and the language used in these platforms is informal, making the
emotion detection task difficult. EmoPars and ArmanEmo are two new
human-labeled emotion datasets for the Persian language. These datasets,
especially EmoPars, are suffering from inequality between several samples
between two classes. In this paper, we evaluate EmoPars and compare them with
ArmanEmo. Throughout this analysis, we use data augmentation techniques, data
re-sampling, and class-weights with Transformer-based Pretrained Language
Models(PLMs) to handle the imbalance problem of these datasets. Moreover,
feature selection is used to enhance the models' performance by emphasizing the
text's specific features. In addition, we provide a new policy for selecting
data from EmoPars, which selects the high-confidence samples; as a result, the
model does not see samples that do not have specific emotion during training.
Our model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and
EmoPars, respectively, which are new state-of-the-art results in these
benchmarks.","14 pages, 5 figures, 9 tables","ACM Transactions on Asian and Low-Resource Language Information
  Processing 2022"
Discourse and conversation impairments in patients with dementia,[arxiv.Result.Author('Charalambos Themistocleous')],2022-11-15 08:18:30+00:00,"Neurodegeneration characterizes individuals with different dementia subtypes
(e.g., individuals with Alzheimer's Disease, Primary Progressive Aphasia, and
Parkinson's Disease), leading to progressive decline in cognitive, linguistic,
and social functioning. Speech and language impairments are early symptoms in
individuals with focal forms of neurodegenerative conditions, coupled with
deficits in cognitive, social, and behavioral domains. This paper reviews the
findings on language and communication deficits and identifies the effects of
dementia on the production and perception of discourse. It discusses findings
concerning (i) language function, cognitive representation, and impairment,
(ii) communicative competence, emotions, empathy, and theory-of-mind, and (iii)
speech-in-interaction. It argues that clinical discourse analysis can provide a
comprehensive assessment of language and communication skills in individuals,
which complements the existing neurolinguistic evaluation for (differential)
diagnosis, prognosis, and treatment efficacy evaluation.",Book chapter,
Show Me the Instruments: Musical Instrument Retrieval from Mixture Audio,"[arxiv.Result.Author('Kyungsu Kim'), arxiv.Result.Author('Minju Park'), arxiv.Result.Author('Haesun Joung'), arxiv.Result.Author('Yunkee Chae'), arxiv.Result.Author('Yeongbeom Hong'), arxiv.Result.Author('Seonghyeon Go'), arxiv.Result.Author('Kyogu Lee')]",2022-11-15 07:32:39+00:00,"As digital music production has become mainstream, the selection of
appropriate virtual instruments plays a crucial role in determining the quality
of music. To search the musical instrument samples or virtual instruments that
make one's desired sound, music producers use their ears to listen and compare
each instrument sample in their collection, which is time-consuming and
inefficient. In this paper, we call this task as Musical Instrument Retrieval
and propose a method for retrieving desired musical instruments using reference
music mixture as a query. The proposed model consists of the Single-Instrument
Encoder and the Multi-Instrument Encoder, both based on convolutional neural
networks. The Single-Instrument Encoder is trained to classify the instruments
used in single-track audio, and we take its penultimate layer's activation as
the instrument embedding. The Multi-Instrument Encoder is trained to estimate
multiple instrument embeddings using the instrument embeddings computed by the
Single-Instrument Encoder as a set of target embeddings. For more generalized
training and realistic evaluation, we also propose a new dataset called Nlakh.
Experimental results showed that the Single-Instrument Encoder was able to
learn the mapping from the audio signal of unseen instruments to the instrument
embedding space and the Multi-Instrument Encoder was able to extract multiple
embeddings from the mixture of music and retrieve the desired instruments
successfully. The code used for the experiment and audio samples are available
at: https://github.com/minju0821/musical_instrument_retrieval","5 pages, 4 figures, submitted to ICASSP 2023",
Music Similarity Calculation of Individual Instrumental Sounds Using Metric Learning,"[arxiv.Result.Author('Yuka Hashizume'), arxiv.Result.Author('Li Li'), arxiv.Result.Author('Tomoki Toda')]",2022-11-15 03:03:22+00:00,"The criteria for measuring music similarity are important for developing a
flexible music recommendation system. Some data-driven methods have been
proposed to calculate music similarity from only music signals, such as metric
learning based on a triplet loss using tag information on each musical piece.
However, the resulting music similarity metric usually captures the entire
piece of music, i.e., the mixing of various instrumental sound sources,
limiting the capability of the music recommendation system, e.g., it is
difficult to search for a musical piece containing similar drum sounds. Towards
the development of a more flexible music recommendation system, we propose a
music similarity calculation method that focuses on individual instrumental
sound sources in a musical piece. By fully exploiting the potential of
data-driven methods for our proposed method, we employ weakly supervised metric
learning to individual instrumental sound source signals without using any tag
information, where positive and negative samples in a triplet loss are defined
by whether or not they are from the same musical piece. Furthermore, assuming
that each instrumental sound source is not always available in practice, we
also investigate the effects of using instrumental sound source separation to
obtain each source in the proposed method. Experimental results have shown that
(1) unique similarity metrics can be learned for individual instrumental sound
sources, (2) similarity metrics learned using some instrumental sound sources
are possible to lead to more accurate results than that learned using the
entire musical piece, (3) the performance degraded when learning with the
separated instrumental sounds, and (4) similarity metrics learned by the
proposed method well produced results that correspond to perception by human
senses.",APSIPA ASC 2022 (pp.33--38),
Prompting Language Models for Linguistic Structure,"[arxiv.Result.Author('Terra Blevins'), arxiv.Result.Author('Hila Gonen'), arxiv.Result.Author('Luke Zettlemoyer')]",2022-11-15 01:13:39+00:00,"Although pretrained language models (PLMs) can be prompted to perform a wide
range of language tasks, it remains an open question how much this ability
comes from generalizable linguistic representations versus more surface-level
lexical patterns. To test this, we present a structured prompting approach that
can be used to prompt for linguistic structure prediction tasks, allowing us to
perform zero- and few-shot sequence tagging with autoregressive PLMs. We
evaluate this approach on part-of-speech tagging, named entity recognition, and
sentence chunking and demonstrate strong few-shot performance in all cases. We
also find that, though the surface forms of the tags provide some signal,
structured prompting can retrieve linguistic structure even with arbitrary
labels, indicating that PLMs contain this knowledge in a general manner robust
to label choice.",,
Rapid Connectionist Speaker Adaptation,"[arxiv.Result.Author('Michael Witbrock'), arxiv.Result.Author('Patrick Haffner')]",2022-11-15 00:15:11+00:00,"We present SVCnet, a system for modelling speaker variability. Encoder Neural
Networks specialized for each speech sound produce low dimensionality models of
acoustical variation, and these models are further combined into an overall
model of voice variability. A training procedure is described which minimizes
the dependence of this model on which sounds have been uttered. Using the
trained model (SVCnet) and a brief, unconstrained sample of a new speaker's
voice, the system produces a Speaker Voice Code that can be used to adapt a
recognition system to the new speaker without retraining. A system which
combines SVCnet with an MS-TDNN recognizer is described","6 Figures, Two Tables, ICASSP-92","ICASSP-92: 1992 IEEE International Conference on Acoustics,
  Speech, and Signal Processing, 1992, pp. 453-456 vol.1"
On Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and Model Calibration,"[arxiv.Result.Author('Nauman Dawalatabad'), arxiv.Result.Author('Sameer Khurana'), arxiv.Result.Author('Antoine Laurent'), arxiv.Result.Author('James Glass')]",2022-11-14 23:20:36+00:00,"Pseudo-label (PL) filtering forms a crucial part of Self-Training (ST)
methods for unsupervised domain adaptation. Dropout-based Uncertainty-driven
Self-Training (DUST) proceeds by first training a teacher model on source
domain labeled data. Then, the teacher model is used to provide PLs for the
unlabeled target domain data. Finally, we train a student on augmented labeled
and pseudo-labeled data. The process is iterative, where the student becomes
the teacher for the next DUST iteration. A crucial step that precedes the
student model training in each DUST iteration is filtering out noisy PLs that
could lead the student model astray. In DUST, we proposed a simple, effective,
and theoretically sound PL filtering strategy based on the teacher model's
uncertainty about its predictions on unlabeled speech utterances. We estimate
the model's uncertainty by computing disagreement amongst multiple samples
drawn from the teacher model during inference by injecting noise via dropout.
In this work, we show that DUST's PL filtering, as initially used, may fail
under severe source and target domain mismatch. We suggest several approaches
to eliminate or alleviate this issue. Further, we bring insights from the
research in neural network model calibration to DUST and show that a
well-calibrated model correlates strongly with a positive outcome of the DUST
PL filtering step.",,
Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations,"[arxiv.Result.Author('Renee Lu'), arxiv.Result.Author('Mostafa Shahin'), arxiv.Result.Author('Beena Ahmed')]",2022-11-14 22:03:36+00:00,"Children's speech recognition is a vital, yet largely overlooked domain when
building inclusive speech technologies. The major challenge impeding progress
in this domain is the lack of adequate child speech corpora; however, recent
advances in self-supervised learning have created a new opportunity for
overcoming this problem of data scarcity. In this paper, we leverage
self-supervised adult speech representations and use three well-known child
speech corpora to build models for children's speech recognition. We assess the
performance of fine-tuning on both native and non-native children's speech,
examine the effect of cross-domain child corpora, and investigate the minimum
amount of child speech required to fine-tune a model which outperforms a
state-of-the-art adult model. We also analyze speech recognition performance
across children's ages. Our results demonstrate that fine-tuning with
cross-domain child corpora leads to relative improvements of up to 46.08% and
45.53% for native and non-native child speech respectively, and absolute
improvements of 14.70% and 31.10%. We also show that with as little as 5 hours
of transcribed children's speech, it is possible to fine-tune a children's
speech recognition system that outperforms a state-of-the-art adult model
fine-tuned on 960 hours of adult speech.",Under-review @ Speech Communication Journal,
Describing emotions with acoustic property prompts for speech emotion recognition,"[arxiv.Result.Author('Hira Dhamyal'), arxiv.Result.Author('Benjamin Elizalde'), arxiv.Result.Author('Soham Deshmukh'), arxiv.Result.Author('Huaming Wang'), arxiv.Result.Author('Bhiksha Raj'), arxiv.Result.Author('Rita Singh')]",2022-11-14 20:29:37+00:00,"Emotions lie on a broad continuum and treating emotions as a discrete number
of classes limits the ability of a model to capture the nuances in the
continuum. The challenge is how to describe the nuances of emotions and how to
enable a model to learn the descriptions. In this work, we devise a method to
automatically create a description (or prompt) for a given audio by computing
acoustic properties, such as pitch, loudness, speech rate, and articulation
rate. We pair a prompt with its corresponding audio using 5 different emotion
datasets. We trained a neural network model using these audio-text pairs. Then,
we evaluate the model using one more dataset. We investigate how the model can
learn to associate the audio with the descriptions, resulting in performance
improvement of Speech Emotion Recognition and Speech Audio Retrieval. We expect
our findings to motivate research describing the broad continuum of emotion",,
The Birds Need Attention Too: Analysing usage of Self Attention in identifying bird calls in soundscapes,"[arxiv.Result.Author('Chandra Kanth Nagesh'), arxiv.Result.Author('Abhishek Purushothama')]",2022-11-14 19:48:45+00:00,"Birds are vital parts of ecosystems across the world and are an excellent
measure of the quality of life on earth. Many bird species are endangered while
others are already extinct. Ecological efforts in understanding and monitoring
bird populations are important to conserve their habitat and species, but this
mostly relies on manual methods in rough terrains. Recent advances in Machine
Learning and Deep Learning have made automatic bird recognition in diverse
environments possible. Birdcall recognition till now has been performed using
convolutional neural networks. In this work, we try and understand how
self-attention can aid in this endeavor. With that we build an pre-trained
Attention-based Spectrogram Transformer baseline for BirdCLEF 2022 and compare
the results against the pre-trained Convolution-based baseline. Our results
show that the transformer models outperformed the convolutional model and we
further validate our results by building baselines and analyzing the results
for the previous year BirdCLEF 2021 challenge. Source code available at
https://github.com/ck090/BirdCLEF-22","12 pages, 9 tables and 7 figures",
Is my automatic audio captioning system so bad? spider-max: a metric to consider several caption candidates,"[arxiv.Result.Author('Etienne Labbé'), arxiv.Result.Author('Thomas Pellegrini'), arxiv.Result.Author('Julien Pinquier')]",2022-11-14 19:16:45+00:00,"Automatic Audio Captioning (AAC) is the task that aims to describe an audio
signal using natural language. AAC systems take as input an audio signal and
output a free-form text sentence, called a caption. Evaluating such systems is
not trivial, since there are many ways to express the same idea. For this
reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr,
are used to compare a single automatic caption to one or several captions of
reference, produced by a human annotator. Nevertheless, an automatic system can
produce several caption candidates, either using some randomness in the
sentence generation process, or by considering the various competing
hypothesized captions during decoding with beam-search, for instance. If we
consider an end-user of an AAC system, presenting several captions instead of a
single one seems relevant to provide some diversity, similarly to information
retrieval systems. In this work, we explore the possibility to consider several
predicted captions in the evaluation process instead of one. For this purpose,
we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the
scores of several caption candidates. To advocate for our metric, we report
experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On
AudioCaps for example, this system reached a SPIDEr-max value (with 5
candidates) close to the SPIDEr human score of reference.","Workshop on Detection and Classification of Acoustic Scenes and
  Events (DCASE 2022), Nov 2022, Nancy, France",
The Potential of Neural Speech Synthesis-based Data Augmentation for Personalized Speech Enhancement,"[arxiv.Result.Author('Anastasia Kuznetsova'), arxiv.Result.Author('Aswin Sivaraman'), arxiv.Result.Author('Minje Kim')]",2022-11-14 16:20:41+00:00,"With the advances in deep learning, speech enhancement systems benefited from
large neural network architectures and achieved state-of-the-art quality.
However, speaker-agnostic methods are not always desirable, both in terms of
quality and their complexity, when they are to be used in a
resource-constrained environment. One promising way is personalized speech
enhancement (PSE), which is a smaller and easier speech enhancement problem for
small models to solve, because it focuses on a particular test-time user. To
achieve the personalization goal, while dealing with the typical lack of
personal data, we investigate the effect of data augmentation based on neural
speech synthesis (NSS). In the proposed method, we show that the quality of the
NSS system's synthetic data matters, and if they are good enough the augmented
dataset can be used to improve the PSE system that outperforms the
speaker-agnostic baseline. The proposed PSE systems show significant complexity
reduction while preserving the enhancement quality.",,
Exploring the Impact of Noise and Degradations on Heart Sound Classification Models,"[arxiv.Result.Author('Davoud Shariat Panah'), arxiv.Result.Author('Andrew Hines'), arxiv.Result.Author('Susan McKeever')]",2022-11-14 15:18:31+00:00,"The development of data-driven heart sound classification models has been an
active area of research in recent years. To develop such data-driven models in
the first place, heart sound signals need to be captured using a signal
acquisition device. However, it is almost impossible to capture noise-free
heart sound signals due to the presence of internal and external noises in most
situations. Such noises and degradations in heart sound signals can potentially
reduce the accuracy of data-driven classification models. Although different
techniques have been proposed in the literature to address the noise issue, how
and to what extent different noise and degradations in heart sound signals
impact the accuracy of data-driven classification models remains unexplored. To
answer this question, we produced a synthetic heart sound dataset including
normal and abnormal heart sounds contaminated with a large variety of noise and
degradations. We used this dataset to investigate the impact of noise and
degradation in heart sound recordings on the performance of different
classification models. The results show different noises and degradations
affect the performance of heart sound classification models to a different
extent; some are more problematic for classification models, and others are
less destructive. Comparing the findings of this study with the results of a
survey we previously carried out with a group of clinicians shows noise and
degradations that are more detrimental to classification models are also more
disruptive to accurate auscultation. The findings of this study can be
leveraged to develop targeted heart sound quality enhancement approaches -
which adapt the type and aggressiveness of quality enhancement based on the
characteristics of noise and degradation in heart sound signals.",Submitted to Computers in Biology and Medicine Journal,
Multi-Label Training for Text-Independent Speaker Identification,[arxiv.Result.Author('Yuqi Xue')],2022-11-14 14:07:25+00:00,"In this paper, we propose a novel strategy for text-independent speaker
identification system: Multi-Label Training (MLT). Instead of the commonly used
one-to-one correspondence between the speech and the speaker label, we divide
all the speeches of each speaker into several subgroups, with each subgroup
assigned a different set of labels. During the identification process, a
specific speaker is identified as long as the predicted label is the same as
one of his/her corresponding labels. We found that this method can force the
model to distinguish the data more accurately, and somehow takes advantages of
ensemble learning, while avoiding the significant increase of computation and
storage burden. In the experiments, we found that not only in clean conditions,
but also in noisy conditions with speech enhancement, Multi-Label Training can
still achieve better identification performance than commom methods. It should
be noted that the proposed strategy can be easily applied to almost all current
text-independent speaker identification models to achieve further improvements.",,
Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition,"[arxiv.Result.Author('Jiaxin Ye'), arxiv.Result.Author('Xincheng Wen'), arxiv.Result.Author('Yujie Wei'), arxiv.Result.Author('Yong Xu'), arxiv.Result.Author('Kunhong Liu'), arxiv.Result.Author('Hongming Shan')]",2022-11-14 13:35:01+00:00,"Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
Remarkably, TIM-Net outperforms the latest domain-adaptation method on the
cross-corpus SER tasks, demonstrating strong generalizability.","Submitted to ICASSP 2023. 8 pages, 6 figures",
MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets,"[arxiv.Result.Author('Ziyang Ma'), arxiv.Result.Author('Zhisheng Zhen'), arxiv.Result.Author('Changli Tang'), arxiv.Result.Author('Yujin Wang'), arxiv.Result.Author('Xie Chen')]",2022-11-14 13:00:47+00:00,"In this paper, we provide a new perspective on self-supervised speech models
from how the self-training targets are obtained. We generalize the targets
extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor
(On-TE), without caring about specific pretext tasks. Based on this, we propose
a new multi-tasking learning framework for self-supervised learning, MT4SSL,
which stands for Boosting Self-Supervised Speech Representation Learning by
Integrating Multiple Targets. MT4SSL refers to two typical models, HuBERT and
data2vec, which use the K-means algorithm as an Off-TE and a teacher network
without gradients as an On-TE, respectively. Our model outperforms previous SSL
methods by nontrivial margins on the LibriSpeech benchmark, and is comparable
to or even better than the best-performing models with no need for that much
data. Furthermore, we find that using both Off-TE and On-TE results in better
convergence in the pre-training phase. With both effectiveness and efficiency,
we think that doing multi-task learning on self-supervised speech models from
our perspective is a promising trend.",Submitted to ICASSP 2023,
Sentiment recognition of Italian elderly through domain adaptation on cross-corpus speech dataset,"[arxiv.Result.Author('Francesca Gasparini'), arxiv.Result.Author('Alessandra Grossi')]",2022-11-14 12:39:41+00:00,"The aim of this work is to define a speech emotion recognition (SER) model
able to recognize positive, neutral and negative emotions in natural
conversations of Italian elderly people. Several datasets for SER are available
in the literature. However most of them are in English or Chinese, have been
recorded while actors and actresses pronounce short phrases and thus are not
related to natural conversation. Moreover only few speeches among all the
databases are related to elderly people. Therefore, in this work, a
multi-language and multi-age corpus is considered merging a dataset in English,
that includes also elderly people, with a dataset in Italian. A general model,
trained on young and adult English actors and actresses is proposed, based on
XGBoost. Then two strategies of domain adaptation are proposed to adapt the
model either to elderly people and to Italian speakers. The results suggest
that this approach increases the classification performance, underlining also
that new datasets should be collected.","15 pages, 3 figures, 5 tables",
MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation,"[arxiv.Result.Author('Chang-Bin Jeon'), arxiv.Result.Author('Hyeongi Moon'), arxiv.Result.Author('Keunwoo Choi'), arxiv.Result.Author('Ben Sangbae Chon'), arxiv.Result.Author('Kyogu Lee')]",2022-11-14 12:27:35+00:00,"Separation of multiple singing voices into each voice is a rarely studied
area in music source separation research. The absence of a benchmark dataset
has hindered its progress. In this paper, we present an evaluation dataset and
provide baseline studies for multiple singing voices separation. First, we
introduce MedleyVox, an evaluation dataset for multiple singing voices
separation that corresponds to such categories. We specify the problem
definition in this dataset by categorizing the problem into i) duet, ii)
unison, iii)main vs. rest, and iv) N-singing separation. Second, we present a
strategy for construction of multiple singing mixtures using various
single-singing datasets. This can be used to obtain training data. Third, we
propose the improved super-resolution network (iSRNet). Jointly trained with
the Conv-TasNet and the multi-singing mixture construction strategy, the
proposed iSRNet achieved comparable performance to ideal time-frequency masks
on duet and unison subsets of MedleyVox. Audio samples, the dataset, and codes
are available on our GitHub page (https://github.com/jeonchangbin49/MedleyVox).","5 pages, 3 figures, 6 tables, submitted to ICASSP 2023",
SNIPER Training: Variable Sparsity Rate Training For Text-To-Speech,"[arxiv.Result.Author('Perry Lam'), arxiv.Result.Author('Huayun Zhang'), arxiv.Result.Author('Nancy F. Chen'), arxiv.Result.Author('Berrak Sisman'), arxiv.Result.Author('Dorien Herremans')]",2022-11-14 11:26:13+00:00,"Text-to-speech (TTS) models have achieved remarkable naturalness in recent
years, yet like most deep neural models, they have more parameters than
necessary. Sparse TTS models can improve on dense models via pruning and extra
retraining, or converge faster than dense models with some performance loss.
Inspired by these results, we propose training TTS models using a decaying
sparsity rate, i.e. a high initial sparsity to accelerate training first,
followed by a progressive rate reduction to obtain better eventual performance.
This decremental approach differs from current methods of incrementing sparsity
to a desired target, which costs significantly more time than dense training.
We call our method SNIPER training: Single-shot Initialization Pruning
Evolving-Rate training. Our experiments on FastSpeech2 show that although we
were only able to obtain better losses in the first few epochs before being
overtaken by the baseline, the final SNIPER-trained models beat
constant-sparsity models and pip dense models in performance.",,
Hope Speech Detection on Social Media Platforms,"[arxiv.Result.Author('Pranjal Aggarwal'), arxiv.Result.Author('Pasupuleti Chandana'), arxiv.Result.Author('Jagrut Nemade'), arxiv.Result.Author('Shubham Sharma'), arxiv.Result.Author('Sunil Saumya'), arxiv.Result.Author('Shankar Biradar')]",2022-11-14 10:58:22+00:00,"Since personal computers became widely available in the consumer market, the
amount of harmful content on the internet has significantly expanded. In simple
terms, harmful content is anything online which causes a person distress or
harm. It may include hate speech, violent content, threats, non-hope speech,
etc. The online content must be positive, uplifting and supportive. Over the
past few years, many studies have focused on solving this problem through hate
speech detection, but very few focused on identifying hope speech. This paper
discusses various machine learning approaches to identify a sentence as Hope
Speech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study
contains English YouTube comments and is released as a part of the shared task
""EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion"".
Initially, the dataset obtained from the shared task had three classes: Hope
Speech, non-Hope speech, and not in English; however, upon deeper inspection,
we discovered that dataset relabeling is required. A group of undergraduates
was hired to help perform the entire dataset's relabeling task. We experimented
with conventional machine learning models (such as Na\""ive Bayes, logistic
regression and support vector machine) and pre-trained models (such as BERT) on
relabeled data. According to the experimental results, the relabeled data has
achieved a better accuracy for Hope speech identification than the original
data set.","14 pages, 05 figures. accepted for publication in the book chapter
  ""Cyber Crime in Social Media: Theory and Solutions""",
Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts,"[arxiv.Result.Author('Karim M. Ibrahim'), arxiv.Result.Author('Elena V. Epure'), arxiv.Result.Author('Geoffroy Peeters'), arxiv.Result.Author('Gaël Richard')]",2022-11-14 10:08:12+00:00,"As music has become more available especially on music streaming platforms,
people have started to have distinct preferences to fit to their varying
listening situations, also known as context. Hence, there has been a growing
interest in considering the user's situation when recommending music to users.
Previous works have proposed user-aware autotaggers to infer situation-related
tags from music content and user's global listening preferences. However, in a
practical music retrieval system, the autotagger could be only used by assuming
that the context class is explicitly provided by the user. In this work, for
designing a fully automatised music retrieval system, we propose to
disambiguate the user's listening information from their stream data. Namely,
we propose a system which can generate a situational playlist for a user at a
certain time 1) by leveraging user-aware music autotaggers, and 2) by
automatically inferring the user's situation from stream data (e.g. device,
network) and user's general profile information (e.g. age). Experiments show
that such a context-aware personalized music retrieval system is feasible, but
the performance decreases in the case of new users, new tracks or when the
number of context classes increases.",Published in ISMIR,
Buying Privacy: User Perceptions of Privacy Threats from Mobile Apps,"[arxiv.Result.Author('Jenny Tang'), arxiv.Result.Author('Hannah Shoemaker'), arxiv.Result.Author('Leah Teffera'), arxiv.Result.Author('Eleanor Birrell'), arxiv.Result.Author('Ada Lerner')]",2022-11-14 09:48:07+00:00,"As technology and technology companies have grown in power, ubiquity, and
societal influence, some companies -- and notably some mobile apps -- have come
to be perceived as privacy threats. Prior work has considered how various
factors impact perceptions of threat, including social factors, political
speech, and user-interface design. In this work, we investigate how
user-visible context clues impact perceptions about whether a mobile
application application poses a privacy threat. We conduct a user study with
2109 users in which we find that users depend on context clues -- such as
presence of advertising and occurrence (and timing of payment) -- to determine
the extent to which a mobile app poses a privacy threat. We also quantify how
accurately user assessments match published data collection practices, and we
identify a commonly-held misconception about how payments are processed. This
work provides new insight into how users assess the privacy threat posed by
mobile apps and into social norms around data collection.",,
Towards A Unified Conformer Structure: from ASR to ASV Task,"[arxiv.Result.Author('Dexin Liao'), arxiv.Result.Author('Tao Jiang'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Lin Li'), arxiv.Result.Author('Qingyang Hong')]",2022-11-14 08:44:53+00:00,"Transformer has achieved extraordinary performance in Natural Language
Processing and Computer Vision tasks thanks to its powerful self-attention
mechanism, and its variant Conformer has become a state-of-the-art architecture
in the field of Automatic Speech Recognition (ASR). However, the main-stream
architecture for Automatic Speaker Verification (ASV) is convolutional Neural
Networks, and there is still much room for research on the Conformer based ASV.
In this paper, firstly, we modify the Conformer architecture from ASR to ASV
with very minor changes. Length-Scaled Attention (LSA) method and
Sharpness-Aware Minimizationis (SAM) are adopted to improve model
generalization. Experiments conducted on VoxCeleb and CN-Celeb show that our
Conformer based ASV achieves competitive performance compared with the popular
ECAPA-TDNN. Secondly, inspired by the transfer learning strategy, ASV Conformer
is natural to be initialized from the pretrained ASR model. Via parameter
transferring, self-attention mechanism could better focus on the relationship
between sequence features, brings about 11% relative improvement in EER on test
set of VoxCeleb and CN-Celeb, which reveals the potential of Conformer to unify
ASV and ASR task. Finally, we provide a runtime in ASV-Subtools to evaluate its
inference speed in production scenario. Our code is released at
https://github.com/Snowdar/asv-subtools/tree/master/doc/papers/conformer.md.",,
Exchanging Keys with Authentication and Identity Protection for Secure Voice Communication without Side-channel,"[arxiv.Result.Author('Piotr Krasnowski'), arxiv.Result.Author('Jerome Lebrun'), arxiv.Result.Author('Bruno Martin')]",2022-11-14 08:24:50+00:00,"Motivated by an increasing need for privacy-preserving voice communications,
we investigate here the original idea of sending encrypted data and speech in
the form of pseudo-speech signals in the audio domain. Being less constrained
than military ``Crypto Phones'' and allowing genuine public evaluation, this
approach is quite promising for public unsecured voice communication
infrastructures, such as 3G cellular network and VoIP.A cornerstone of secure
voice communications is the authenticated exchange of cryptographic keys with
sole resource the voice channel, and neither Public Key Infrastructure (PKI)
nor Certificate Authority (CA). In this paper, we detail our new robust double
authentication mechanism based on signatures and Short Authentication Strings
(SAS) ensuring strong authentication between the users while mitigating errors
caused by unreliable voice channels and also identity protection against
passive eavesdroppers. As symbolic model, our protocol has been formally
proof-checked for security and fully validated by Tamarin Prover.",,"International Research Workshop on Computer Security &
  Cybersecurity Challenges (CSCC 2022), Polish Academy of Sciences; Wroclaw
  University of Science and Technology, Poland, May 2022, Paris, France"
YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations,"[arxiv.Result.Author('Eunjin Choi'), arxiv.Result.Author('Yoonjin Chung'), arxiv.Result.Author('Seolhee Lee'), arxiv.Result.Author('JongIk Jeon'), arxiv.Result.Author('Taegyun Kwon'), arxiv.Result.Author('Juhan Nam')]",2022-11-14 06:18:25+00:00,"Existing multi-instrumental datasets tend to be biased toward pop and
classical music. In addition, they generally lack high-level annotations such
as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game
music dataset with multi-label emotion annotations. It includes 669 audio and
MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a
programmable sound generator based on FM. The collected game music is arranged
with a subset of 15 monophonic instruments and one drum instrument. They were
converted from binary commands of the YM2413 sound chip. Each song was labeled
with 19 emotion tags by two annotators and validated by three verifiers to
obtain refined tags. We provide the baseline models and results for emotion
recognition and emotion-conditioned symbolic music generation using YM2413-MDB.",The paper has been accepted for publication at ISMIR 2022,
Autovocoder: Fast Waveform Generation from a Learned Speech Representation using Differentiable Digital Signal Processing,"[arxiv.Result.Author('Jacob J Webber'), arxiv.Result.Author('Cassia Valentini-Botinhao'), arxiv.Result.Author('Evelyn Williams'), arxiv.Result.Author('Gustav Eje Henter'), arxiv.Result.Author('Simon King')]",2022-11-13 18:37:57+00:00,"Most state-of-the-art Text-to-Speech systems use the mel-spectrogram as an
intermediate representation, to decompose the task into acoustic modelling and
waveform generation.
  A mel-spectrogram is extracted from the waveform by a simple, fast DSP
operation, but generating a high-quality waveform from a mel-spectrogram
requires computationally expensive machine learning: a neural vocoder. Our
proposed ``autovocoder'' reverses this arrangement. We use machine learning to
obtain a representation that replaces the mel-spectrogram, and that can be
inverted back to a waveform using simple, fast operations including a
differentiable implementation of the inverse STFT.
  The autovocoder generates a waveform 5 times faster than the DSP-based
Griffin-Lim algorithm, and 14 times faster than the neural vocoder HiFi-GAN. We
provide perceptual listening test results to confirm that the speech is of
comparable quality to HiFi-GAN in the copy synthesis task.","Submitted to the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2023)",
BiFSMNv2: Pushing Binary Neural Networks for Keyword Spotting to Real-Network Performance,"[arxiv.Result.Author('Haotong Qin'), arxiv.Result.Author('Xudong Ma'), arxiv.Result.Author('Yifu Ding'), arxiv.Result.Author('Xiaoyang Li'), arxiv.Result.Author('Yang Zhang'), arxiv.Result.Author('Zejun Ma'), arxiv.Result.Author('Jiakai Wang'), arxiv.Result.Author('Jie Luo'), arxiv.Result.Author('Xianglong Liu')]",2022-11-13 18:31:45+00:00,"Deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications while suffering expensive computation and
storage. Therefore, network compression technologies like binarization are
studied to deploy KWS models on edge. In this paper, we present a strong yet
efficient binary neural network for KWS, namely BiFSMNv2, pushing it to the
real-network accuracy performance. First, we present a Dual-scale Thinnable
1-bit-Architecture to recover the representation capability of the binarized
computation units by dual-scale activation binarization and liberate the
speedup potential from an overall architecture perspective. Second, we also
construct a Frequency Independent Distillation scheme for KWS
binarization-aware training, which distills the high and low-frequency
components independently to mitigate the information mismatch between
full-precision and binarized representations. Moreover, we propose the Learning
Propagation Binarizer, a general and efficient binarizer that enables the
forward and backward propagation of binary KWS networks to be continuously
improved through learning. We implement and deploy the BiFSMNv2 on ARMv8
real-world hardware with a novel Fast Bitwise Computation Kernel, which is
proposed to fully utilize registers and increase instruction throughput.
Comprehensive experiments show our BiFSMNv2 outperforms existing binary
networks for KWS by convincing margins across different datasets and achieves
comparable accuracy with the full-precision networks (only a tiny 1.51% drop on
Speech Commands V1-12). We highlight that benefiting from the compact
architecture and optimized hardware kernel, BiFSMNv2 can achieve an impressive
25.1x speedup and 20.2x storage-saving on edge hardware.",arXiv admin note: text overlap with arXiv:2202.06483,
FullPack: Full Vector Utilization for Sub-Byte Quantized Inference on General Purpose CPUs,"[arxiv.Result.Author('Hossein Katebi'), arxiv.Result.Author('Navidreza Asadi'), arxiv.Result.Author('Maziar Goudarzi')]",2022-11-13 18:13:31+00:00,"Although prior art has demonstrated negligible accuracy drop in sub-byte
quantization -- where weights and/or activations are represented by less than 8
bits -- popular SIMD instructions of CPUs do not natively support these
datatypes. While recent methods, such as ULPPACK, are already using sub-byte
quantization on general-purpose CPUs with vector units, they leave out several
empty bits between the sub-byte values in memory and in vector registers to
avoid overflow to the neighbours during the operations. This results in memory
footprint and bandwidth-usage inefficiencies and suboptimal performance. In
this paper, we present memory layouts for storing, and mechanisms for
processing sub-byte (4-, 2-, or 1-bit) models that utilize all the bits in the
memory as well as in the vector registers for the actual data. We provide
compute kernels for the proposed layout for the GEMV (GEneral Matrix-Vector
multiplication) operations between weights and activations of different
datatypes (e.g., 8-bit activations and 4-bit weights). For evaluation, we
extended the TFLite package and added our methods to it, then ran the models on
the cycle-accurate gem5 simulator to compare detailed memory and CPU cycles of
each method. We compare against nine other methods that are actively used in
production including GEMLOWP, Ruy, XNNPack, and ULPPACK. Furthermore, we
explore the effect of different input and output sizes of deep learning layers
on the performance of our proposed method. Experimental results show 0.96-2.1x
speedup for small sizes and 1.2-6.7x speedup for mid to large sizes. Applying
our proposal to a real-world speech recognition model, Mozilla DeepSpeech, we
proved that our method achieves 1.56-2.11x end-to-end speedup compared to the
state-of-the-art, depending on the bit-width employed.",,
Bounds and Estimates on the Average Edit Distance,"[arxiv.Result.Author('Gianfranco Bilardi'), arxiv.Result.Author('Michele Schimd')]",2022-11-13 14:10:57+00:00,"The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
  This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
  An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
  Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
  The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.","42 pages, 1 figure, 9 tables, submitted for review",
OverFlow: Putting flows on top of neural transducers for better TTS,"[arxiv.Result.Author('Shivam Mehta'), arxiv.Result.Author('Ambika Kirkland'), arxiv.Result.Author('Harm Lameris'), arxiv.Result.Author('Jonas Beskow'), arxiv.Result.Author('Éva Székely'), arxiv.Result.Author('Gustav Eje Henter')]",2022-11-13 12:53:05+00:00,"Neural HMMs are a type of neural transducer recently proposed for
sequence-to-sequence modelling in text-to-speech. They combine the best
features of classic statistical speech synthesis and modern neural TTS,
requiring less data and fewer training updates, and are less prone to gibberish
output caused by neural attention failures. In this paper, we combine neural
HMM TTS with normalising flows for describing the highly non-Gaussian
distribution of speech acoustics. The result is a powerful, fully probabilistic
model of durations and acoustics that can be trained using exact maximum
likelihood. Compared to dominant flow-based acoustic models, our approach
integrates autoregression for improved modelling of long-range dependences such
as utterance-level prosody. Experiments show that a system based on our
proposal gives more accurate pronunciations and better subjective speech
quality than comparable methods, whilst retaining the original advantages of
neural HMMs. Audio examples and code are available at
https://shivammehta25.github.io/OverFlow/","5 pages, 2 figures, submitted to ICASSP 2023",
Multi-Speaker and Wide-Band Simulated Conversations as Training Data for End-to-End Neural Diarization,"[arxiv.Result.Author('Federico Landini'), arxiv.Result.Author('Mireia Diez'), arxiv.Result.Author('Alicia Lozano-Diez'), arxiv.Result.Author('Lukáš Burget')]",2022-11-12 21:32:06+00:00,"End-to-end diarization presents an attractive alternative to standard
cascaded diarization systems because a single system can handle all aspects of
the task at once. Many flavors of end-to-end models have been proposed but all
of them require (so far non-existing) large amounts of annotated data for
training. The compromise solution consists in generating synthetic data and the
recently proposed simulated conversations (SC) have shown remarkable
improvements over the original simulated mixtures (SM). In this work, we create
SC with multiple speakers per conversation and show that they allow for
substantially better performance than SM, also reducing the dependence on a
fine-tuning stage. We also create SC with wide-band public audio sources and
present an analysis on several evaluation sets. Together with this publication,
we release the recipes for generating such data and models trained on public
sets as well as the implementation to efficiently handle multiple speakers per
conversation and an auxiliary voice activity detection loss.",,
Online Phase Reconstruction via DNN-based Phase Differences Estimation,"[arxiv.Result.Author('Yoshiki Masuyama'), arxiv.Result.Author('Kohei Yatabe'), arxiv.Result.Author('Kento Nagatomo'), arxiv.Result.Author('Yasuhiro Oikawa')]",2022-11-12 20:45:51+00:00,"This paper presents a two-stage online phase reconstruction framework using
causal deep neural networks (DNNs). Phase reconstruction is a task of
recovering phase of the short-time Fourier transform (STFT) coefficients only
from the corresponding magnitude. However, phase is sensitive to waveform
shifts and not easy to estimate from the magnitude even with a DNN. To overcome
this problem, we propose to use DNNs for estimating differences of phase
between adjacent time-frequency bins. We show that convolutional neural
networks are suitable for phase difference estimation, according to the
theoretical relation between partial derivatives of STFT phase and magnitude.
The estimated phase differences are used for reconstructing phase by solving a
weighted least squares problem in a frame-by-frame manner. In contrast to
existing DNN-based phase reconstruction methods, the proposed framework is
causal and does not require any iterative procedure. The experiments showed
that the proposed method outperforms existing online methods and a DNN-based
method for phase reconstruction.","Accepted to IEEE/ACM Trans. Audio, Speech, and Language Processing",
Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation,"[arxiv.Result.Author('Yusong Wu'), arxiv.Result.Author('Ke Chen'), arxiv.Result.Author('Tianyu Zhang'), arxiv.Result.Author('Yuchen Hui'), arxiv.Result.Author('Taylor Berg-Kirkpatrick'), arxiv.Result.Author('Shlomo Dubnov')]",2022-11-12 15:25:20+00:00,"Contrastive learning has shown remarkable success in the field of multimodal
representation learning. In this paper, we propose a pipeline of contrastive
language-audio pretraining to develop an audio representation by combining
audio data with natural language descriptions. To accomplish this target, we
first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs
from different data sources. Second, we construct a contrastive language-audio
pretraining model by considering different audio encoders and text encoders. We
incorporate the feature fusion mechanism and keyword-to-caption augmentation
into the model design to further enable the model to process audio inputs of
variable lengths and enhance the performance. Third, we perform comprehensive
experiments to evaluate our model across three tasks: text-to-audio retrieval,
zero-shot audio classification, and supervised audio classification. The
results demonstrate that our model achieves superior performance in
text-to-audio retrieval task. In audio classification tasks, the model achieves
state-of-the-art performance in the zero-shot setting and is able to obtain
performance comparable to models' results in the non-zero-shot setting.
LAION-Audio-630K and the proposed model are both available to the public.",,
A Self-Adjusting Fusion Representation Learning Model for Unaligned Text-Audio Sequences,"[arxiv.Result.Author('Kaicheng Yang'), arxiv.Result.Author('Ruxuan Zhang'), arxiv.Result.Author('Hua Xu'), arxiv.Result.Author('Kai Gao')]",2022-11-12 13:05:28+00:00,"Inter-modal interaction plays an indispensable role in multimodal sentiment
analysis. Due to different modalities sequences are usually non-alignment, how
to integrate relevant information of each modality to learn fusion
representations has been one of the central challenges in multimodal learning.
In this paper, a Self-Adjusting Fusion Representation Learning Model (SA-FRLM)
is proposed to learn robust crossmodal fusion representations directly from the
unaligned text and audio sequences. Different from previous works, our model
not only makes full use of the interaction between different modalities but
also maximizes the protection of the unimodal characteristics. Specifically, we
first employ a crossmodal alignment module to project different modalities
features to the same dimension. The crossmodal collaboration attention is then
adopted to model the inter-modal interaction between text and audio sequences
and initialize the fusion representations. After that, as the core unit of the
SA-FRLM, the crossmodal adjustment transformer is proposed to protect original
unimodal characteristics. It can dynamically adapt the fusion representations
by using single modal streams. We evaluate our approach on the public
multimodal sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experiment
results show that our model has significantly improved the performance of all
the metrics on the unaligned text-audio sequences.",8 pages,
Efficient Speech Quality Assessment using Self-supervised Framewise Embeddings,"[arxiv.Result.Author('Karl El Hajal'), arxiv.Result.Author('Zihan Wu'), arxiv.Result.Author('Neil Scheidwasser-Clow'), arxiv.Result.Author('Gasser Elbanna'), arxiv.Result.Author('Milos Cernak')]",2022-11-12 11:57:08+00:00,"Automatic speech quality assessment is essential for audio researchers,
developers, speech and language pathologists, and system quality engineers. The
current state-of-the-art systems are based on framewise speech features
(hand-engineered or learnable) combined with time dependency modeling. This
paper proposes an efficient system with results comparable to the best
performing model in the ConferencingSpeech 2022 challenge. Our proposed system
is characterized by a smaller number of parameters (40-60x), fewer FLOPS
(100x), lower memory consumption (10-15x), and lower latency (30x). Speech
quality practitioners can therefore iterate much faster, deploy the system on
resource-limited hardware, and, overall, the proposed system contributes to
sustainable machine learning. The paper also concludes that framewise
embeddings outperform utterance-level embeddings and that multi-task training
with acoustic conditions modeling does not degrade speech quality prediction
while providing better interpretation.",,
"Improving the Robustness of DistilHuBERT to Unseen Noisy Conditions via Data Augmentation, Curriculum Learning, and Multi-Task Enhancement","[arxiv.Result.Author('Heitor R. Guimarães'), arxiv.Result.Author('Arthur Pimentel'), arxiv.Result.Author('Anderson R. Avila'), arxiv.Result.Author('Mehdi Rezagholizadeh'), arxiv.Result.Author('Tiago H. Falk')]",2022-11-12 03:50:22+00:00,"Self-supervised speech representation learning aims to extract meaningful
factors from the speech signal that can later be used across different
downstream tasks, such as speech and/or emotion recognition. Existing models,
such as HuBERT, however, can be fairly large thus may not be suitable for edge
speech applications. Moreover, realistic applications typically involve speech
corrupted by noise and room reverberation, hence models need to provide
representations that are robust to such environmental factors. In this study,
we build on the so-called DistilHuBERT model, which distils HuBERT to a
fraction of its original size, with three modifications, namely: (i) augment
the training data with noise and reverberation, while the student model needs
to distill the clean representations from the teacher model; (ii) introduce a
curriculum learning approach where increasing levels of noise are introduced as
the model trains, thus helping with convergence and with the creation of more
robust representations; and (iii) introduce a multi-task learning approach
where the model also reconstructs the clean waveform jointly with the
distillation task, thus also acting as an enhancement step to ensure additional
environment robustness to the representation. Experiments on three SUPERB tasks
show the advantages of the proposed method not only relative to the original
DistilHuBERT, but also to the original HuBERT, thus showing the advantages of
the proposed method for ``in the wild'' edge speech applications.","ENLSP-II NeurIPS Workshop 2022, 6 pages",
Investigations in Audio Captioning: Addressing Vocabulary Imbalance and Evaluating Suitability of Language-Centric Performance Metrics,"[arxiv.Result.Author('Sandeep Kothinti'), arxiv.Result.Author('Dimitra Emmanouilidou')]",2022-11-12 02:12:21+00:00,"The analysis, processing, and extraction of meaningful information from
sounds all around us is the subject of the broader area of audio analytics.
Audio captioning is a recent addition to the domain of audio analytics, a
cross-modal translation task that focuses on generating natural descriptions
from sound events occurring in an audio stream. In this work, we identify and
improve on three main challenges in automated audio captioning: i) data
scarcity, ii) imbalance or limitations in the audio captions vocabulary, and
iii) the proper performance evaluation metric that can best capture both
auditory and semantic characteristics. We find that generally adopted loss
functions can result in an unfair vocabulary imbalance during model training.
We propose two audio captioning augmentation methods that enrich the training
dataset and the vocabulary size. We further underline the need for in-domain
pretraining by exploring the suitability of audio encoders that were previously
trained on different audio tasks. Finally, we systematically explore five
performance metrics borrowed from the image captioning domain and highlight
their limitations for the audio domain.",Submitted to ICASSP 2023,
Low Pass Filtering and Bandwidth Extension for Robust Anti-spoofing Countermeasure Against Codec Variabilities,"[arxiv.Result.Author('Yikang Wang'), arxiv.Result.Author('Xingming Wang'), arxiv.Result.Author('Hiromitsu Nishizaki'), arxiv.Result.Author('Ming Li')]",2022-11-12 02:04:00+00:00,"A reliable voice anti-spoofing countermeasure system needs to robustly
protect automatic speaker verification (ASV) systems in various kinds of
spoofing scenarios. However, the performance of countermeasure systems could be
degraded by channel effects and codecs. In this paper, we show that using the
low-frequency subbands of signals as input can mitigate the negative impact
introduced by codecs on the countermeasure systems. To validate this, two types
of low-pass filters with different cut-off frequencies are applied to
countermeasure systems, and the equal error rate (EER) is reduced by up to 25%
relatively. In addition, we propose a deep learning based bandwidth extension
approach to further improve the detection accuracy. Recent studies show that
the error rate of countermeasure systems increase dramatically when the silence
part is removed by Voice Activity Detection (VAD), our experimental results
show that the filtering and bandwidth extension approaches are also effective
under the codec condition when VAD is applied.","5 pages, 3 figures, accepted by ISCSLP 2022",
A unified one-shot prosody and speaker conversion system with self-supervised discrete speech units,"[arxiv.Result.Author('Li-Wei Chen'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Alexander Rudnicky')]",2022-11-12 00:54:09+00:00,"We present a unified system to realize one-shot voice conversion (VC) on the
pitch, rhythm, and speaker attributes. Existing works generally ignore the
correlation between prosody and language content, leading to the degradation of
naturalness in converted speech. Additionally, the lack of proper language
features prevents these systems from accurately preserving language content
after conversion. To address these issues, we devise a cascaded modular system
leveraging self-supervised discrete speech units as language representation.
These discrete units provide duration information essential for rhythm
modeling. Our system first extracts utterance-level prosody and speaker
representations from the raw waveform. Given the prosody representation, a
prosody predictor estimates pitch, energy, and duration for each discrete unit
in the utterance. A synthesizer further reconstructs speech based on the
predicted prosody, speaker representation, and discrete units. Experiments show
that our system outperforms previous approaches in naturalness,
intelligibility, speaker transferability, and prosody transferability. Code and
samples are publicly available.",Submitted to ICASSP 2023,
On the robustness of non-intrusive speech quality model by adversarial examples,"[arxiv.Result.Author('Hsin-Yi Lin'), arxiv.Result.Author('Huan-Hsin Tseng'), arxiv.Result.Author('Yu Tsao')]",2022-11-11 23:06:24+00:00,"It has been shown recently that deep learning based models are effective on
speech quality prediction and could outperform traditional metrics in various
perspectives. Although network models have potential to be a surrogate for
complex human hearing perception, they may contain instabilities in
predictions. This work shows that deep speech quality predictors can be
vulnerable to adversarial perturbations, where the prediction can be changed
drastically by unnoticeable perturbations as small as $-30$ dB compared with
speech inputs. In addition to exposing the vulnerability of deep speech quality
predictors, we further explore and confirm the viability of adversarial
training for strengthening robustness of models.",,
Breaking trade-offs in speech separation with sparsely-gated mixture of experts,"[arxiv.Result.Author('Xiaofei Wang'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Yu Shi'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Naoyuki Kanda'), arxiv.Result.Author('Takuya Yoshioka')]",2022-11-11 22:07:43+00:00,"Several trade-offs need to be balanced when employing monaural speech
separation (SS) models in conversational automatic speech recognition (ASR)
systems. A larger SS model generally achieves better output quality at an
expense of higher computation, meanwhile, a better SS model for overlapping
speech often produces distorted output for non-overlapping speech. This paper
addresses these trade-offs with a sparsely-gated mixture-of-experts (MoE). The
sparsely-gated MoE architecture allows the separation models to be enlarged
without compromising the run-time efficiency, which also helps achieve a better
separation-distortion trade-off. To further reduce the speech distortion
without compromising the SS capability, a multi-gate MoE framework is also
explored, where different gates handle non-overlapping and overlapping frames
differently. ASR experiments are conducted by using a simulated dataset for
measuring both the speech separation accuracy and the speech distortion. Two
advanced SS models, Conformer and WavLM-based models, are used as baselines.
The sparsely-gated MoE models show a superior SS capability with less speech
distortion, meanwhile marginally increasing the run-time computational cost.
Experimental results using real conversation recordings are also presented,
showing MoE's effectiveness in an end-to-end evaluation setting.",Submitted to ICASSP 2023,
Augmenting Transformer-Transducer Based Speaker Change Detection With Token-Level Training Loss,"[arxiv.Result.Author('Guanlong Zhao'), arxiv.Result.Author('Quan Wang'), arxiv.Result.Author('Han Lu'), arxiv.Result.Author('Yiling Huang'), arxiv.Result.Author('Ignacio Lopez Moreno')]",2022-11-11 21:09:58+00:00,"In this work we propose a novel token-based training strategy that improves
Transformer-Transducer (T-T) based speaker change detection (SCD) performance.
The conventional T-T based SCD model loss optimizes all output tokens equally.
Due to the sparsity of the speaker changes in the training data, the
conventional T-T based SCD model loss leads to sub-optimal detection accuracy.
To mitigate this issue, we use a customized edit-distance algorithm to estimate
the token-level SCD false accept (FA) and false reject (FR) rates during
training and optimize model parameters to minimize a weighted combination of
the FA and FR, focusing the model on accurately predicting speaker changes. We
also propose a set of evaluation metrics that align better with commercial use
cases. Experiments on a group of challenging real-world datasets show that the
proposed training method can significantly improve the overall performance of
the SCD model with the same number of parameters.",,
Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting,"[arxiv.Result.Author('Beltrán Labrador'), arxiv.Result.Author('Guanlong Zhao'), arxiv.Result.Author('Ignacio López Moreno'), arxiv.Result.Author('Angelo Scorza Scarpati'), arxiv.Result.Author('Liam Fowl'), arxiv.Result.Author('Quan Wang')]",2022-11-11 20:41:46+00:00,"In this paper, we present a novel approach to adapt a sequence-to-sequence
Transformer-Transducer ASR system to the keyword spotting (KWS) task. We
achieve this by replacing the keyword in the text transcription with a special
token <kw> and training the system to detect the <kw> token in an audio stream.
At inference time, we create a decision function inspired by conventional KWS
approaches, to make our approach more suitable for the KWS task. Furthermore,
we introduce a specific keyword spotting loss by adapting the
sequence-discriminative Minimum Bayes-Risk training technique. We find that our
approach significantly outperforms ASR based KWS systems. When compared with a
conventional keyword spotting system, our proposal has similar performance
while bringing the advantages and flexibility of sequence-to-sequence training.
Additionally, when combined with the conventional KWS system, our approach can
improve the performance at any operation point.",,
Speech-to-Speech Translation For A Real-world Unwritten Language,"[arxiv.Result.Author('Peng-Jen Chen'), arxiv.Result.Author('Kevin Tran'), arxiv.Result.Author('Yilin Yang'), arxiv.Result.Author('Jingfei Du'), arxiv.Result.Author('Justine Kao'), arxiv.Result.Author('Yu-An Chung'), arxiv.Result.Author('Paden Tomasello'), arxiv.Result.Author('Paul-Ambroise Duquenne'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Hongyu Gong'), arxiv.Result.Author('Hirofumi Inaguma'), arxiv.Result.Author('Sravya Popuri'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Juan Pino'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Ann Lee')]",2022-11-11 20:21:38+00:00,"We study speech-to-speech translation (S2ST) that translates speech from one
language into another language and focuses on building systems to support
languages without standard text writing systems. We use English-Taiwanese
Hokkien as a case study, and present an end-to-end solution from training data
collection, modeling choices to benchmark dataset release. First, we present
efforts on creating human annotated data, automatically mining data from large
unlabeled speech datasets, and adopting pseudo-labeling to produce weakly
supervised data. On the modeling, we take advantage of recent advances in
applying self-supervised discrete representations as target for prediction in
S2ST and show the effectiveness of leveraging additional text supervision from
Mandarin, a language similar to Hokkien, in model training. Finally, we release
an S2ST benchmark set to facilitate future research in this field. The demo can
be found at https://huggingface.co/spaces/facebook/Hokkien_Translation .",,
Vocal Breath Sound Based Gender Classification,"[arxiv.Result.Author('Mohammad Shaique Solanki'), arxiv.Result.Author('Ashutosh M Bharadwaj'), arxiv.Result.Author('Jeevan K'), arxiv.Result.Author('Prasanta Kumar Ghosh')]",2022-11-11 17:43:26+00:00,"Voiced speech signals such as continuous speech are known to have acoustic
features such as pitch(F0), and formant frequencies(F1, F2, F3) which can be
used for gender classification. However, gender classification studies using
non-speech signals such as vocal breath sounds have not been explored as they
lack typical gender-specific acoustic features. In this work, we explore
whether vocal breath sounds encode gender information and if so, to what extent
it can be used for automatic gender classification. In this study, we explore
the use of data-driven and knowledge-based features from vocal breath sounds as
well as the classifier complexity for gender classification. We also explore
the importance of the location and duration of breath signal segments to be
used for automatic classification. Experiments with 54.23 minutes of male and
51.83 minutes of female breath sounds reveal that knowledge-based features,
namely MFCC statistics, with low-complexity classifier perform comparably to
the data-driven features with classifiers of higher complexity. Breath segments
with an average duration of 3 seconds are found to be the best choice
irrespective of the location which avoids the need for breath cycle boundary
annotation.",Submitted in ICASSP 2023,
Enhancing and Adversarial: Improve ASR with Speaker Labels,"[arxiv.Result.Author('Wei Zhou'), arxiv.Result.Author('Haotian Wu'), arxiv.Result.Author('Jingjing Xu'), arxiv.Result.Author('Mohammad Zeineldeen'), arxiv.Result.Author('Christoph Lüscher'), arxiv.Result.Author('Ralf Schlüter'), arxiv.Result.Author('Hermann Ney')]",2022-11-11 17:40:08+00:00,"ASR can be improved by multi-task learning (MTL) with domain enhancing or
domain adversarial training, which are two opposite objectives with the aim to
increase/decrease domain variance towards domain-aware/agnostic ASR,
respectively. In this work, we study how to best apply these two opposite
objectives with speaker labels to improve conformer-based ASR. We also propose
a novel adaptive gradient reversal layer for stable and effective adversarial
training without tuning effort. Detailed analysis and experimental verification
are conducted to show the optimal positions in the ASR neural network (NN) to
apply speaker enhancing and adversarial training. We also explore their
combination for further improvement, achieving the same performance as
i-vectors plus adversarial training. Our best speaker-based MTL achieves 7\%
relative improvement on the Switchboard Hub5'00 set. We also investigate the
effect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.",submitted to ICASSP 2023,
Analysis of Male and Female Speakers' Word Choices in Public Speeches,"[arxiv.Result.Author('Md Zobaer Hossain'), arxiv.Result.Author('Ahnaf Mozib Samin')]",2022-11-11 17:30:28+00:00,"The extent to which men and women use language differently has been
questioned previously. Finding clear and consistent gender differences in
language is not conclusive in general, and the research is heavily influenced
by the context and method employed to identify the difference. In addition, the
majority of the research was conducted in written form, and the sample was
collected in writing. Therefore, we compared the word choices of male and
female presenters in public addresses such as TED lectures. The frequency of
numerous types of words, such as parts of speech (POS), linguistic,
psychological, and cognitive terms were analyzed statistically to determine how
male and female speakers use words differently. Based on our data, we
determined that male speakers use specific types of linguistic, psychological,
cognitive, and social words in considerably greater frequency than female
speakers.",,
The Far Side of Failure: Investigating the Impact of Speech Recognition Errors on Subsequent Dementia Classification,"[arxiv.Result.Author('Changye Li'), arxiv.Result.Author('Trevor Cohen'), arxiv.Result.Author('Serguei Pakhomov')]",2022-11-11 17:06:45+00:00,"Linguistic anomalies detectable in spontaneous speech have shown promise for
various clinical applications including screening for dementia and other forms
of cognitive impairment. The feasibility of deploying automated tools that can
classify language samples obtained from speech in large-scale clinical settings
depends on the ability to capture and automatically transcribe the speech for
subsequent analysis. However, the impressive performance of self-supervised
learning (SSL) automatic speech recognition (ASR) models with curated speech
data is not apparent with challenging speech samples from clinical settings.
One of the key questions for successfully applying ASR models for clinical
applications is whether imperfect transcripts they generate provide sufficient
information for downstream tasks to operate at an acceptable level of accuracy.
In this study, we examine the relationship between the errors produced by
several deep learning ASR systems and their impact on the downstream task of
dementia classification. One of our key findings is that, paradoxically, ASR
systems with relatively high error rates can produce transcripts that result in
better downstream classification accuracy than classification based on verbatim
transcripts.",Accepted as extended abstract for ML4H 2022,
MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy,"[arxiv.Result.Author('Ya-Jie Zhang'), arxiv.Result.Author('Wei Song'), arxiv.Result.Author('Yanghao Yue'), arxiv.Result.Author('Zhengchen Zhang'), arxiv.Result.Author('Youzheng Wu'), arxiv.Result.Author('Xiaodong He')]",2022-11-11 12:48:27+00:00,"Humans often speak in a continuous manner which leads to coherent and
consistent prosody properties across neighboring utterances. However, most
state-of-the-art speech synthesis systems only consider the information within
each sentence and ignore the contextual semantic and acoustic features. This
makes it inadequate to generate high-quality paragraph-level speech which
requires high expressiveness and naturalness. To synthesize natural and
expressive speech for a paragraph, a context-aware speech synthesis system
named MaskedSpeech is proposed in this paper, which considers both contextual
semantic and acoustic features. Inspired by the masking strategy in the speech
editing research, the acoustic features of the current sentence are masked out
and concatenated with those of contextual speech, and further used as
additional model input. The phoneme encoder takes the concatenated phoneme
sequence from neighboring sentences as input and learns fine-grained semantic
information from contextual text. Furthermore, cross-utterance coarse-grained
semantic features are employed to improve the prosody generation. The model is
trained to reconstruct the masked acoustic features with the augmentation of
both the contextual semantic and acoustic features. Experimental results
demonstrate that the proposed MaskedSpeech outperformed the baseline system
significantly in terms of naturalness and expressiveness.",,
Continuous Emotional Intensity Controllable Speech Synthesis using Semi-supervised Learning,"[arxiv.Result.Author('Yoori Oh'), arxiv.Result.Author('Juheon Lee'), arxiv.Result.Author('Yoseob Han'), arxiv.Result.Author('Kyogu Lee')]",2022-11-11 12:28:07+00:00,"With the rapid development of the speech synthesis system, recent
text-to-speech models have reached the level of generating natural speech
similar to what humans say. But there still have limitations in terms of
expressiveness. In particular, the existing emotional speech synthesis models
have shown controllability using interpolated features with scaling parameters
in emotional latent space. However, the emotional latent space generated from
the existing models is difficult to control the continuous emotional intensity
because of the entanglement of features like emotions, speakers, etc. In this
paper, we propose a novel method to control the continuous intensity of
emotions using semi-supervised learning. The model learns emotions of
intermediate intensity using pseudo-labels generated from phoneme-level
sequences of speech information. An embedding space built from the proposed
model satisfies the uniform grid geometry with an emotional basis. In addition,
to improve the naturalness of intermediate emotional speech, a discriminator is
applied to the generation of low-level elements like duration, pitch and
energy. The experimental results showed that the proposed method was superior
in controllability and naturalness. The synthesized speech samples are
available at https://tinyurl.com/34zaehh2",Submitted to ICASSP 2023,
How Much Hate with #china? A Preliminary Analysis on China-related Hateful Tweets Two Years After the Covid Pandemic Began,"[arxiv.Result.Author('Jinghua Xu'), arxiv.Result.Author('Zarah Weiss')]",2022-11-11 10:48:00+00:00,"Following the outbreak of a global pandemic, online content is filled with
hate speech. Donald Trump's ''Chinese Virus'' tweet shifted the blame for the
spread of the Covid-19 virus to China and the Chinese people, which triggered a
new round of anti-China hate both online and offline. This research intends to
examine China-related hate speech on Twitter during the two years following the
burst of the pandemic (2020 and 2021). Through Twitter's API, in total
2,172,333 tweets hashtagged #china posted during the time were collected. By
employing multiple state-of-the-art pretrained language models for hate speech
detection, we identify a wide range of hate of various types, resulting in an
automatically labeled anti-China hate speech dataset. We identify a hateful
rate in #china tweets of 2.5% in 2020 and 1.9% in 2021. This is well above the
average rate of online hate speech on Twitter at 0.6% identified in Gao et al.,
2017. We further analyzed the longitudinal development of #china tweets and
those identified as hateful in 2020 and 2021 through visualizing the daily
number and hate rate over the two years. Our keyword analysis of hate speech in
#china tweets reveals the most frequently mentioned terms in the hateful #china
tweets, which can be used for further social science studies.",,
SceneFake: An Initial Dataset and Benchmarks for Scene Fake Audio Detection,"[arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Zhengkun Tian'), arxiv.Result.Author('Cunhang Fan'), arxiv.Result.Author('Haoxin Ma'), arxiv.Result.Author('Ruibo Fu')]",2022-11-11 09:05:50+00:00,"Previous databases have been designed to further the development of fake
audio detection. However, fake utterances are mostly generated by altering
timbre, prosody, linguistic content or channel noise of original audios. They
ignore a fake situation, in which the attacker manipulates an acoustic scene of
the original audio with another forgery one. It will pose a major threat to our
society if some people misuse the manipulated audio with malicious purpose.
Therefore, this motivates us to fill in the gap. This paper designs such a
dataset for scene fake audio detection (SceneFake). A manipulated audio in the
SceneFake dataset involves only tampering the acoustic scene of an utterance by
using speech enhancement technologies. We can not only detect fake utterances
on a seen test set but also evaluate the generalization of fake detection
models to unseen manipulation attacks. Some benchmark results are described on
the SceneFake dataset. Besides, an analysis of fake attacks with different
speech enhancement technologies and signal-to-noise ratios are presented on the
dataset. The results show that scene manipulated utterances can not be detected
reliably by the existing baseline models of ASVspoof 2019. Furthermore, the
detection of unseen scene manipulation audio is still challenging.",,
An Adapter based Multi-label Pre-training for Speech Separation and Enhancement,"[arxiv.Result.Author('Tianrui Wang'), arxiv.Result.Author('Xie Chen'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Shu Yu'), arxiv.Result.Author('Weibin Zhu')]",2022-11-11 07:34:32+00:00,"In recent years, self-supervised learning (SSL) has achieved tremendous
success in various speech tasks due to its power to extract representations
from massive unlabeled data. However, compared with tasks such as speech
recognition (ASR), the improvements from SSL representation in speech
separation (SS) and enhancement (SE) are considerably smaller. Based on HuBERT,
this work investigates improving the SSL model for SS and SE. We first update
HuBERT's masked speech prediction (MSP) objective by integrating the separation
and denoising terms, resulting in a multiple pseudo label pre-training scheme,
which significantly improves HuBERT's performance on SS and SE but degrades the
performance on ASR. To maintain its performance gain on ASR, we further propose
an adapter-based architecture for HuBERT's Transformer encoder, where only a
few parameters of each layer are adjusted to the multiple pseudo label MSP
while other parameters remain frozen as default HuBERT. Experimental results
show that our proposed adapter-based multiple pseudo label HuBERT yield
consistent and significant performance improvements on SE, SS, and ASR tasks,
with a faster pre-training speed, at only marginal parameters increase.",5 pages,
Continuous Soft Pseudo-Labeling in ASR,"[arxiv.Result.Author('Tatiana Likhomanenko'), arxiv.Result.Author('Ronan Collobert'), arxiv.Result.Author('Navdeep Jaitly'), arxiv.Result.Author('Samy Bengio')]",2022-11-11 05:16:18+00:00,"Continuous pseudo-labeling (PL) algorithms such as slimIPL have recently
emerged as a powerful strategy for semi-supervised learning in speech
recognition. In contrast with earlier strategies that alternated between
training a model and generating pseudo-labels (PLs) with it, here PLs are
generated in end-to-end manner as training proceeds, improving training speed
and the accuracy of the final model. PL shares a common theme with
teacher-student models such as distillation in that a teacher model generates
targets that need to be mimicked by the student model being trained. However,
interestingly, PL strategies in general use hard-labels, whereas distillation
uses the distribution over labels as the target to mimic. Inspired by
distillation we expect that specifying the whole distribution (aka soft-labels)
over sequences as the target for unlabeled data, instead of a single best pass
pseudo-labeled transcript (hard-labels) should improve PL performance and
convergence. Surprisingly and unexpectedly, we find that soft-labels targets
can lead to training divergence, with the model collapsing to a degenerate
token distribution per frame. We hypothesize that the reason this does not
happen with hard-labels is that training loss on hard-labels imposes
sequence-level consistency that keeps the model from collapsing to the
degenerate solution. In this paper, we show several experiments that support
this hypothesis, and experiment with several regularization approaches that can
ameliorate the degenerate collapse when using soft-labels. These approaches can
bring the accuracy of soft-labels closer to that of hard-labels, and while they
are unable to outperform them yet, they serve as a useful framework for further
improvements.",,
Acoustic Pornography Recognition Using Convolutional Neural Networks and Bag of Refinements,"[arxiv.Result.Author('Lifeng Zhou'), arxiv.Result.Author('Kaifeng Wei'), arxiv.Result.Author('Yuke Li'), arxiv.Result.Author('Yiya Hao'), arxiv.Result.Author('Weiqiang Yang'), arxiv.Result.Author('Haoqi Zhu')]",2022-11-11 03:21:32+00:00,"A large number of pornographic audios publicly available on the Internet
seriously threaten the mental and physical health of children, but these audios
are rarely detected and filtered. In this paper, we firstly propose a
convolutional neural networks (CNN) based model for acoustic pornography
recognition. Then, we research a collection of refinements and verify their
effectiveness through ablation studies. Finally, we stack all refinements
together to verify whether they can further improve the accuracy of the model.
Experimental results on our newly-collected large dataset consisting of 224127
pornographic audios and 274206 normal samples demonstrate the effectiveness of
our proposed model and these refinements. Specifically, the proposed model
achieves an accuracy of 92.46% and the accuracy is further improved to 97.19%
when all refinements are combined.",,
"Align, Write, Re-order: Explainable End-to-End Speech Translation via Operation Sequence Generation","[arxiv.Result.Author('Motoi Omachi'), arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Siddharth Dalmia'), arxiv.Result.Author('Yuya Fujita'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-11 02:29:28+00:00,"The black-box nature of end-to-end speech translation (E2E ST) systems makes
it difficult to understand how source language inputs are being mapped to the
target language. To solve this problem, we would like to simultaneously
generate automatic speech recognition (ASR) and ST predictions such that each
source language word is explicitly mapped to a target language word. A major
challenge arises from the fact that translation is a non-monotonic sequence
transduction task due to word ordering differences between languages -- this
clashes with the monotonic nature of ASR. Therefore, we propose to generate ST
tokens out-of-order while remembering how to re-order them later. We achieve
this by predicting a sequence of tuples consisting of a source word, the
corresponding target words, and post-editing operations dictating the correct
insertion points for the target word. We examine two variants of such operation
sequences which enable generation of monotonic transcriptions and non-monotonic
translations from the same speech input simultaneously. We apply our approach
to offline and real-time streaming models, demonstrating that we can provide
explainable translations without sacrificing quality or latency. In fact, the
delayed re-ordering ability of our approach improves performance during
streaming. As an added benefit, our method performs ASR and ST simultaneously,
making it faster than using two separate systems to perform these tasks.",,
A Gait Triaging Toolkit for Overlapping Acoustic Events in Indoor Environments,"[arxiv.Result.Author('Kelvin Summoogum'), arxiv.Result.Author('Debayan Das'), arxiv.Result.Author('Parvati Jayakumar')]",2022-11-11 01:33:14+00:00,"Gait has been used in clinical and healthcare applications to assess the
physical and cognitive health of older adults. Acoustic based gait detection is
a promising approach to collect gait data of older adults passively and
non-intrusively. However, there has been limited work in developing acoustic
based gait detectors that can operate in noisy polyphonic acoustic scenes of
homes and care homes. We attribute this to the lack of good quality gait
datasets from the real-world to train a gait detector on. In this paper, we put
forward a novel machine learning based filter which can triage gait audio
samples suitable for training machine learning models for gait detection. The
filter achieves this by eliminating noisy samples at an f(1) score of 0.85 and
prioritising gait samples with distinct spectral features and minimal noise. To
demonstrate the effectiveness of the filter, we train and evaluate a deep
learning model on gait datasets collected from older adults with and without
applying the filter. The model registers an increase of 25 points in its f(1)
score on unseen real-word gait data when trained with the filtered gait
samples. The proposed filter will help automate the task of manual annotation
of gait samples for training acoustic based gait detection models for older
adults in indoor environments.",5 pages,
Optimal Condition Training for Target Source Separation,"[arxiv.Result.Author('Efthymios Tzinis'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Paris Smaragdis'), arxiv.Result.Author('Jonathan Le Roux')]",2022-11-11 00:04:55+00:00,"Recent research has shown remarkable performance in leveraging multiple
extraneous conditional and non-mutually exclusive semantic concepts for sound
source separation, allowing the flexibility to extract a given target source
based on multiple different queries. In this work, we propose a new optimal
condition training (OCT) method for single-channel target source separation,
based on greedy parameter updates using the highest performing condition among
equivalent conditions associated with a given target source. Our experiments
show that the complementary information carried by the diverse semantic
concepts significantly helps to disentangle and isolate sources of interest
much more efficiently compared to single-conditioned models. Moreover, we
propose a variation of OCT with condition refinement, in which an initial
conditional vector is adapted to the given mixture and transformed to a more
amenable representation for target source extraction. We showcase the
effectiveness of OCT on diverse source separation experiments where it improves
upon permutation invariant models with oracle assignment and obtains
state-of-the-art performance in the more challenging task of text-based source
separation, outperforming even dedicated text-only conditioned models.",Submitted to ICASSP 2023,
"A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding","[arxiv.Result.Author('Yifan Peng'), arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Yosuke Higuchi'), arxiv.Result.Author('Yushi Ueda'), arxiv.Result.Author('Sujay Kumar'), arxiv.Result.Author('Karthik Ganesan'), arxiv.Result.Author('Siddharth Dalmia'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-10 20:59:13+00:00,"Collecting sufficient labeled data for spoken language understanding (SLU) is
expensive and time-consuming. Recent studies achieved promising results by
using pre-trained models in low-resource scenarios. Inspired by this, we aim to
ask: which (if any) pre-training strategies can improve performance across SLU
benchmarks? To answer this question, we employ four types of pre-trained models
and their combinations for SLU. We leverage self-supervised speech and language
models (LM) pre-trained on large quantities of unpaired data to extract strong
speech and text representations. We also explore using supervised models
pre-trained on larger external automatic speech recognition (ASR) or SLU
corpora. We conduct extensive experiments on the SLU Evaluation (SLUE)
benchmark and observe self-supervised pre-trained models to be more powerful,
with pre-trained LM and speech models being most beneficial for the Sentiment
Analysis and Named Entity Recognition task, respectively.",Accepted at SLT 2022,
"Remap, warp and attend: Non-parallel many-to-many accent conversion with Normalizing Flows","[arxiv.Result.Author('Abdelhamid Ezzerg'), arxiv.Result.Author('Thomas Merritt'), arxiv.Result.Author('Kayoko Yanagisawa'), arxiv.Result.Author('Piotr Bilinski'), arxiv.Result.Author('Magdalena Proszewska'), arxiv.Result.Author('Kamil Pokora'), arxiv.Result.Author('Renard Korzeniowski'), arxiv.Result.Author('Roberto Barra-Chicote'), arxiv.Result.Author('Daniel Korzekwa')]",2022-11-10 20:14:01+00:00,"Regional accents of the same language affect not only how words are
pronounced (i.e., phonetic content), but also impact prosodic aspects of speech
such as speaking rate and intonation. This paper investigates a novel
flow-based approach to accent conversion using normalizing flows. The proposed
approach revolves around three steps: remapping the phonetic conditioning, to
better match the target accent, warping the duration of the converted speech,
to better suit the target phonemes, and an attention mechanism that implicitly
aligns source and target speech sequences. The proposed remap-warp-attend
system enables adaptation of both phonetic and prosodic aspects of speech while
allowing for source and converted speech signals to be of different lengths.
Objective and subjective evaluations show that the proposed approach
significantly outperforms a competitive CopyCat baseline model in terms of
similarity to the target accent, naturalness and intelligibility.",IEEE Spoken Language Technology Workshop 2022,
"Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities","[arxiv.Result.Author('Andros Tjandra'), arxiv.Result.Author('Nayan Singhal'), arxiv.Result.Author('David Zhang'), arxiv.Result.Author('Ozlem Kalinli'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Duc Le'), arxiv.Result.Author('Michael L. Seltzer')]",2022-11-10 18:43:42+00:00,"End-to-end multilingual ASR has become more appealing because of several
reasons such as simplifying the training and deployment process and positive
performance transfer from high-resource to low-resource languages. However,
scaling up the number of languages, total hours, and number of unique tokens is
not a trivial task. This paper explores large-scale multilingual ASR models on
70 languages. We inspect two architectures: (1) Shared embedding and output and
(2) Multiple embedding and output model. In the shared model experiments, we
show the importance of tokenization strategy across different languages. Later,
we use our optimal tokenization strategy to train multiple embedding and output
model to further improve our result. Our multilingual ASR achieves 13.9%-15.6%
average WER relative improvement compared to monolingual models. We show that
our multilingual ASR generalizes well on an unseen dataset and domain,
achieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot
and finetuning, respectively.",Submitted to ICASSP 2023,
Self-supervised learning with bi-label masked speech prediction for streaming multi-talker speech recognition,"[arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Naoyuki Kanda'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Yiming Wang'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Takuya Yoshioka'), arxiv.Result.Author('Xiaofei Wang'), arxiv.Result.Author('Peidong Wang')]",2022-11-10 13:36:27+00:00,"Self-supervised learning (SSL), which utilizes the input data itself for
representation learning, has achieved state-of-the-art results for various
downstream speech tasks. However, most of the previous studies focused on
offline single-talker applications, with limited investigations in multi-talker
cases, especially for streaming scenarios. In this paper, we investigate SSL
for streaming multi-talker speech recognition, which generates transcriptions
of overlapping speakers in a streaming fashion. We first observe that
conventional SSL techniques do not work well on this task due to the poor
representation of overlapping speech. We then propose a novel SSL training
objective, referred to as bi-label masked speech prediction, which explicitly
preserves representations of all speakers in overlapping speech. We investigate
various aspects of the proposed system including data configuration and
quantizer selection. The proposed SSL setup achieves substantially better word
error rates on the LibriSpeechMix dataset.",submitted to ICASSP 2023,
Assistive Completion of Agrammatic Aphasic Sentences: A Transfer Learning Approach using Neurolinguistics-based Synthetic Dataset,"[arxiv.Result.Author('Rohit Misra'), arxiv.Result.Author('Sapna S Mishra'), arxiv.Result.Author('Tapan K. Gandhi')]",2022-11-10 13:24:02+00:00,"Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic
aphasia wherein patients, although able to comprehend, lack the ability to form
complete sentences. This inability leads to communication gaps which cause
difficulties in their daily lives. The usage of assistive devices can help in
mitigating these issues and enable the patients to communicate effectively.
However, due to lack of large scale studies of linguistic deficits in aphasia,
research on such assistive technology is relatively limited. In this work, we
present two contributions that aim to re-initiate research and development in
this field. Firstly, we propose a model that uses linguistic features from
small scale studies on aphasia patients and generates large scale datasets of
synthetic aphasic utterances from grammatically correct datasets. We show that
the mean length of utterance, the noun/verb ratio, and the simple/complex
sentence ratio of our synthetic datasets correspond to the reported features of
aphasic speech. Further, we demonstrate how the synthetic datasets may be
utilized to develop assistive devices for aphasia patients. The pre-trained T5
transformer is fine-tuned using the generated dataset to suggest 5 corrected
sentences given an aphasic utterance as input. We evaluate the efficacy of the
T5 model using the BLEU and cosine semantic similarity scores. Affirming
results with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00
were obtained. These results provide a strong foundation for the concept that a
synthetic dataset based on small scale studies on aphasia can be used to
develop effective assistive technology.",,
Vis2Mus: Exploring Multimodal Representation Mapping for Controllable Music Generation,"[arxiv.Result.Author('Runbang Zhang'), arxiv.Result.Author('Yixiao Zhang'), arxiv.Result.Author('Kai Shao'), arxiv.Result.Author('Ying Shan'), arxiv.Result.Author('Gus Xia')]",2022-11-10 13:01:26+00:00,"In this study, we explore the representation mapping from the domain of
visual arts to the domain of music, with which we can use visual arts as an
effective handle to control music generation. Unlike most studies in multimodal
representation learning that are purely data-driven, we adopt an
analysis-by-synthesis approach that combines deep music representation learning
with user studies. Such an approach enables us to discover
\textit{interpretable} representation mapping without a huge amount of paired
data. In particular, we discover that visual-to-music mapping has a nice
property similar to equivariant. In other words, we can use various image
transformations, say, changing brightness, changing contrast, style transfer,
to control the corresponding transformations in the music domain. In addition,
we released the Vis2Mus system as a controllable interface for symbolic music
generation.","Submitted to ICASSP 2023. GitHub repo:
  https://github.com/ldzhangyx/vis2mus",
Privacy-Utility Balanced Voice De-Identification Using Adversarial Examples,"[arxiv.Result.Author('Meng Chen'), arxiv.Result.Author('Li Lu'), arxiv.Result.Author('Jiadi Yu'), arxiv.Result.Author('Yingying Chen'), arxiv.Result.Author('Zhongjie Ba'), arxiv.Result.Author('Feng Lin'), arxiv.Result.Author('Kui Ren')]",2022-11-10 09:35:58+00:00,"Faced with the threat of identity leakage during voice data publishing, users
are engaged in a privacy-utility dilemma when enjoying convenient voice
services. Existing studies employ direct modification or text-based
re-synthesis to de-identify users' voices, but resulting in inconsistent
audibility in the presence of human participants. In this paper, we propose a
voice de-identification system, which uses adversarial examples to balance the
privacy and utility of voice services. Instead of typical additive examples
inducing perceivable distortions, we design a novel convolutional adversarial
example that modulates perturbations into real-world room impulse responses.
Benefit from this, our system could preserve user identity from exposure by
Automatic Speaker Identification (ASI) while remaining the voice perceptual
quality for non-intrusive de-identification. Moreover, our system learns a
compact speaker distribution through a conditional variational auto-encoder to
sample diverse target embeddings on demand. Combining diverse target generation
and input-specific perturbation construction, our system enables any-to-any
identify transformation for adaptive de-identification. Experimental results
show that our system could achieve 98% and 79% successful de-identification on
mainstream ASIs and commercial systems with an objective Mel cepstral
distortion of 4.31dB and a subjective mean opinion score of 4.48.",,
Self-supervised learning of audio representations using angular contrastive loss,"[arxiv.Result.Author('Shanshan Wang'), arxiv.Result.Author('Soumya Tripathy'), arxiv.Result.Author('Annamaria Mesaros')]",2022-11-10 09:32:10+00:00,"In Self-Supervised Learning (SSL), various pretext tasks are designed for
learning feature representations through contrastive loss. However, previous
studies have shown that this loss is less tolerant to semantically similar
samples due to the inherent defect of instance discrimination objectives, which
may harm the quality of learned feature embeddings used in downstream tasks. To
improve the discriminative ability of feature embeddings in SSL, we propose a
new loss function called Angular Contrastive Loss (ACL), a linear combination
of angular margin and contrastive loss. ACL improves contrastive learning by
explicitly adding an angular margin between positive and negative augmented
pairs in SSL. Experimental results show that using ACL for both supervised and
unsupervised learning significantly improves performance. We validated our new
loss function using the FSDnoisy18k dataset, where we achieved 73.6% and 77.1%
accuracy in sound event classification using supervised and self-supervised
learning, respectively.",,
Speech Enhancement with Fullband-Subband Cross-Attention Network,"[arxiv.Result.Author('Jun Chen'), arxiv.Result.Author('Wei Rao'), arxiv.Result.Author('Zilin Wang'), arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Yannan Wang'), arxiv.Result.Author('Tao Yu'), arxiv.Result.Author('Shidong Shang'), arxiv.Result.Author('Helen Meng')]",2022-11-10 09:17:06+00:00,"FullSubNet has shown its promising performance on speech enhancement by
utilizing both fullband and subband information. However, the relationship
between fullband and subband in FullSubNet is achieved by simply concatenating
the output of fullband model and subband units. It only supplements the subband
units with a small quantity of global information and has not considered the
interaction between fullband and subband. This paper proposes a
fullband-subband cross-attention (FSCA) module to interactively fuse the global
and local information and applies it to FullSubNet. This new framework is
called as FS-CANet. Moreover, different from FullSubNet, the proposed FS-CANet
optimize the fullband extractor by temporal convolutional network (TCN) blocks
to further reduce the model size. Experimental results on DNS Challenge -
Interspeech 2021 dataset show that the proposed FS-CANet outperforms other
state-of-the-art speech enhancement approaches, and demonstrate the
effectiveness of fullband-subband cross-attention.","Accepted by InterSpeech 2022. arXiv admin note: text overlap with
  arXiv:2203.12188",
GANStrument: Adversarial Instrument Sound Synthesis with Pitch-invariant Instance Conditioning,"[arxiv.Result.Author('Gaku Narita'), arxiv.Result.Author('Junichi Shimizu'), arxiv.Result.Author('Taketo Akama')]",2022-11-10 07:24:09+00:00,"We propose GANStrument, a generative adversarial model for instrument sound
synthesis. Given a one-shot sound as input, it is able to generate pitched
instrument sounds that reflect the timbre of the input within an interactive
time. By exploiting instance conditioning, GANStrument achieves better fidelity
and diversity of synthesized sounds and generalization ability to various
inputs. In addition, we introduce an adversarial training scheme for a
pitch-invariant feature extractor that significantly improves the pitch
accuracy and timbre consistency. Experimental results show that GANStrument
outperforms strong baselines that do not use instance conditioning in terms of
generation quality and input editability. Qualitative examples are available
online.","5 pages, 4 figures, Audio examples:
  https://ganstrument.github.io/ganstrument-demo/",
EmoFake: An Initial Dataset for Emotion Fake Audio Detection,"[arxiv.Result.Author('Yan Zhao'), arxiv.Result.Author('Jiangyan Yi'), arxiv.Result.Author('Jianhua Tao'), arxiv.Result.Author('Chenglong Wang'), arxiv.Result.Author('Chu Yuan Zhang'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Yongfeng Dong')]",2022-11-10 06:09:51+00:00,"There are already some datasets used for fake audio detection, such as the
ASVspoof and ADD datasets. However, these databases do not consider a situation
that the emotion of the audio has been changed from one to another, while other
information (e.g. speaker identity and content) remains the same. Changing
emotions often leads to semantic changes. This may be a great threat to social
stability. Therefore, this paper reports our progress in developing such an
emotion fake audio detection dataset involving changing emotion state of the
original audio. The dataset is named EmoFake. The fake audio in EmoFake is
generated using the state-of-the-art emotion voice conversion models. Some
benchmark experiments are conducted on this dataset. The results show that our
designed dataset poses a challenge to the LCNN and RawNet2 baseline models of
ASVspoof 2021.",,
A comparison of several AI techniques for authorship attribution on Romanian texts,"[arxiv.Result.Author('Sanda Maria Avram'), arxiv.Result.Author('Mihai Oltean')]",2022-11-09 20:24:48+00:00,"Determining the author of a text is a difficult task. Here we compare
multiple AI techniques for classifying literary texts written by multiple
authors by taking into account a limited number of speech parts (prepositions,
adverbs, and conjunctions). We also introduce a new dataset composed of texts
written in the Romanian language on which we have run the algorithms. The
compared methods are Artificial Neural Networks, Support Vector Machines, Multi
Expression Programming, Decision Trees with C5.0, and k-Nearest Neighbour.
Numerical experiments show, first of all, that the problem is difficult, but
some algorithms are able to generate decent errors on the test set.","We initially used the Accuracy evaluation tool to compute the
  macro-accuracy, obtaining a value of 88.84%. We, thereafter discovered that
  this value was erroneous and used other methods which gave us the value of
  80.94% for the macro-accuracy. In this version of the paper we present the
  python module solution by using sklearn.metrics's classification_report and
  balanced_accuracy_score","Mathematics 2022, 10(23), 4589"
Speech separation with large-scale self-supervised learning,"[arxiv.Result.Author('Zhuo Chen'), arxiv.Result.Author('Naoyuki Kanda'), arxiv.Result.Author('Jian Wu'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Xiaofei Wang'), arxiv.Result.Author('Takuya Yoshioka'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Sunit Sivasankaran'), arxiv.Result.Author('Sefik Emre Eskimez')]",2022-11-09 20:00:21+00:00,"Self-supervised learning (SSL) methods such as WavLM have shown promising
speech separation (SS) results in small-scale simulation-based experiments. In
this work, we extend the exploration of the SSL-based SS by massively scaling
up both the pre-training data (more than 300K hours) and fine-tuning data (10K
hours). We also investigate various techniques to efficiently integrate the
pre-trained model with the SS network under a limited computation budget,
including a low frame rate SSL model training setup and a fine-tuning scheme
using only the part of the pre-trained model. Compared with a supervised
baseline and the WavLM-based SS model using feature embeddings obtained with
the previously released 94K hours trained WavLM, our proposed model obtains
15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for
a simulated far-field speech mixture test set. For conversation transcription
on real meeting recordings using continuous speech separation, the proposed
model achieves 6.8% and 10.6% of relative WER reductions over the purely
supervised baseline on AMI and ICSI evaluation sets, respectively, while
reducing the computational cost by 38%.",,
Multimodal Dyadic Impression Recognition via Listener Adaptive Cross-Domain Fusion,"[arxiv.Result.Author('Yuanchao Li'), arxiv.Result.Author('Peter Bell'), arxiv.Result.Author('Catherine Lai')]",2022-11-09 19:23:00+00:00,"As a sub-branch of affective computing, impression recognition, e.g.,
perception of speaker characteristics such as warmth or competence, is
potentially a critical part of both human-human conversations and spoken
dialogue systems. Most research has studied impressions only from the behaviors
expressed by the speaker or the response from the listener, yet ignored their
latent connection. In this paper, we perform impression recognition using a
proposed listener adaptive cross-domain architecture, which consists of a
listener adaptation function to model the causality between speaker and
listener behaviors and a cross-domain fusion function to strengthen their
connection. The experimental evaluation on the dyadic IMPRESSION dataset
verified the efficacy of our method, producing concordance correlation
coefficients of 78.8% and 77.5% in the competence and warmth dimensions,
outperforming previous studies. The proposed method is expected to be
generalized to similar dyadic interaction scenarios.","submitted to ICASSP2023. arXiv admin note: substantial text overlap
  with arXiv:2203.13932",
Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models,"[arxiv.Result.Author('Travis M. Bartley'), arxiv.Result.Author('Fei Jia'), arxiv.Result.Author('Krishna C. Puvvada'), arxiv.Result.Author('Samuel Kriman'), arxiv.Result.Author('Boris Ginsburg')]",2022-11-09 18:53:59+00:00,"In this paper, we extend previous self-supervised approaches for language
identification by experimenting with Conformer based architecture in a
multilingual pre-training paradigm. We find that pre-trained speech models
optimally encode language discriminatory information in lower layers. Further,
we demonstrate that the embeddings obtained from these layers are significantly
robust to classify unseen languages and different acoustic environments without
additional training. After fine-tuning a pre-trained Conformer model on the
VoxLingua107 dataset, we achieve results similar to current state-of-the-art
systems for language identification. More, our model accomplishes this with 5x
less parameters. We open-source the model through the NVIDIA NeMo toolkit.",Submitted to ICASSP 2023,
A Diffeomorphic Flow-based Variational Framework for Multi-speaker Emotion Conversion,"[arxiv.Result.Author('Ravi Shankar'), arxiv.Result.Author('Hsi-Wei Hsieh'), arxiv.Result.Author('Nicolas Charon'), arxiv.Result.Author('Archana Venkataraman')]",2022-11-09 18:03:29+00:00,"This paper introduces a new framework for non-parallel emotion conversion in
speech. Our framework is based on two key contributions. First, we propose a
stochastic version of the popular CycleGAN model. Our modified loss function
introduces a Kullback Leibler (KL) divergence term that aligns the source and
target data distributions learned by the generators, thus overcoming the
limitations of sample wise generation. By using a variational approximation to
this stochastic loss function, we show that our KL divergence term can be
implemented via a paired density discriminator. We term this new architecture a
variational CycleGAN (VCGAN). Second, we model the prosodic features of target
emotion as a smooth and learnable deformation of the source prosodic features.
This approach provides implicit regularization that offers key advantages in
terms of better range alignment to unseen and out of distribution speakers. We
conduct rigorous experiments and comparative studies to demonstrate that our
proposed framework is fairly robust with high performance against several
state-of-the-art baselines.","Accepted in IEEE Transactions on Audio, Speech and Language
  Processing",
A Comparative Study of Data Augmentation Techniques for Deep Learning Based Emotion Recognition,"[arxiv.Result.Author('Ravi Shankar'), arxiv.Result.Author('Abdouh Harouna Kenfack'), arxiv.Result.Author('Arjun Somayazulu'), arxiv.Result.Author('Archana Venkataraman')]",2022-11-09 17:27:03+00:00,"Automated emotion recognition in speech is a long-standing problem. While
early work on emotion recognition relied on hand-crafted features and simple
classifiers, the field has now embraced end-to-end feature learning and
classification using deep neural networks. In parallel to these models,
researchers have proposed several data augmentation techniques to increase the
size and variability of existing labeled datasets. Despite many seminal
contributions in the field, we still have a poor understanding of the interplay
between the network architecture and the choice of data augmentation. Moreover,
only a handful of studies demonstrate the generalizability of a particular
model across multiple datasets, which is a prerequisite for robust real-world
performance. In this paper, we conduct a comprehensive evaluation of popular
deep learning approaches for emotion recognition. To eliminate bias, we fix the
model architectures and optimization hyperparameters using the VESUS dataset
and then use repeated 5-fold cross validation to evaluate the performance on
the IEMOCAP and CREMA-D datasets. Our results demonstrate that long-range
dependencies in the speech signal are critical for emotion recognition and that
speed/rate augmentation offers the most robust performance gain across models.",Under Submission,
Efficient Speech Translation with Pre-trained Models,"[arxiv.Result.Author('Zhaolin Li'), arxiv.Result.Author('Jan Niehues')]",2022-11-09 15:07:06+00:00,"When building state-of-the-art speech translation models, the need for large
computational resources is a significant obstacle due to the large training
data size and complex models. The availability of pre-trained models is a
promising opportunity to build strong speech translation systems efficiently.
In a first step, we investigate efficient strategies to build cascaded and
end-to-end speech translation systems based on pre-trained models. Using this
strategy, we can train and apply the models on a single GPU. While the
end-to-end models show superior translation performance to cascaded ones, the
application of this technology has a limitation on the need for additional
end-to-end training data. In a second step, we proposed an additional
similarity loss to encourage the model to generate similar hidden
representations for speech and transcript. Using this technique, we can
increase the data efficiency and improve the translation quality by 6 BLEU
points in scenarios with limited end-to-end training data.",,
Utilising Bayesian Networks to combine multimodal data and expert opinion for the robust prediction of depression and its symptoms,"[arxiv.Result.Author('Salvatore Fara'), arxiv.Result.Author('Orlaith Hickey'), arxiv.Result.Author('Alexandra Georgescu'), arxiv.Result.Author('Stefano Goria'), arxiv.Result.Author('Emilia Molimpakis'), arxiv.Result.Author('Nicholas Cummins')]",2022-11-09 14:48:13+00:00,"Predicting the presence of major depressive disorder (MDD) using behavioural
and cognitive signals is a highly non-trivial task. The heterogeneous clinical
profile of MDD means that any given speech, facial expression and/or observed
cognitive pattern may be associated with a unique combination of depressive
symptoms. Conventional discriminative machine learning models potentially lack
the complexity to robustly model this heterogeneity. Bayesian networks,
however, may instead be well-suited to such a scenario. These networks are
probabilistic graphical models that efficiently describe the joint probability
distribution over a set of random variables by explicitly capturing their
conditional dependencies. This framework provides further advantages over
standard discriminative modelling by offering the possibility to incorporate
expert opinion in the graphical structure of the models, generating explainable
model predictions, informing about the uncertainty of predictions, and
naturally handling missing data. In this study, we apply a Bayesian framework
to capture the relationships between depression, depression symptoms, and
features derived from speech, facial expression and cognitive game data
collected at thymia.",Submitted to ICASSP 2023,
"Global, and Local Optimization Beamforming for Broadband Sources",[arxiv.Result.Author('Armin Goudarzi')],2022-11-09 14:45:56+00:00,"This paper presents an extension to global optimization beamforming for
acoustic broadband sources. Given, that properties such as the source location,
spatial shape, multipole rotation, or flow conditions can be parameterized over
the frequency, a CSM-fitting can be performed for all frequencies at the same
time. A numerical analysis shows that the non-linear error function for the
standard global optimization problem is similar to the source's Point Spread
Function and contains local minima, but can be improved with the proposed
broadband error. Not only increases the broadband optimization process the
ratio of equations to unknown variables, but it also smooths out the cost
function. It also simplifies the process of identifying sources and
reconstructing their spectra from the results. The paper shows that the method
is superior on synthetic monopoles compared to standard global optimization and
CLEAN-SC. For real-world data the results of broadband global optimization,
standard global optimization, and CLEAN-SC are similar. However, the proposed
method does not require the identification and integration of Regions Of
Interest. Additionally it is shown, that by using reasonable initial values the
global optimization problem reduces to a local optimization problem with
similar results. Further, it is shown that the proposed method is able to
identify multipoles with different pole amplitudes and unknown pole rotations.",Submitted to Journal of Sound and Vibration,
Distribution-based Emotion Recognition in Conversation,"[arxiv.Result.Author('Wen Wu'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Philip C. Woodland')]",2022-11-09 12:16:28+00:00,"Automatic emotion recognition in conversation (ERC) is crucial for
emotion-aware conversational artificial intelligence. This paper proposes a
distribution-based framework that formulates ERC as a sequence-to-sequence
problem for emotion distribution estimation. The inherent ambiguity of emotions
and the subjectivity of human perception lead to disagreements in emotion
labels, which is handled naturally in our framework from the perspective of
uncertainty estimation in emotion distributions. A Bayesian training loss is
introduced to improve the uncertainty estimation by conditioning each emotional
state on an utterance-specific Dirichlet prior distribution. Experimental
results on the IEMOCAP dataset show that ERC outperformed the
single-utterance-based system, and the proposed distribution-based ERC methods
have not only better classification accuracy, but also show improved
uncertainty estimation.",To appear in SLT 2022,
Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation,"[arxiv.Result.Author('Florian Schmid'), arxiv.Result.Author('Khaled Koutini'), arxiv.Result.Author('Gerhard Widmer')]",2022-11-09 09:58:22+00:00,"Audio Spectrogram Transformer models rule the field of Audio Tagging,
outrunning previously dominating Convolutional Neural Networks (CNNs). Their
superiority is based on the ability to scale up and exploit large-scale
datasets such as AudioSet. However, Transformers are demanding in terms of
model size and computational requirements compared to CNNs. We propose a
training procedure for efficient CNNs based on offline Knowledge Distillation
(KD) from high-performing yet complex transformers. The proposed training
schema and the efficient CNN design based on MobileNetV3 results in models
outperforming previous solutions in terms of parameter and computational
efficiency and prediction performance. We provide models of different
complexity levels, scaling from low-complexity models up to a new
state-of-the-art performance of .483 mAP on AudioSet. Source Code available at:
https://github.com/fschmid56/EfficientAT","Submitted to ICASSP 2023. Source Code available at:
  https://github.com/fschmid56/EfficientAT",
Absolute decision corrupts absolutely: conservative online speaker diarisation,"[arxiv.Result.Author('Youngki Kwon'), arxiv.Result.Author('Hee-Soo Heo'), arxiv.Result.Author('Bong-Jin Lee'), arxiv.Result.Author('You Jin Kim'), arxiv.Result.Author('Jee-weon Jung')]",2022-11-09 09:52:40+00:00,"Our focus lies in developing an online speaker diarisation framework which
demonstrates robust performance across diverse domains. In online speaker
diarisation, outputs generated in real-time are irreversible, and a few
misjudgements in the early phase of an input session can lead to catastrophic
results. We hypothesise that cautiously increasing the number of estimated
speakers is of paramount importance among many other factors. Thus, our
proposed framework includes decreasing the number of speakers by one when the
system judges that an increase in the past was faulty. We also adopt dual
buffers, checkpoints and centroids, where checkpoints are combined with
silhouette coefficients to estimate the number of speakers and centroids
represent speakers. Again, we believe that more than one centroid can be
generated from one speaker. Thus we design a clustering-based label matching
technique to assign labels in real-time. The resulting system is lightweight
yet surprisingly effective. The system demonstrates state-of-the-art
performance on DIHARD 2 and 3 datasets, where it is also competitive in AMI and
VoxConverse test sets.","5pages, 2 figure, 4 tables, submitted to ICASSP",
Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition,"[arxiv.Result.Author('Yu Chen'), arxiv.Result.Author('Wen Ding'), arxiv.Result.Author('Junjie Lai')]",2022-11-09 07:23:15+00:00,"Noisy Student Training (NST) has recently demonstrated extremely strong
performance in Automatic Speech Recognition (ASR). In this paper, we propose a
data selection strategy named LM Filter to improve the performances of NST on
non-target domain data in ASR tasks. Hypothesis with and without Language Model
are generated and CER differences between them are utilized as a filter
threshold. Results reveal that significant improvements of 10.4% compared with
no data filtering baselines. We can achieve 3.31% CER in AISHELL-1 test set,
which is best result from our knowledge without any other supervised data. We
also perform evaluations on supervised 1000 hour AISHELL-2 dataset and
competitive results of 4.72% CER can be achieved.",,
Expressive-VC: Highly Expressive Voice Conversion with Attention Fusion of Bottleneck and Perturbation Features,"[arxiv.Result.Author('Ziqian Ning'), arxiv.Result.Author('Qicong Xie'), arxiv.Result.Author('Pengcheng Zhu'), arxiv.Result.Author('Zhichao Wang'), arxiv.Result.Author('Liumeng Xue'), arxiv.Result.Author('Jixun Yao'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Mengxiao Bi')]",2022-11-09 07:03:59+00:00,"Voice conversion for highly expressive speech is challenging. Current
approaches struggle with the balancing between speaker similarity,
intelligibility and expressiveness. To address this problem, we propose
Expressive-VC, a novel end-to-end voice conversion framework that leverages
advantages from both neural bottleneck feature (BNF) approach and information
perturbation approach. Specifically, we use a BNF encoder and a Perturbed-Wav
encoder to form a content extractor to learn linguistic and para-linguistic
features respectively, where BNFs come from a robust pre-trained ASR model and
the perturbed wave becomes speaker-irrelevant after signal perturbation. We
further fuse the linguistic and para-linguistic features through an attention
mechanism, where speaker-dependent prosody features are adopted as the
attention query, which result from a prosody encoder with target speaker
embedding and normalized pitch and energy of source speech as input. Finally
the decoder consumes the integrated features and the speaker-dependent prosody
feature to generate the converted speech. Experiments demonstrate that
Expressive-VC is superior to several state-of-the-art systems, achieving both
high expressiveness captured from the source speech and high speaker similarity
with the target speaker; meanwhile intelligibility is well maintained.",,
Adaptive Multi-Corpora Language Model Training for Speech Recognition,"[arxiv.Result.Author('Yingyi Ma'), arxiv.Result.Author('Zhe Liu'), arxiv.Result.Author('Xuedong Zhang')]",2022-11-09 06:54:50+00:00,"Neural network language model (NNLM) plays an essential role in automatic
speech recognition (ASR) systems, especially in adaptation tasks when text-only
data is available. In practice, an NNLM is typically trained on a combination
of data sampled from multiple corpora. Thus, the data sampling strategy is
important to the adaptation performance. Most existing works focus on designing
static sampling strategies. However, each corpus may show varying impacts at
different NNLM training stages. In this paper, we introduce a novel adaptive
multi-corpora training algorithm that dynamically learns and adjusts the
sampling probability of each corpus along the training process. The algorithm
is robust to corpora sizes and domain relevance. Compared with static sampling
strategy baselines, the proposed approach yields remarkable improvement by
achieving up to relative 7% and 9% word error rate (WER) reductions on
in-domain and out-of-domain adaptation tasks, respectively.",,
FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration,"[arxiv.Result.Author('Yangjun Wu'), arxiv.Result.Author('Kebin Fang'), arxiv.Result.Author('Yao Zhao'), arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Lifeng Shi'), arxiv.Result.Author('Mengqi Zhang')]",2022-11-09 06:18:17+00:00,"To accomplish punctuation restoration, most existing methods focus on
introducing extra information (e.g., part-of-speech) or addressing the class
imbalance problem. Recently, large-scale transformer-based pre-trained language
models (PLMS) have been utilized widely and obtained remarkable success.
However, the PLMS are trained on the large dataset with marks, which may not
fit well with the small dataset without marks, causing the convergence to be
not ideal. In this study, we propose a Feature Fusion two-stream framework
(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained
language model to capture the semantic feature, while another auxiliary module
captures the feature at hand. We also modify the computation of multi-head
attention to encourage communication among heads. Then, two features with
different perspectives are aggregated to fuse information and enhance context
awareness. Without additional data, the experimental results on the popular
benchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which
verifies that our approach is effective.","5pages. arXiv admin note: substantial text overlap with
  arXiv:2203.12487",
MFDNet: Towards Real-time Image Denoising On Mobile Devices,"[arxiv.Result.Author('Zhuoqun Liu'), arxiv.Result.Author('Meiguang Jin'), arxiv.Result.Author('Ying Chen'), arxiv.Result.Author('Huaida Liu'), arxiv.Result.Author('Canqian Yang'), arxiv.Result.Author('Hongkai Xiong')]",2022-11-09 05:19:26+00:00,"Deep convolutional neural networks have achieved great progress in image
denoising tasks. However, their complicated architectures and heavy
computational cost hinder their deployments on a mobile device. Some recent
efforts in designing lightweight denoising networks focus on reducing either
FLOPs (floating-point operations) or the number of parameters. However, these
metrics are not directly correlated with the on-device latency. By performing
extensive analysis and experiments, we identify the network architectures that
can fully utilize powerful neural processing units (NPUs) and thus enjoy both
low latency and excellent denoising performance. To this end, we propose a
mobile-friendly denoising network, namely MFDNet. The experiments show that
MFDNet achieves state-of-the-art performance on real-world denoising benchmarks
SIDD and DND under real-time latency on mobile devices. The code and
pre-trained models will be released.","Under review at the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2023)",
LiCo-Net: Linearized Convolution Network for Hardware-efficient Keyword Spotting,"[arxiv.Result.Author('Haichuan Yang'), arxiv.Result.Author('Zhaojun Yang'), arxiv.Result.Author('Li Wan'), arxiv.Result.Author('Biqiao Zhang'), arxiv.Result.Author('Yangyang Shi'), arxiv.Result.Author('Yiteng Huang'), arxiv.Result.Author('Ivaylo Enchev'), arxiv.Result.Author('Limin Tang'), arxiv.Result.Author('Raziel Alvarez'), arxiv.Result.Author('Ming Sun'), arxiv.Result.Author('Xin Lei'), arxiv.Result.Author('Raghuraman Krishnamoorthi'), arxiv.Result.Author('Vikas Chandra')]",2022-11-09 01:52:17+00:00,"This paper proposes a hardware-efficient architecture, Linearized Convolution
Network (LiCo-Net) for keyword spotting. It is optimized specifically for
low-power processor units like microcontrollers. ML operators exhibit
heterogeneous efficiency profiles on power-efficient hardware. Given the exact
theoretical computation cost, int8 operators are more computation-effective
than float operators, and linear layers are often more efficient than other
layers. The proposed LiCo-Net is a dual-phase system that uses the efficient
int8 linear operators at the inference phase and applies streaming convolutions
at the training phase to maintain a high model capacity. The experimental
results show that LiCo-Net outperforms single-value decomposition filter (SVDF)
on hardware efficiency with on-par detection performance. Compared to SVDF,
LiCo-Net reduces cycles by 40% on HiFi4 DSP.",,
PhaseAug: A Differentiable Augmentation for Speech Synthesis to Simulate One-to-Many Mapping,"[arxiv.Result.Author('Junhyeok Lee'), arxiv.Result.Author('Seungu Han'), arxiv.Result.Author('Hyunjae Cho'), arxiv.Result.Author('Wonbin Jung')]",2022-11-08 23:37:05+00:00,"Previous generative adversarial network (GAN)-based neural vocoders are
trained to reconstruct the exact ground truth waveform from the paired
mel-spectrogram and do not consider the one-to-many relationship of speech
synthesis. This conventional training causes overfitting for both the
discriminators and the generator, leading to the periodicity artifacts in the
generated audio signal. In this work, we present PhaseAug, the first
differentiable augmentation for speech synthesis that rotates the phase of each
frequency bin to simulate one-to-many mapping. With our proposed method, we
outperform baselines without any architecture modification. Code and audio
samples will be available at https://github.com/mindslab-ai/phaseaug.",Submitted to ICASSP 2023,
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,"[arxiv.Result.Author('Paul-Ambroise Duquenne'), arxiv.Result.Author('Hongyu Gong'), arxiv.Result.Author('Ning Dong'), arxiv.Result.Author('Jingfei Du'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Vedanuj Goswani'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Juan Pino'), arxiv.Result.Author('Benoît Sagot'), arxiv.Result.Author('Holger Schwenk')]",2022-11-08 19:09:27+00:00,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available.",18 pages,
A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer,"[arxiv.Result.Author('Loukas Ilias'), arxiv.Result.Author('Dimitris Askounis'), arxiv.Result.Author('John Psarras')]",2022-11-08 16:43:58+00:00,"Alzheimer's disease (AD) is a progressive neurological disorder, meaning that
the symptoms develop gradually throughout the years. It is also the main cause
of dementia, which affects memory, thinking skills, and mental abilities.
Nowadays, researchers have moved their interest towards AD detection from
spontaneous speech, since it constitutes a time-effective procedure. However,
existing state-of-the-art works proposing multimodal approaches do not take
into consideration the inter- and intra-modal interactions and propose early
and late fusion approaches. To tackle these limitations, we propose deep neural
networks, which can be trained in an end-to-end trainable way and capture the
inter- and intra-modal interactions. Firstly, each audio file is converted to
an image consisting of three channels, i.e., log-Mel spectrogram, delta, and
delta-delta. Next, each transcript is passed through a BERT model followed by a
gated self-attention layer. Similarly, each image is passed through a Swin
Transformer followed by an independent gated self-attention layer. Acoustic
features are extracted also from each audio file. Finally, the representation
vectors from the different modalities are fed to a tensor fusion layer for
capturing the inter-modal interactions. Extensive experiments conducted on the
ADReSS Challenge dataset indicate that our introduced approaches obtain
valuable advantages over existing research initiatives reaching Accuracy and
F1-score up to 86.25% and 85.48% respectively.","2022 IEEE-EMBS International Conference on Biomedical and Health
  Informatics (BHI) - Oral Presentation",
Cross-Attention is all you need: Real-Time Streaming Transformers for Personalised Speech Enhancement,"[arxiv.Result.Author('Shucong Zhang'), arxiv.Result.Author('Malcolm Chadwick'), arxiv.Result.Author('Alberto Gil C. P. Ramos'), arxiv.Result.Author('Sourav Bhattacharya')]",2022-11-08 16:12:38+00:00,"Personalised speech enhancement (PSE), which extracts only the speech of a
target user and removes everything else from a recorded audio clip, can
potentially improve users' experiences of audio AI modules deployed in the
wild. To support a large variety of downstream audio tasks, such as real-time
ASR and audio-call enhancement, a PSE solution should operate in a streaming
mode, i.e., input audio cleaning should happen in real-time with a small
latency and real-time factor. Personalisation is typically achieved by
extracting a target speaker's voice profile from an enrolment audio, in the
form of a static embedding vector, and then using it to condition the output of
a PSE model. However, a fixed target speaker embedding may not be optimal under
all conditions. In this work, we present a streaming Transformer-based PSE
model and propose a novel cross-attention approach that gives adaptive target
speaker representations. We present extensive experiments and show that our
proposed cross-attention approach outperforms competitive baselines
consistently, even when our model is only approximately half the size.",,
DiffPhase: Generative Diffusion-based STFT Phase Retrieval,"[arxiv.Result.Author('Tal Peer'), arxiv.Result.Author('Simon Welker'), arxiv.Result.Author('Timo Gerkmann')]",2022-11-08 15:50:35+00:00,"Diffusion probabilistic models have been recently used in a variety of tasks,
including speech enhancement and synthesis. As a generative approach, diffusion
models have been shown to be especially suitable for imputation problems, where
missing data is generated based on existing data. Phase retrieval is inherently
an imputation problem, where phase information has to be generated based on the
given magnitude. In this work we build upon previous work in the speech domain,
adapting a speech enhancement diffusion model specifically for STFT phase
retrieval. Evaluation using speech quality and intelligibility metrics shows
the diffusion approach is well-suited to the phase retrieval task, with
performance surpassing both classical and modern methods.",Submitted to ICASSP 2023,
BER: Balanced Error Rate For Speaker Diarization,"[arxiv.Result.Author('Tao Liu'), arxiv.Result.Author('Kai Yu')]",2022-11-08 15:17:39+00:00,"DER is the primary metric to evaluate diarization performance while facing a
dilemma: the errors in short utterances or segments tend to be overwhelmed by
longer ones. Short segments, e.g., `yes' or `no,' still have semantic
information. Besides, DER overlooks errors in less-talked speakers. Although
JER balances speaker errors, it still suffers from the same dilemma.
Considering all those aspects, duration error, segment error, and
speaker-weighted error constituting a complete diarization evaluation, we
propose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we
propose a segment-level error rate (SER) via connected sub-graphs and adaptive
IoU threshold to get accurate segment matching. Second, to evaluate diarization
in a unified way, we adopt a speaker-specific harmonic mean between duration
and segment, followed by a speaker-weighted average. Third, we analyze our
metric via the modularized system, EEND, and the multi-modal method on real
datasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.","5 pages, 2 figures",
Single-anchor UWB Localization using Channel Impulse Response Distributions,"[arxiv.Result.Author('Sitian Li'), arxiv.Result.Author('Alexios Balatsoukas-Stimming'), arxiv.Result.Author('Andreas Burg')]",2022-11-08 13:49:49+00:00,"Ultra-wideband (UWB) devices are widely used in indoor localization
scenarios. Single-anchor UWB localization shows advantages because of its
simple system setup compared to conventional two-way ranging (TWR) and
trilateration localization methods. In this work, we focus on single-anchor UWB
localization methods that learn statistical features of the channel impulse
response (CIR) in different location areas using a Gaussian mixture model
(GMM). We show that by learning the joint distributions of the amplitudes of
different delay components, we achieve a more accurate location estimate
compared to considering each delay bin independently. Moreover, we develop a
similarity metric between sets of CIRs. With this set-based similarity metric,
we can further improve the estimation performance, compared to treating each
snapshot separately. We showcase the advantages of the proposed methods in
multiple application scenarios.","submitted to the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2023)",
Pushing the limits of self-supervised speaker verification using regularized distillation framework,"[arxiv.Result.Author('Yafeng Chen'), arxiv.Result.Author('Siqi Zheng'), arxiv.Result.Author('Hui Wang'), arxiv.Result.Author('Luyao Cheng'), arxiv.Result.Author('Qian Chen')]",2022-11-08 11:21:38+00:00,"Training robust speaker verification systems without speaker labels has long
been a challenging task. Previous studies observed a large performance gap
between self-supervised and fully supervised methods. In this paper, we apply a
non-contrastive self-supervised learning framework called DIstillation with NO
labels (DINO) and propose two regularization terms applied to embeddings in
DINO. One regularization term guarantees the diversity of the embeddings, while
the other regularization term decorrelates the variables of each embedding. The
effectiveness of various data augmentation techniques are explored, on both
time and frequency domain. A range of experiments conducted on the VoxCeleb
datasets demonstrate the superiority of the regularized DINO framework in
speaker verification. Our method achieves the state-of-the-art speaker
verification performance under a single-stage self-supervised setting on
VoxCeleb. The codes will be made publicly-available.",,
Unsupervised vocal dereverberation with diffusion-based generative models,"[arxiv.Result.Author('Koichi Saito'), arxiv.Result.Author('Naoki Murata'), arxiv.Result.Author('Toshimitsu Uesaka'), arxiv.Result.Author('Chieh-Hsin Lai'), arxiv.Result.Author('Yuhta Takida'), arxiv.Result.Author('Takao Fukui'), arxiv.Result.Author('Yuki Mitsufuji')]",2022-11-08 09:43:01+00:00,"Removing reverb from reverberant music is a necessary technique to clean up
audio for downstream music manipulations. Reverberation of music contains two
categories, natural reverb, and artificial reverb. Artificial reverb has a
wider diversity than natural reverb due to its various parameter setups and
reverberation types. However, recent supervised dereverberation methods may
fail because they rely on sufficiently diverse and numerous pairs of
reverberant observations and retrieved data for training in order to be
generalizable to unseen observations during inference. To resolve these
problems, we propose an unsupervised method that can remove a general kind of
artificial reverb for music without requiring pairs of data for training. The
proposed method is based on diffusion models, where it initializes the unknown
reverberation operator with a conventional signal processing technique and
simultaneously refines the estimate with the help of diffusion models. We show
through objective and perceptual evaluations that our method outperforms the
current leading vocal dereverberation benchmarks.","6 pages, 2 figures, submitted to ICASSP 2023",
Improving performance of real-time full-band blind packet-loss concealment with predictive network,"[arxiv.Result.Author('Viet-Anh Nguyen'), arxiv.Result.Author('Anh H. T. Nguyen'), arxiv.Result.Author('Andy W. H. Khong')]",2022-11-08 08:04:25+00:00,"Packet loss concealment (PLC) is a tool for enhancing speech degradation
caused by poor network conditions or underflow/overflow in audio processing
pipelines. We propose a real-time recurrent method that leverages previous
outputs to mitigate artefact of lost packets without the prior knowledge of
loss mask. The proposed full-band recurrent network (FRN) model operates at 48
kHz, which is suitable for high-quality telecommunication applications.
Experiment results highlight the superiority of FRN over an offline non-causal
baseline and a top performer in a recent PLC challenge.","Submitted to ICASSP 2023, 5 pages, 1 figure, 4 tables",
On Negative Sampling for Contrastive Audio-Text Retrieval,"[arxiv.Result.Author('Huang Xie'), arxiv.Result.Author('Okko Räsänen'), arxiv.Result.Author('Tuomas Virtanen')]",2022-11-08 08:03:05+00:00,"This paper investigates negative sampling for contrastive learning in the
context of audio-text retrieval. The strategy for negative sampling refers to
selecting negatives (either audio clips or textual descriptions) from a pool of
candidates for a positive audio-text pair. We explore sampling strategies via
model-estimated within-modality and cross-modality relevance scores for audio
and text samples. With a constant training setting on the retrieval system from
[1], we study eight sampling strategies, including hard and semi-hard negative
sampling. Experimental results show that retrieval performance varies
dramatically among different strategies. Particularly, by selecting semi-hard
negatives with cross-modality scores, the retrieval system gains improved
performance in both text-to-audio and audio-to-text retrieval. Besides, we show
that feature collapse occurs while sampling hard negatives with cross-modality
scores.",Submitted to ICASSP2023,
High-resolution embedding extractor for speaker diarisation,"[arxiv.Result.Author('Hee-Soo Heo'), arxiv.Result.Author('Youngki Kwon'), arxiv.Result.Author('Bong-Jin Lee'), arxiv.Result.Author('You Jin Kim'), arxiv.Result.Author('Jee-weon Jung')]",2022-11-08 07:41:18+00:00,"Speaker embedding extractors significantly influence the performance of
clustering-based speaker diarisation systems. Conventionally, only one
embedding is extracted from each speech segment. However, because of the
sliding window approach, a segment easily includes two or more speakers owing
to speaker change points. This study proposes a novel embedding extractor
architecture, referred to as a high-resolution embedding extractor (HEE), which
extracts multiple high-resolution embeddings from each speech segment. Hee
consists of a feature-map extractor and an enhancer, where the enhancer with
the self-attention mechanism is the key to success. The enhancer of HEE
replaces the aggregation process; instead of a global pooling layer, the
enhancer combines relative information to each frame via attention leveraging
the global context. Extracted dense frame-level embeddings can each represent a
speaker. Thus, multiple speakers can be represented by different frame-level
features in each segment. We also propose an artificially generating mixture
data training framework to train the proposed HEE. Through experiments on five
evaluation sets, including four public datasets, the proposed HEE demonstrates
at least 10% improvement on each evaluation set, except for one dataset, which
we analyse that rapid speaker changes less exist.","5pages, 2 figure, 3 tables, submitted to ICASSP",
ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications,"[arxiv.Result.Author('Juan Zuluaga-Gomez'), arxiv.Result.Author('Karel Veselý'), arxiv.Result.Author('Igor Szöke'), arxiv.Result.Author('Petr Motlicek'), arxiv.Result.Author('Martin Kocour'), arxiv.Result.Author('Mickael Rigault'), arxiv.Result.Author('Khalid Choukri'), arxiv.Result.Author('Amrutha Prasad'), arxiv.Result.Author('Seyyed Saeed Sarfjoo'), arxiv.Result.Author('Iuliia Nigmatulina'), arxiv.Result.Author('Claudia Cevenini'), arxiv.Result.Author('Pavel Kolčárek'), arxiv.Result.Author('Allan Tart'), arxiv.Result.Author('Jan Černocký')]",2022-11-08 07:26:45+00:00,"Personal assistants, automatic speech recognizers and dialogue understanding
systems are becoming more critical in our interconnected digital world. A clear
example is air traffic control (ATC) communications. ATC aims at guiding
aircraft and controlling the airspace in a safe and optimal manner. These
voice-based dialogues are carried between an air traffic controller (ATCO) and
pilots via very-high frequency radio channels. In order to incorporate these
novel technologies into ATC (low-resource domain), large-scale annotated
datasets are required to develop the data-driven AI systems. Two examples are
automatic speech recognition (ASR) and natural language understanding (NLU). In
this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering
research on the challenging ATC field, which has lagged behind due to lack of
annotated data. The ATCO2 corpus covers 1) data collection and pre-processing,
2) pseudo-annotations of speech data, and 3) extraction of ATC-related named
entities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set
corpus contains 4 hours of ATC speech with manual transcripts and a subset with
gold annotations for named-entity recognition (callsign, command, value). 2)
The ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched
with automatic transcripts from an in-domain speech recognizer, contextual
information, speaker turn information, signal-to-noise ratio estimate and
English language detection score per sample. Both available for purchase
through ELDA at http://catalog.elra.info/en-us/repository/browse/ELRA-S0484. 3)
The ATCO2-test-set-1h corpus is a one-hour subset from the original test set
corpus, that we are offering for free at https://www.atco2.org/data. We expect
the ATCO2 corpus will foster research on robust ASR and NLU not only in the
field of ATC communications but also in the general research community.","Manuscript under review; The code will be available at
  https://github.com/idiap/atco2-corpus",
Robust Unstructured Knowledge Access in Conversational Dialogue with ASR Errors,"[arxiv.Result.Author('Yik-Cheung Tam'), arxiv.Result.Author('Jiacheng Xu'), arxiv.Result.Author('Jiakai Zou'), arxiv.Result.Author('Zecheng Wang'), arxiv.Result.Author('Tinglong Liao'), arxiv.Result.Author('Shuhan Yuan')]",2022-11-08 04:04:02+00:00,"Performance of spoken language understanding (SLU) can be degraded with
automatic speech recognition (ASR) errors. We propose a novel approach to
improve SLU robustness by randomly corrupting clean training text with an ASR
error simulator, followed by self-correcting the errors and minimizing the
target classification loss in a joint manner. In the proposed error simulator,
we leverage confusion networks generated from an ASR decoder without human
transcriptions to generate a variety of error patterns for model training. We
evaluate our approach on the DSTC10 challenge targeted for knowledge-grounded
task-oriented conversational dialogues with ASR errors. Experimental results
show the effectiveness of our proposed approach, boosting the knowledge-seeking
turn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster
classification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge
document re-ranking, our approach shows significant improvement in all
knowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to
0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 on the test set. In the
recent DSTC10 evaluation, our approach demonstrates significant improvement in
knowledge selection, boosting Recall@1 from 0.495 to 0.7144 compared to the
official baseline. Our source code is released in GitHub
https://github.com/yctam/dstc10_track2_task2.git.","7 pages, 2 figures. Accepted at ICASSP 2022","ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), 2022, pp. 6702-6706"
Comparative layer-wise analysis of self-supervised speech models,"[arxiv.Result.Author('Ankita Pasad'), arxiv.Result.Author('Bowen Shi'), arxiv.Result.Author('Karen Livescu')]",2022-11-08 00:59:05+00:00,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive empirical successes on downstream tasks, we still
have a limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.","Submitted to ICASSP 2022. Code:
  https://github.com/ankitapasad/layerwise-analysis",
Towards Improved Room Impulse Response Estimation for Speech Recognition,"[arxiv.Result.Author('Anton Ratnarajah'), arxiv.Result.Author('Ishwarya Ananthabhotla'), arxiv.Result.Author('Vamsi Krishna Ithapu'), arxiv.Result.Author('Pablo Hoffmann'), arxiv.Result.Author('Dinesh Manocha'), arxiv.Result.Author('Paul Calamia')]",2022-11-08 00:40:27+00:00,"We propose to characterize and improve the performance of blind room impulse
response (RIR) estimation systems in the context of a downstream application
scenario, far-field automatic speech recognition (ASR). We first draw the
connection between improved RIR estimation and improved ASR performance, as a
means of evaluating neural RIR estimators. We then propose a GAN-based
architecture that encodes RIR features from reverberant speech and constructs
an RIR from the encoded features, and uses a novel energy decay relief loss to
optimize for capturing energy-based properties of the input reverberant speech.
We show that our model outperforms the state-of-the-art baselines on acoustic
benchmarks (by 72% on the energy decay relief and 22% on an early-reflection
energy metric), as well as in an ASR evaluation task (by 6.9% in word error
rate).",,
"Streaming, fast and accurate on-device Inverse Text Normalization for Automatic Speech Recognition","[arxiv.Result.Author('Yashesh Gaur'), arxiv.Result.Author('Nick Kibre'), arxiv.Result.Author('Jian Xue'), arxiv.Result.Author('Kangyuan Shu'), arxiv.Result.Author('Yuhui Wang'), arxiv.Result.Author('Issac Alphanso'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Yifan Gong')]",2022-11-07 17:48:54+00:00,"Automatic Speech Recognition (ASR) systems typically yield output in lexical
form. However, humans prefer a written form output. To bridge this gap, ASR
systems usually employ Inverse Text Normalization (ITN).
  In previous works, Weighted Finite State Transducers (WFST) have been
employed to do ITN. WFSTs are nicely suited to this task but their size and
run-time costs can make deployment on embedded applications challenging.
  In this paper, we describe the development of an on-device ITN system that is
streaming, lightweight & accurate. At the core of our system is a streaming
transformer tagger, that tags lexical tokens from ASR. The tag informs which
ITN category might be applied, if at all. Following that, we apply an
ITN-category-specific WFST, only on the tagged text, to reliably perform the
ITN conversion. We show that the proposed ITN solution performs equivalent to
strong baselines, while being significantly smaller in size and retaining
customization capabilities.",8 pages. 6 page paper 2 page references,
Egocentric Audio-Visual Noise Suppression,"[arxiv.Result.Author('Roshan Sharma'), arxiv.Result.Author('Weipeng He'), arxiv.Result.Author('Ju Lin'), arxiv.Result.Author('Egor Lakomkin'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Kaustubh Kalgaonkar')]",2022-11-07 15:53:12+00:00,"This paper studies audio-visual suppression for egocentric videos -- where
the speaker is not captured in the video. Instead, potential noise sources are
visible on screen with the camera emulating the off-screen speaker's view of
the outside world. This setting is different from prior work in audio-visual
speech enhancement that relies on lip and facial visuals. In this paper, we
first demonstrate that egocentric visual information is helpful for noise
suppression. We compare object recognition and action classification based
visual feature extractors, and investigate methods to align audio and visual
representations. Then, we examine different fusion strategies for the aligned
features, and locations within the noise suppression model to incorporate
visual information. Experiments demonstrate that visual features are most
helpful when used to generate additive correction masks. Finally, in order to
ensure that the visual features are discriminative with respect to different
noise types, we introduce a multi-task learning framework that jointly
optimizes audio-visual noise suppression and video based acoustic event
detection. This proposed multi-task framework outperforms the audio only
baseline on all metrics, including a 0.16 PESQ improvement. Extensive ablations
reveal the improved performance of the proposed model with multiple active
distractors, over all noise types and across different SNRs.",Under Review at ICASSP 2023,
Generalized Product-of-Experts for Learning Multimodal Representations in Noisy Environments,"[arxiv.Result.Author('Abhinav Joshi'), arxiv.Result.Author('Naman Gupta'), arxiv.Result.Author('Jinang Shah'), arxiv.Result.Author('Binod Bhattarai'), arxiv.Result.Author('Ashutosh Modi'), arxiv.Result.Author('Danail Stoyanov')]",2022-11-07 14:27:38+00:00,"A real-world application or setting involves interaction between different
modalities (e.g., video, speech, text). In order to process the multimodal
information automatically and use it for an end application, Multimodal
Representation Learning (MRL) has emerged as an active area of research in
recent times. MRL involves learning reliable and robust representations of
information from heterogeneous sources and fusing them. However, in practice,
the data acquired from different sources are typically noisy. In some extreme
cases, a noise of large magnitude can completely alter the semantics of the
data leading to inconsistencies in the parallel multimodal data. In this paper,
we propose a novel method for multimodal representation learning in a noisy
environment via the generalized product of experts technique. In the proposed
method, we train a separate network for each modality to assess the credibility
of information coming from that modality, and subsequently, the contribution
from each modality is dynamically varied while estimating the joint
distribution. We evaluate our method on two challenging benchmarks from two
diverse domains: multimodal 3D hand-pose estimation and multimodal surgical
video segmentation. We attain state-of-the-art performance on both benchmarks.
Our extensive quantitative and qualitative evaluations show the advantages of
our method compared to previous approaches.","11 Pages, Accepted at ICMI 2022 Oral",
ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech,"[arxiv.Result.Author('Xiaoran Fan'), arxiv.Result.Author('Chao Pang'), arxiv.Result.Author('Tian Yuan'), arxiv.Result.Author('He Bai'), arxiv.Result.Author('Renjie Zheng'), arxiv.Result.Author('Pengfei Zhu'), arxiv.Result.Author('Shuohuan Wang'), arxiv.Result.Author('Junkun Chen'), arxiv.Result.Author('Zeyu Chen'), arxiv.Result.Author('Liang Huang'), arxiv.Result.Author('Yu Sun'), arxiv.Result.Author('Hua Wu')]",2022-11-07 13:35:16+00:00,"Speech representation learning has improved both speech understanding and
speech synthesis tasks for single language. However, its ability in
cross-lingual scenarios has not been explored. In this paper, we extend the
pretraining method for cross-lingual multi-speaker speech synthesis tasks,
including cross-lingual multi-speaker voice cloning and cross-lingual
multi-speaker speech editing. We propose a speech-text joint pretraining
framework, where we randomly mask the spectrogram and the phonemes given a
speech example and its transcription. By learning to reconstruct the masked
parts of the input in different languages, our model shows great improvements
over speaker-embedding-based multi-speaker TTS methods. Moreover, our framework
is end-to-end for both the training and the inference without any finetuning
effort. In cross-lingual multi-speaker voice cloning and cross-lingual
multi-speaker speech editing tasks, our experiments show that our model
outperforms speaker-embedding-based multi-speaker TTS methods.",,
Adaptive User-Centered Multimodal Interaction towards Reliable and Trusted Automotive Interfaces,[arxiv.Result.Author('Amr Gomaa')],2022-11-07 13:31:00+00:00,"With the recently increasing capabilities of modern vehicles, novel
approaches for interaction emerged that go beyond traditional touch-based and
voice command approaches. Therefore, hand gestures, head pose, eye gaze, and
speech have been extensively investigated in automotive applications for object
selection and referencing. Despite these significant advances, existing
approaches mostly employ a one-model-fits-all approach unsuitable for varying
user behavior and individual differences. Moreover, current referencing
approaches either consider these modalities separately or focus on a stationary
situation, whereas the situation in a moving vehicle is highly dynamic and
subject to safety-critical constraints. In this paper, I propose a research
plan for a user-centered adaptive multimodal fusion approach for referencing
external objects from a moving vehicle. The proposed plan aims to provide an
open-source framework for user-centered adaptation and personalization using
user observations and heuristics, multimodal fusion, clustering,
transfer-of-learning for model adaptation, and continuous learning, moving
towards trusted human-centered artificial intelligence.",,"In Proceedings of the 2022 International Conference on Multimodal
  Interaction, pp. 690-695. 2022"
End-to-End Evaluation of a Spoken Dialogue System for Learning Basic Mathematics,"[arxiv.Result.Author('Eda Okur'), arxiv.Result.Author('Saurav Sahay'), arxiv.Result.Author('Roddy Fuentes Alba'), arxiv.Result.Author('Lama Nachman')]",2022-11-07 12:58:24+00:00,"The advances in language-based Artificial Intelligence (AI) technologies
applied to build educational applications can present AI for social-good
opportunities with a broader positive impact. Across many disciplines,
enhancing the quality of mathematics education is crucial in building critical
thinking and problem-solving skills at younger ages. Conversational AI systems
have started maturing to a point where they could play a significant role in
helping students learn fundamental math concepts. This work presents a
task-oriented Spoken Dialogue System (SDS) built to support play-based learning
of basic math concepts for early childhood education. The system has been
evaluated via real-world deployments at school while the students are
practicing early math concepts with multimodal interactions. We discuss our
efforts to improve the SDS pipeline built for math learning, for which we
explore utilizing MathBERT representations for potential enhancement to the
Natural Language Understanding (NLU) module. We perform an end-to-end
evaluation using real-world deployment outputs from the Automatic Speech
Recognition (ASR), Intent Recognition, and Dialogue Manager (DM) components to
understand how error propagation affects the overall performance in real-world
scenarios.","Proceedings of the 1st Workshop on Mathematical Natural Language
  Processing (MathNLP) at EMNLP 2022",
Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering,"[arxiv.Result.Author('Helena Bonaldi'), arxiv.Result.Author('Sara Dellantonio'), arxiv.Result.Author('Serra Sinem Tekiroglu'), arxiv.Result.Author('Marco Guerini')]",2022-11-07 10:37:13+00:00,"Fighting online hate speech is a challenge that is usually addressed using
Natural Language Processing via automatic detection and removal of hate
content. Besides this approach, counter narratives have emerged as an effective
tool employed by NGOs to respond to online hate on social media platforms. For
this reason, Natural Language Generation is currently being studied as a way to
automatize counter narrative writing. However, the existing resources necessary
to train NLG models are limited to 2-turn interactions (a hate speech and a
counter narrative as response), while in real life, interactions can consist of
multiple turns. In this paper, we present a hybrid approach for dialogical data
collection, which combines the intervention of human expert annotators over
machine generated dialogues obtained using 19 different configurations. The
result of this work is DIALOCONAN, the first dataset comprising over 3000
fictitious multi-turn dialogues between a hater and an NGO operator, covering 6
targets of hate.","To appear in Proceedings of the 2022 Conference on Empirical Methods
  in Natural Language Processing (long paper)",
"Hi,KIA: A Speech Emotion Recognition Dataset for Wake-Up Words","[arxiv.Result.Author('Taesu Kim'), arxiv.Result.Author('SeungHeon Doh'), arxiv.Result.Author('Gyunpyo Lee'), arxiv.Result.Author('Hyungseok Jeon'), arxiv.Result.Author('Juhan Nam'), arxiv.Result.Author('Hyeon-Jeong Suk')]",2022-11-07 08:57:16+00:00,"Wake-up words (WUW) is a short sentence used to activate a speech recognition
system to receive the user's speech input. WUW utterances include not only the
lexical information for waking up the system but also non-lexical information
such as speaker identity or emotion. In particular, recognizing the user's
emotional state may elaborate the voice communication. However, there is few
dataset where the emotional state of the WUW utterances is labeled. In this
paper, we introduce Hi, KIA, a new WUW dataset which consists of 488 Korean
accent emotional utterances collected from four male and four female speakers
and each of utterances is labeled with four emotional states including anger,
happy, sad, or neutral. We present the step-by-step procedure to build the
dataset, covering scenario selection, post-processing, and human validation for
label agreement. Also, we provide two classification models for WUW speech
emotion recognition using the dataset. One is based on traditional hand-craft
features and the other is a transfer-learning approach using a pre-trained
neural network. These classification models could be used as benchmarks in
further research.","Asia Pacific Signal and Information Processing Association Annual
  Summit and Conference (APSIPA), 2022",
Accented Text-to-Speech Synthesis with a Conditional Variational Autoencoder,"[arxiv.Result.Author('Jan Melechovsky'), arxiv.Result.Author('Ambuj Mehrish'), arxiv.Result.Author('Berrak Sisman'), arxiv.Result.Author('Dorien Herremans')]",2022-11-07 05:36:30+00:00,"Accent plays a significant role in speech communication, influencing
understanding capabilities and also conveying a person's identity. This paper
introduces a novel and efficient framework for accented Text-to-Speech (TTS)
synthesis based on a Conditional Variational Autoencoder. It has the ability to
synthesize a selected speaker's speech that is converted to any desired target
accent. Our thorough experiments validate the effectiveness of our proposed
framework using both objective and subjective evaluations. The results also
show remarkable performance in terms of the ability to manipulate accents in
the synthesized speech and provide a promising avenue for future accented TTS
research.","preprint submitted to a conference, under review",
Peak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization,"[arxiv.Result.Author('Zhengkun Tian'), arxiv.Result.Author('Hongyu Xiang'), arxiv.Result.Author('Min Li'), arxiv.Result.Author('Feifei Lin'), arxiv.Result.Author('Ke Ding'), arxiv.Result.Author('Guanglu Wan')]",2022-11-07 03:36:00+00:00,"The CTC model has been widely applied to many application scenarios because
of its simple structure, excellent performance, and fast inference speed. There
are many peaks in the probability distribution predicted by the CTC models, and
each peak represents a non-blank token. The recognition latency of CTC models
can be reduced by encouraging the model to predict peaks earlier. Existing
methods to reduce latency require modifying the transition relationship between
tokens in the forward-backward algorithm, and the gradient calculation. Some of
these methods even depend on the forced alignment results provided by other
pretrained models. The above methods are complex to implement. To reduce the
peak latency, we propose a simple and novel method named peak-first
regularization, which utilizes a frame-wise knowledge distillation function to
force the probability distribution of the CTC model to shift left along the
time axis instead of directly modifying the calculation process of CTC loss and
gradients. All the experiments are conducted on a Chinese Mandarin dataset
AISHELL-1. We have verified the effectiveness of the proposed regularization on
both streaming and non-streaming CTC models respectively. The results show that
the proposed method can reduce the average peak latency by about 100 to 200
milliseconds with almost no degradation of recognition accuracy.","Submitted to ICASSP 2023(5 pages, 2 figures)",
Robust Total Least Mean M-Estimate normalized subband filter Adaptive Algorithm for impulse noises and noisy inputs,"[arxiv.Result.Author('Haiquan Zhao'), arxiv.Result.Author('Zian Cao'), arxiv.Result.Author('Yida Chen')]",2022-11-07 03:28:04+00:00,"When the input signal is correlated input signals, and the input and output
signal is contaminated by Gaussian noise, the total least squares normalized
subband adaptive filter (TLS-NSAF) algorithm shows good performance. However,
when it is disturbed by impulse noise, the TLS-NSAF algorithm shows the rapidly
deteriorating convergence performance. To solve this problem, this paper
proposed the robust total minimum mean M-estimator normalized subband filter
(TLMM-NSAF) algorithm. In addition, this paper also conducts a detailed
theoretical performance analysis of the TLMM-NSAF algorithm and obtains the
stable step size range and theoretical steady-state mean squared deviation
(MSD) of the algorithm. To further improve the performance of the algorithm, we
also propose a new variable step size (VSS) method of the algorithm. Finally,
the robustness of our proposed algorithm and the consistency of theoretical and
simulated values are verified by computer simulations of system identification
and echo cancellation under different noise models.",,
A Context-Aware Computational Approach for Measuring Vocal Entrainment in Dyadic Conversations,"[arxiv.Result.Author('Rimita Lahiri'), arxiv.Result.Author('Md Nasir'), arxiv.Result.Author('Catherine Lord'), arxiv.Result.Author('So Hyun Kim'), arxiv.Result.Author('Shrikanth Narayanan')]",2022-11-07 03:07:37+00:00,"Vocal entrainment is a social adaptation mechanism in human interaction,
knowledge of which can offer useful insights to an individual's
cognitive-behavioral characteristics. We propose a context-aware approach for
measuring vocal entrainment in dyadic conversations. We use conformers(a
combination of convolutional network and transformer) for capturing both
short-term and long-term conversational context to model entrainment patterns
in interactions across different domains. Specifically we use cross-subject
attention layers to learn intra- as well as inter-personal signals from dyadic
conversations. We first validate the proposed method based on classification
experiments to distinguish between real(consistent) and
fake(inconsistent/shuffled) conversations. Experimental results on interactions
involving individuals with Autism Spectrum Disorder also show evidence of a
statistically-significant association between the introduced entrainment
measure and clinical scores relevant to symptoms, including across gender and
age groups.",,
Parallel Attention Forcing for Machine Translation,"[arxiv.Result.Author('Qingyun Dou'), arxiv.Result.Author('Mark Gales')]",2022-11-06 23:29:07+00:00,"Attention-based autoregressive models have achieved state-of-the-art
performance in various sequence-to-sequence tasks, including Text-To-Speech
(TTS) and Neural Machine Translation (NMT), but can be difficult to train. The
standard training approach, teacher forcing, guides a model with the reference
back-history. During inference, the generated back-history must be used. This
mismatch limits the evaluation performance. Attention forcing has been
introduced to address the mismatch, guiding the model with the generated
back-history and reference attention. While successful in tasks with continuous
outputs like TTS, attention forcing faces additional challenges in tasks with
discrete outputs like NMT. This paper introduces the two extensions of
attention forcing to tackle these challenges. (1) Scheduled attention forcing
automatically turns attention forcing on and off, which is essential for tasks
with discrete outputs. (2) Parallel attention forcing makes training parallel,
and is applicable to Transformer-based models. The experiments show that the
proposed approaches improve the performance of models based on RNNs and
Transformers.","13 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2104.01264",
Deliberation Networks and How to Train Them,"[arxiv.Result.Author('Qingyun Dou'), arxiv.Result.Author('Mark Gales')]",2022-11-06 20:47:18+00:00,"Deliberation networks are a family of sequence-to-sequence models, which have
achieved state-of-the-art performance in a wide range of tasks such as machine
translation and speech synthesis. A deliberation network consists of multiple
standard sequence-to-sequence models, each one conditioned on the initial input
and the output of the previous model. During training, there are several key
questions: whether to apply Monte Carlo approximation to the gradients or the
loss, whether to train the standard models jointly or separately, whether to
run an intermediate model in teacher forcing or free running mode, whether to
apply task-specific techniques. Previous work on deliberation networks
typically explores one or two training options for a specific task. This work
introduces a unifying framework, covering various training options, and
addresses the above questions. In general, it is simpler to approximate the
gradients. When parallel training is essential, separate training should be
adopted. Regardless of the task, the intermediate model should be in free
running mode. For tasks where the output is continuous, a guided attention loss
can be used to prevent degradation into a standard model.","10 pages, 2 figures",
"""Seeing Sound"": Audio Classification with the Wigner-Wille Distribution and Convolutional Neural Networks","[arxiv.Result.Author('Antonios Marios Christonasis'), arxiv.Result.Author('Stef van Eijndhoven'), arxiv.Result.Author('Peter Duin')]",2022-11-06 19:01:02+00:00,"With big data becoming increasingly available, IoT hardware becoming widely
adopted, and AI capabilities becoming more powerful, organizations are
continuously investing in sensing. Data coming from sensor networks are
currently combined with sensor fusion and AI algorithms to drive innovation in
fields such as self-driving cars. Data from these sensors can be utilized in
numerous use cases, including alerts in safety systems of urban settings, for
events such as gun shots and explosions. Moreover, diverse types of sensors,
such as sound sensors, can be utilized in low-light conditions or at locations
where a camera is not available. This paper investigates the potential of the
utilization of sound-sensor data in an urban context. Technically, we propose a
novel approach of classifying sound data using the Wigner-Ville distribution
and Convolutional Neural Networks. In this paper, we report on the performance
of the approach on open-source datasets. The concept and work presented is
based on my doctoral thesis, which was performed as part of the Engineering
Doctorate program in Data Science at the University of Eindhoven, in
collaboration with the Dutch National Police. Additional work on real-world
datasets was performed during the thesis, which are not presented here due to
confidentiality.",,
Going In Style: Audio Backdoors Through Stylistic Transformations,"[arxiv.Result.Author('Stefanos Koffas'), arxiv.Result.Author('Luca Pajola'), arxiv.Result.Author('Stjepan Picek'), arxiv.Result.Author('Mauro Conti')]",2022-11-06 13:39:45+00:00,"A backdoor attack places triggers in victims' deep learning models to enable
a targeted misclassification at testing time. In general, triggers are fixed
artifacts attached to samples, making backdoor attacks easy to spot. Only
recently, a new trigger generation harder to detect has been proposed: the
stylistic triggers that apply stylistic transformations to the input samples
(e.g., a specific writing style). Currently, stylistic backdoor literature
lacks a proper formalization of the attack, which is established in this paper.
Moreover, most studies of stylistic triggers focus on text and images, while
there is no understanding of whether they can work in sound. This work fills
this gap. We propose JingleBack, the first stylistic backdoor attack based on
audio transformations such as chorus and gain. Using 444 models in a speech
classification task, we confirm the feasibility of stylistic triggers in audio,
achieving 96% attack success.",,
I Hear Your True Colors: Image Guided Audio Generation,"[arxiv.Result.Author('Roy Sheffer'), arxiv.Result.Author('Yossi Adi')]",2022-11-06 11:48:20+00:00,"We propose Im2Wav, an image guided open-domain audio generation system. Given
an input image or a sequence of images, Im2Wav generates a semantically
relevant sound. Im2Wav is based on two Transformer language models, that
operate over a hierarchical discrete audio representation obtained from a
VQ-VAE based model. We first produce a low-level audio representation using a
language model. Then, we upsample the audio tokens using an additional language
model to generate a high-fidelity audio sample. We use the rich semantics of a
pre-trained CLIP embedding as a visual representation to condition the language
model. In addition, to steer the generation process towards the conditioning
image, we apply the classifier-free guidance method. Results suggest that
Im2Wav significantly outperforms the evaluated baselines in both fidelity and
relevance evaluation metrics. Additionally, we provide an ablation study to
better assess the impact of each of the method components on overall
performance. Lastly, to better evaluate image-to-audio models, we propose an
out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a
benchmark for evaluating future image-to-audio models. Samples and code can be
found inside the manuscript.",,
An Empirical Study on L2 Accents of Cross-lingual Text-to-Speech Systems via Vowel Space,"[arxiv.Result.Author('Jihwan Lee'), arxiv.Result.Author('Jae-Sung Bae'), arxiv.Result.Author('Seongkyu Mun'), arxiv.Result.Author('Heejin Choi'), arxiv.Result.Author('Joun Yeop Lee'), arxiv.Result.Author('Hoon-Young Cho'), arxiv.Result.Author('Chanwoo Kim')]",2022-11-06 10:45:01+00:00,"With the recent developments in cross-lingual Text-to-Speech (TTS) systems,
L2 (second-language, or foreign) accent problems arise. Moreover, running a
subjective evaluation for such cross-lingual TTS systems is troublesome. The
vowel space analysis, which is often utilized to explore various aspects of
language including L2 accents, is a great alternative analysis tool. In this
study, we apply the vowel space analysis method to explore L2 accents of
cross-lingual TTS systems. Through the vowel space analysis, we observe the
three followings: a) a parallel architecture (Glow-TTS) is less L2-accented
than an auto-regressive one (Tacotron); b) L2 accents are more dominant in
non-shared vowels in a language pair; and c) L2 accents of cross-lingual TTS
systems share some phenomena with those of human L2 learners. Our findings
imply that it is necessary for TTS systems to handle each language pair
differently, depending on their linguistic characteristics such as non-shared
vowels. They also hint that we can further incorporate linguistics knowledge in
developing cross-lingual TTS systems.",Submitted to ICASSP 2023,
Distinguishable Speaker Anonymization based on Formant and Fundamental Frequency Scaling,"[arxiv.Result.Author('Jixun Yao'), arxiv.Result.Author('Qing Wang'), arxiv.Result.Author('Yi Lei'), arxiv.Result.Author('Pengcheng Guo'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Namin Wang'), arxiv.Result.Author('Jie Liu')]",2022-11-06 06:08:44+00:00,"Speech data on the Internet are proliferating exponentially because of the
emergence of social media, and the sharing of such personal data raises obvious
security and privacy concerns. One solution to mitigate these concerns involves
concealing speaker identities before sharing speech data, also referred to as
speaker anonymization. In our previous work, we have developed an automatic
speaker verification (ASV)-model-free anonymization framework to protect
speaker privacy while preserving speech intelligibility. Although the framework
ranked first place in VoicePrivacy 2022 challenge, the anonymization was
imperfect, since the speaker distinguishability of the anonymized speech was
deteriorated. To address this issue, in this paper, we directly model the
formant distribution and fundamental frequency (F0) to represent speaker
identity and anonymize the source speech by the uniformly scaling formant and
F0. By directly scaling the formant and F0, the speaker distinguishability
degradation of the anonymized speech caused by the introduction of other
speakers is prevented. The experimental results demonstrate that our proposed
framework can improve the speaker distinguishability and significantly
outperforms our previous framework in voice distinctiveness. Furthermore, our
proposed method also can trade off the privacy-utility by using different
scaling factors.",Submitted to ICASSP 2023,
Preserving background sound in noise-robust voice conversion via multi-task learning,"[arxiv.Result.Author('Jixun Yao'), arxiv.Result.Author('Yi Lei'), arxiv.Result.Author('Qing Wang'), arxiv.Result.Author('Pengcheng Guo'), arxiv.Result.Author('Ziqian Ning'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Hai Li'), arxiv.Result.Author('Junhui Liu'), arxiv.Result.Author('Danming Xie')]",2022-11-06 06:00:50+00:00,"Background sound is an informative form of art that is helpful in providing a
more immersive experience in real-application voice conversion (VC) scenarios.
However, prior research about VC, mainly focusing on clean voices, pay rare
attention to VC with background sound. The critical problem for preserving
background sound in VC is inevitable speech distortion by the neural separation
model and the cascade mismatch between the source separation model and the VC
model. In this paper, we propose an end-to-end framework via multi-task
learning which sequentially cascades a source separation (SS) module, a
bottleneck feature extraction module and a VC module. Specifically, the source
separation task explicitly considers critical phase information and confines
the distortion caused by the imperfect separation process. The source
separation task, the typical VC task and the unified task shares a uniform
reconstruction loss constrained by joint training to reduce the mismatch
between the SS and VC modules. Experimental results demonstrate that our
proposed framework significantly outperforms the baseline systems while
achieving comparable quality and speaker similarity to the VC models trained
with clean data.",Submitted to ICASSP 2023,
Bridging Speech and Textual Pre-trained Models with Unsupervised ASR,"[arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Chan-Jan Hsu'), arxiv.Result.Author('Holam Chung'), arxiv.Result.Author('Dongji Gao'), arxiv.Result.Author('Paola Garcia'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Hung-yi Lee')]",2022-11-06 04:50:37+00:00,"Spoken language understanding (SLU) is a task aiming to extract high-level
semantics from spoken utterances. Previous works have investigated the use of
speech self-supervised models and textual pre-trained models, which have shown
reasonable improvements to various SLU tasks. However, because of the
mismatched modalities between speech signals and text tokens, previous methods
usually need complex designs of the frameworks. This work proposes a simple yet
efficient unsupervised paradigm that connects speech and textual pre-trained
models, resulting in an unsupervised speech-to-semantic pre-trained model for
various tasks in SLU. To be specific, we propose to use unsupervised automatic
speech recognition (ASR) as a connector that bridges different modalities used
in speech and textual pre-trained models. Our experiments show that
unsupervised ASR itself can improve the representations from speech
self-supervised models. More importantly, it is shown as an efficient connector
between speech and textual pre-trained models, improving the performances of
five different SLU tasks. Notably, on spoken question answering, we reach the
state-of-the-art result over the challenging NMSQA benchmark.",ICASSP2023 submission,
Breaking the trade-off in personalized speech enhancement with cross-task knowledge distillation,"[arxiv.Result.Author('Hassan Taherian'), arxiv.Result.Author('Sefik Emre Eskimez'), arxiv.Result.Author('Takuya Yoshioka')]",2022-11-05 17:02:55+00:00,"Personalized speech enhancement (PSE) models achieve promising results
compared with unconditional speech enhancement models due to their ability to
remove interfering speech in addition to background noise. Unlike unconditional
speech enhancement, causal PSE models may occasionally remove the target speech
by mistake. The PSE models also tend to leak interfering speech when the target
speaker is silent for an extended period. We show that existing PSE methods
suffer from a trade-off between speech over-suppression and interference
leakage by addressing one problem at the expense of the other. We propose a new
PSE model training framework using cross-task knowledge distillation to
mitigate this trade-off. Specifically, we utilize a personalized voice activity
detector (pVAD) during training to exclude the non-target speech frames that
are wrongly identified as containing the target speaker with hard or soft
classification. This prevents the PSE model from being too aggressive while
still allowing the model to learn to suppress the input speech when it is
likely to be spoken by interfering speakers. Comprehensive evaluation results
are presented, covering various PSE usage scenarios.",Submitted to ICASSP 2023,
Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block,"[arxiv.Result.Author('Yunhao Chen'), arxiv.Result.Author('Yunjie Zhu'), arxiv.Result.Author('Zihui Yan'), arxiv.Result.Author('Lifang Chen')]",2022-11-05 16:47:17+00:00,"Recently, massive architectures based on Convolutional Neural Network (CNN)
and self-attention mechanisms have become necessary for audio classification.
While these techniques are state-of-the-art, these works' effectiveness can
only be guaranteed with huge computational costs and parameters, large amounts
of data augmentation, transfer from large datasets and some other tricks. By
utilizing the lightweight nature of audio, we propose an efficient network
structure called Paired Inverse Pyramid Structure (PIP) and a network called
Paired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\% of
Environmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset
and 93.2\% of Music Genre Classification (MGC) on the GTAZN dataset, with only
1 million parameters. Both of the results are achieved without data
augmentation or model transfer. Public code is available at:
https://github.com/JNAIC/PIPMN",,
VISinger 2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer,"[arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Heyang Xue'), arxiv.Result.Author('Hanzhao Li'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Tingwei Guo'), arxiv.Result.Author('Ruixiong Zhang'), arxiv.Result.Author('Caixia Gong')]",2022-11-05 13:35:00+00:00,"End-to-end singing voice synthesis (SVS) model VISinger can achieve better
performance than the typical two-stage model with fewer parameters. However,
VISinger has several problems: text-to-phase problem, the end-to-end model
learns the meaningless mapping of text-to-phase; glitches problem, the harmonic
components corresponding to the periodic signal of the voiced segment occurs a
sudden change with audible artefacts; low sampling rate, the sampling rate of
24KHz does not meet the application needs of high-fidelity generation with the
full-band rate (44.1KHz or higher). In this paper, we propose VISinger 2 to
address these issues by integrating the digital signal processing (DSP) methods
with VISinger. Specifically, inspired by recent advances in differentiable
digital signal processing (DDSP), we incorporate a DSP synthesizer into the
decoder to solve the above issues. The DSP synthesizer consists of a harmonic
synthesizer and a noise synthesizer to generate periodic and aperiodic signals,
respectively, from the latent representation z in VISinger. It supervises the
posterior encoder to extract the latent representation without phase
information and avoid the prior encoder modelling text-to-phase mapping. To
avoid glitch artefacts, the HiFi-GAN is modified to accept the waveforms
generated by the DSP synthesizer as a condition to produce the singing voice.
Moreover, with the improved waveform decoder, VISinger 2 manages to generate
44.1kHz singing audio with richer expression and better quality. Experiments on
OpenCpop corpus show that VISinger 2 outperforms VISinger, CpopSing and
RefineSinger in both subjective and objective metrics.",Submitted to ICASSP 2023,
Evaluation of Automated Speech Recognition Systems for Conversational Speech: A Linguistic Perspective,"[arxiv.Result.Author('Hannaneh B. Pasandi'), arxiv.Result.Author('Haniyeh B. Pasandi')]",2022-11-05 04:35:40+00:00,"Automatic speech recognition (ASR) meets more informal and free-form input
data as voice user interfaces and conversational agents such as the voice
assistants such as Alexa, Google Home, etc., gain popularity. Conversational
speech is both the most difficult and environmentally relevant sort of data for
speech recognition. In this paper, we take a linguistic perspective, and take
the French language as a case study toward disambiguation of the French
homophones. Our contribution aims to provide more insight into human speech
transcription accuracy in conditions to reproduce those of state-of-the-art ASR
systems, although in a much focused situation. We investigate a case study
involving the most common errors encountered in the automatic transcription of
French language.",10 pages,
LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and Translation Using Neural Transducers,"[arxiv.Result.Author('Peidong Wang'), arxiv.Result.Author('Eric Sun'), arxiv.Result.Author('Jian Xue'), arxiv.Result.Author('Yu Wu'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Yashesh Gaur'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Jinyu Li')]",2022-11-05 04:03:55+00:00,"End-to-end formulation of automatic speech recognition (ASR) and speech
translation (ST) makes it easy to use a single model for both multilingual ASR
and many-to-many ST. In this paper, we propose streaming language-agnostic
multilingual speech recognition and translation using neural transducers
(LAMASSU). To enable multilingual text generation in LAMASSU, we conduct a
systematic comparison between specified and unified prediction and joint
networks. We leverage a language-agnostic multilingual encoder that
substantially outperforms shared encoders. To enhance LAMASSU, we propose to
feed target LID to encoders. We also apply connectionist temporal
classification regularization to transducer training. Experimental results show
that LAMASSU not only drastically reduces the model size but also outperforms
monolingual ASR and bilingual ST models.",Submitted to ICASSP 2023,
Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered Speech,"[arxiv.Result.Author('Xin Zhang'), arxiv.Result.Author('Iván Vallés-Pérez'), arxiv.Result.Author('Andreas Stolcke'), arxiv.Result.Author('Chengzhu Yu'), arxiv.Result.Author('Jasha Droppo'), arxiv.Result.Author('Olabanji Shonibare'), arxiv.Result.Author('Roberto Barra-Chicote'), arxiv.Result.Author('Venkatesh Ravichandran')]",2022-11-04 23:45:31+00:00,"Stuttering is a speech disorder where the natural flow of speech is
interrupted by blocks, repetitions or prolongations of syllables, words and
phrases. The majority of existing automatic speech recognition (ASR) interfaces
perform poorly on utterances with stutter, mainly due to lack of matched
training data. Synthesis of speech with stutter thus presents an opportunity to
improve ASR for this type of speech. We describe Stutter-TTS, an end-to-end
neural text-to-speech model capable of synthesizing diverse types of stuttering
utterances. We develop a simple, yet effective prosody-control strategy whereby
additional tokens are introduced into source text during training to represent
specific stuttering characteristics. By choosing the position of the stutter
tokens, Stutter-TTS allows word-level control of where stuttering occurs in the
synthesized utterance. We are able to synthesize stutter events with high
accuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By
fine-tuning an ASR model on synthetic stuttered speech we are able to reduce
word error by 5.7% relative on stuttered utterances, with only minor (<0.2%
relative) degradation for fluent utterances.","8 pages, 3 figures, 2 tables","NeurIPS Workshop on SyntheticData4ML, December 2022"
Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation with E3Net,"[arxiv.Result.Author('Sefik Emre Eskimez'), arxiv.Result.Author('Takuya Yoshioka'), arxiv.Result.Author('Alex Ju'), arxiv.Result.Author('Min Tang'), arxiv.Result.Author('Tanel Parnamaa'), arxiv.Result.Author('Huaming Wang')]",2022-11-04 22:29:00+00:00,"Personalized speech enhancement (PSE), a process of estimating a clean target
speech signal in real time by leveraging a speaker embedding vector of the
target talker, has garnered much attention from the research community due to
the recent surge of online meetings across the globe. For practical full duplex
communication, PSE models require an acoustic echo cancellation (AEC)
capability. In this work, we employ a recently proposed causal end-to-end
enhancement network (E3Net) and modify it to obtain a joint PSE-AEC model. We
dedicate the early layers to the AEC task while encouraging later layers for
personalization by adding a bypass connection from the early layers to the mask
prediction layer. This allows us to employ a multi-task learning framework for
joint PSE and AEC training. We provide extensive evaluation test scenarios with
both simulated and real-world recordings. The results show that our joint model
comes close to the expert models for each task and performs significantly
better for the combined PSE-AEC scenario.",Submitted to ICASSP 23,
SAMO: Speaker Attractor Multi-Center One-Class Learning for Voice Anti-Spoofing,"[arxiv.Result.Author('Siwen Ding'), arxiv.Result.Author('You Zhang'), arxiv.Result.Author('Zhiyao Duan')]",2022-11-04 19:31:33+00:00,"Voice anti-spoofing systems are crucial auxiliaries for automatic speaker
verification (ASV) systems. A major challenge is caused by unseen attacks
empowered by advanced speech synthesis technologies. Our previous research on
one-class learning has improved the generalization ability to unseen attacks by
compacting the bona fide speech in the embedding space. However, such
compactness lacks consideration of the diversity of speakers. In this work, we
propose speaker attractor multi-center one-class learning (SAMO), which
clusters bona fide speech around a number of speaker attractors and pushes away
spoofing attacks from all the attractors in a high-dimensional embedding space.
For training, we propose an algorithm for the co-optimization of bona fide
speech clustering and bona fide/spoof classification. For inference, we propose
strategies to enable anti-spoofing for speakers without enrollment. Our
proposed system outperforms existing state-of-the-art single systems with a
relative improvement of 38% on equal error rate (EER) on the ASVspoof2019 LA
evaluation set.",,
Resource-Efficient Transfer Learning From Speech Foundation Model Using Hierarchical Feature Fusion,"[arxiv.Result.Author('Zhouyuan Huo'), arxiv.Result.Author('Khe Chai Sim'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Dongseong Hwang'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Trevor Strohman')]",2022-11-04 19:03:45+00:00,"Self-supervised pre-training of a speech foundation model, followed by
supervised fine-tuning, has shown impressive quality improvements on automatic
speech recognition (ASR) tasks. Fine-tuning separate foundation models for many
downstream tasks are expensive since the foundation model is usually very big.
Parameter-efficient fine-tuning methods (e.g. adapter, sparse update methods)
offer an alternative paradigm where a small set of parameters are updated to
adapt the foundation model to new tasks. However, these methods still suffer
from a high computational memory cost and slow training speed because they
require backpropagation through the entire neural network at each step. In the
paper, we analyze the performance of features at different layers of a
foundation model on the speech recognition task and propose a novel
hierarchical feature fusion method for resource-efficient transfer learning
from speech foundation models. Experimental results show that the proposed
method can achieve better performance on speech recognition task than existing
algorithms with fewer number of trainable parameters, less computational memory
cost and faster training speed. After combining with Adapters at all layers,
the proposed method can achieve the same performance as fine-tuning the whole
model with $97\%$ fewer trainable encoder parameters and $53\%$ faster training
speed.",,
Speech enhancement using ego-noise references with a microphone array embedded in an unmanned aerial vehicle,"[arxiv.Result.Author('Elisa Tengan'), arxiv.Result.Author('Thomas Dietzen'), arxiv.Result.Author('Santiago Ruiz'), arxiv.Result.Author('Mansour Alkmim'), arxiv.Result.Author('João Cardenuto'), arxiv.Result.Author('Toon van Waterschoot')]",2022-11-04 18:15:21+00:00,"A method is proposed for performing speech enhancement using ego-noise
references with a microphone array embedded in an unmanned aerial vehicle
(UAV). The ego-noise reference signals are captured with microphones located
near the UAV's propellers and used in the prior knowledge multichannel Wiener
filter (PK-MWF) to obtain the speech correlation matrix estimate. Speech
presence probability (SPP) can be estimated for detecting speech activity from
an external microphone near the speech source, providing a performance
benchmark, or from one of the embedded microphones, assuming a more realistic
scenario. Experimental measurements are performed in a semi-anechoic chamber,
with a UAV mounted on a stand and a loudspeaker playing a speech signal, while
setting three distinct and fixed propeller rotation speeds, resulting in three
different signal-to-noise ratios (SNRs). The recordings obtained and made
available online are used to compare the proposed method to the use of the
standard multichannel Wiener filter (MWF) estimated with and without the
propellers' microphones being used in its formulation. Results show that
compared to those, the use of PK-MWF achieves higher levels of improvement in
speech intelligibility and quality, measured by STOI and PESQ, while the SNR
improvement is similar.",,"Proceedings of the 24th International Congress on Acoustics (ICA),
  Gyeongju, South Korea, 24 Oct 2022-28 Oct 2022"
CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment,"[arxiv.Result.Author('Yuchen Liu'), arxiv.Result.Author('Li-Chia Yang'), arxiv.Result.Author('Alex Pawlicki'), arxiv.Result.Author('Marko Stamenovic')]",2022-11-04 16:46:11+00:00,"Speech quality assessment has been a critical component in many voice
communication related applications such as telephony and online conferencing.
Traditional intrusive speech quality assessment requires the clean reference of
the degraded utterance to provide an accurate quality measurement. This
requirement limits the usability of these methods in real-world scenarios. On
the other hand, non-intrusive subjective measurement is the ``golden standard""
in evaluating speech quality as human listeners can intrinsically evaluate the
quality of any degraded speech with ease. In this paper, we propose a novel
end-to-end model structure called Convolutional Context-Aware Transformer
(CCAT) network to predict the mean opinion score (MOS) of human raters. We
evaluate our model on three MOS-annotated datasets spanning multiple languages
and distortion types and submit our results to the ConferencingSpeech 2022
Challenge. Our experiments show that CCAT provides promising MOS predictions
compared to current state-of-art non-intrusive speech assessment models with
average Pearson correlation coefficient (PCC) increasing from 0.530 to 0.697
and average RMSE decreasing from 0.768 to 0.570 compared to the baseline model
on the challenge evaluation test set.",,
Multi-blank Transducers for Speech Recognition,"[arxiv.Result.Author('Hainan Xu'), arxiv.Result.Author('Fei Jia'), arxiv.Result.Author('Somshubra Majumdar'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Boris Ginsburg')]",2022-11-04 16:24:46+00:00,"This paper proposes a modification to RNN-Transducer (RNN-T) models for
automatic speech recognition (ASR). In standard RNN-T, the emission of a blank
symbol consumes exactly one input frame; in our proposed method, we introduce
additional blank symbols, which consume two or more input frames when emitted.
We refer to the added symbols as big blanks, and the method multi-blank RNN-T.
For training multi-blank RNN-Ts, we propose a novel logit under-normalization
method in order to prioritize emissions of big blanks. With experiments on
multiple languages and datasets, we show that multi-blank RNN-T methods could
bring relative speedups of over +90%/+139% to model inference for English
Librispeech and German Multilingual Librispeech datasets, respectively. The
multi-blank RNN-T method also improves ASR accuracy consistently. We will
release our implementation of the method in the NeMo
(\url{https://github.com/NVIDIA/NeMo}) toolkit.",Submitted to ICASSP 2023,
Self-Supervised Learning for Speech Enhancement through Synthesis,"[arxiv.Result.Author('Bryce Irvin'), arxiv.Result.Author('Marko Stamenovic'), arxiv.Result.Author('Mikolaj Kegler'), arxiv.Result.Author('Li-Chia Yang')]",2022-11-04 16:06:56+00:00,"Modern speech enhancement (SE) networks typically implement noise suppression
through time-frequency masking, latent representation masking, or
discriminative signal prediction. In contrast, some recent works explore SE via
generative speech synthesis, where the system's output is synthesized by a
neural vocoder after an inherently lossy feature-denoising step. In this paper,
we propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy
representations and learns to directly synthesize clean speech. We leverage
rich representations from self-supervised learning (SSL) speech models to
discover relevant features. We conduct a candidate search across 15 potential
SSL front-ends and subsequently train our vocoder adversarially with the best
SSL configuration. Additionally, we demonstrate a causal version capable of
running on streaming audio with 10ms latency and minimal performance
degradation. Finally, we conduct both objective evaluations and subjective
listening studies to show our system improves objective metrics and outperforms
an existing state-of-the-art SE model subjectively.",,
Biased Self-supervised learning for ASR,"[arxiv.Result.Author('Florian L. Kreyssig'), arxiv.Result.Author('Yangyang Shi'), arxiv.Result.Author('Jinxi Guo'), arxiv.Result.Author('Leda Sari'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Philip C. Woodland')]",2022-11-04 15:57:59+00:00,"Self-supervised learning via masked prediction pre-training (MPPT) has shown
impressive performance on a range of speech-processing tasks. This paper
proposes a method to bias self-supervised learning towards a specific task. The
core idea is to slightly finetune the model that is used to obtain the target
sequence. This leads to better performance and a substantial increase in
training speed. Furthermore, this paper proposes a variant of MPPT that allows
low-footprint streaming models to be trained effectively by computing the MPPT
loss on masked and unmasked frames. These approaches are evaluated for
automatic speech recognition on the Librispeech corpus, where 100 hours of data
served as the labelled data and 860 hours as the unlabelled data. The biased
training outperforms the unbiased training by 15.5% after 250k updates and
23.8% after 100k updates on test-other. For the streaming models, the
pre-training approach yields a reduction in word error rate of 44.1%.",Submitted to ICASSP 2023,
Cold Diffusion for Speech Enhancement,"[arxiv.Result.Author('Hao Yen'), arxiv.Result.Author('François G. Germain'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('Jonathan Le Roux')]",2022-11-04 15:41:01+00:00,"Diffusion models have recently shown promising results for difficult
enhancement tasks such as the conditional and unconditional restoration of
natural images and audio signals. In this work, we explore the possibility of
leveraging a recently proposed advanced iterative diffusion model, namely cold
diffusion, to recover clean speech signals from noisy signals. The unique
mathematical properties of the sampling process from cold diffusion could be
utilized to restore high-quality samples from arbitrary degradations. Based on
these properties, we propose an improved training algorithm and objective to
help the model generalize better during the sampling process. We verify our
proposed framework by investigating two model architectures. Experimental
results on benchmark speech enhancement dataset VoiceBank-DEMAND demonstrate
the strong performance of the proposed approach compared to representative
discriminative models and diffusion-based enhancement models.","5 pages, 1 figure, 1 table, 3 algorithms. Submitted to ICASSP 2023",
Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding,"[arxiv.Result.Author('Haici Yang'), arxiv.Result.Author('Wootaek Lim'), arxiv.Result.Author('Minje Kim')]",2022-11-04 15:07:42+00:00,"Low and ultra-low-bitrate neural speech coding achieves unprecedented coding
gain by generating speech signals from compact speech features. This paper
introduces additional coding efficiency in neural speech coding by reducing the
temporal redundancy existing in the frame-level feature sequence via a
recurrent neural predictor. The prediction can achieve a low-entropy residual
representation, which we discriminatively code based on their contribution to
the signal reconstruction. The harmonization of feature prediction and
discriminative coding results in a dynamic bit allocation algorithm that spends
more bits on unpredictable but rare events. As a result, we develop a scalable,
lightweight, low-latency, and low-bitrate neural speech coding system. We
demonstrate the advantage of the proposed methods using the LPCNet as a neural
vocoder. While the proposed method guarantees causality in its prediction, the
subjective tests and feature space analysis show that our model achieves
superior coding efficiency compared to LPCNet and Lyra V2 in the very low
bitrates.",,
A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability,"[arxiv.Result.Author('Jian Xue'), arxiv.Result.Author('Peidong Wang'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Eric Sun')]",2022-11-04 14:59:55+00:00,"In this paper, we introduce our work of building a Streaming Multilingual
Speech Model (SM2), which can transcribe or translate multiple spoken languages
into texts of the target language. The backbone of SM2 is Transformer
Transducer, which has high streaming capability. Instead of human labeled
speech translation (ST) data, SM2 models are trained using weakly supervised
data generated by converting the transcriptions in speech recognition corpora
with a machine translation service. With 351 thousand hours of anonymized
speech training data from 25 languages, SM2 models achieve comparable or even
better ST quality than some recent popular large-scale non-streaming speech
models. More importantly, we show that SM2 has the truly zero-shot capability
when expanding to new target languages, yielding high quality ST results for
{source-speech, target-text} pairs that are not seen during training.",submitted to ICASSP 2023,
Sampling Rate Offset Estimation and Compensation for Distributed Adaptive Node-Specific Signal Estimation in Wireless Acoustic Sensor Networks,"[arxiv.Result.Author('Paul Didier'), arxiv.Result.Author('Toon van Waterschoot'), arxiv.Result.Author('Simon Doclo'), arxiv.Result.Author('Marc Moonen')]",2022-11-04 14:38:14+00:00,"Sampling rate offsets (SROs) between devices in a heterogeneous wireless
acoustic sensor network (WASN) can hinder the ability of distributed adaptive
algorithms to perform as intended when they rely on coherent signal processing.
In this paper, we present an SRO estimation and compensation method to allow
the deployment of the distributed adaptive node-specific signal estimation
(DANSE) algorithm in WASNs composed of asynchronous devices. The signals
available at each node are first utilised in a coherence-drift-based method to
blindly estimate SROs which are then compensated for via phase shifts in the
frequency domain. A modification of the weighted overlap-add (WOLA)
implementation of DANSE is introduced to account for SRO-induced full-sample
drifts, permitting per-sample signal transmission via an approximation of the
WOLA process as a time-domain convolution. The performance of the proposed
algorithm is evaluated in the context of distributed noise reduction for the
estimation of a target speech signal in an asynchronous WASN.","9 pages, 6 figures",
NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS,"[arxiv.Result.Author('Dongchao Yang'), arxiv.Result.Author('Songxiang Liu'), arxiv.Result.Author('Jianwei Yu'), arxiv.Result.Author('Helin Wang'), arxiv.Result.Author('Chao Weng'), arxiv.Result.Author('Yuexian Zou')]",2022-11-04 13:32:58+00:00,"Expressive text-to-speech (TTS) can synthesize a new speaking style by
imiating prosody and timbre from a reference audio, which faces the following
challenges: (1) The highly dynamic prosody information in the reference audio
is difficult to extract, especially, when the reference audio contains
background noise. (2) The TTS systems should have good generalization for
unseen speaking styles. In this paper, we present a
\textbf{no}ise-\textbf{r}obust \textbf{e}xpressive TTS model (NoreSpeech),
which can robustly transfer speaking style in a noisy reference utterance to
synthesized speech. Specifically, our NoreSpeech includes several components:
(1) a novel DiffStyle module, which leverages powerful probabilistic denoising
diffusion models to learn noise-agnostic speaking style features from a teacher
model by knowledge distillation; (2) a VQ-VAE block, which maps the style
features into a controllable quantized latent space for improving the
generalization of style transfer; and (3) a straight-forward but effective
parameter-free text-style alignment module, which enables NoreSpeech to
transfer style to a textual input from a length-mismatched reference utterance.
Experiments demonstrate that NoreSpeech is more effective than previous
expressive TTS models in noise environments. Audio samples and code are
available at:
\href{http://dongchaoyang.top/NoreSpeech\_demo/}{http://dongchaoyang.top/NoreSpeech\_demo/}",Submitted to ICASSP2023,
Spatially Selective Deep Non-linear Filters for Speaker Extraction,"[arxiv.Result.Author('Kristina Tesch'), arxiv.Result.Author('Timo Gerkmann')]",2022-11-04 12:54:06+00:00,"In a scenario with multiple persons talking simultaneously, the spatial
characteristics of the signals are the most distinct feature for extracting the
target signal. In this work, we develop a deep joint spatial-spectral
non-linear filter that can be steered in an arbitrary target direction. For
this we propose a simple and effective conditioning mechanism, which sets the
initial state of the filter's recurrent layers based on the target direction.
We show that this scheme is more effective than the baseline approach and
increases the flexibility of the filter at no performance cost. The resulting
spatially selective non-linear filters can also be used for speech separation
of an arbitrary number of speakers and enable very accurate multi-speaker
localization as we demonstrate in this paper.",Submitted to ICASSP 2023,
Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration,"[arxiv.Result.Author('Jean-Marie Lemercier'), arxiv.Result.Author('Julius Richter'), arxiv.Result.Author('Simon Welker'), arxiv.Result.Author('Timo Gerkmann')]",2022-11-04 12:06:14+00:00,"Diffusion-based generative models have had a high impact on the computer
vision and speech processing communities these past years. Besides data
generation tasks, they have also been employed for data restoration tasks like
speech enhancement and dereverberation. While discriminative models have
traditionally been argued to be more powerful e.g. for speech enhancement,
generative diffusion approaches have recently been shown to narrow this
performance gap considerably. In this paper, we systematically compare the
performance of generative diffusion models and discriminative approaches on
different speech restoration tasks. For this, we extend our prior contributions
on diffusion-based speech enhancement in the complex time-frequency domain to
the task of bandwith extension. We then compare it to a discriminatively
trained neural network with the same network architecture on three restoration
tasks, namely speech denoising, dereverberation and bandwidth extension. We
observe that the generative approach performs globally better than its
discriminative counterpart on all tasks, with the strongest benefit for
non-additive distortion models, like in dereverberation and bandwidth
extension. Code and audio examples can be found online at
https://uhh.de/inf-sp-sgmsemultitask",Submitted to ICASSP 2023,
SPEAKER VGG CCT: Cross-corpus Speech Emotion Recognition with Speaker Embedding and Vision Transformers,"[arxiv.Result.Author('A. Arezzo'), arxiv.Result.Author('S. Berretti')]",2022-11-04 10:49:44+00:00,"In recent years, Speech Emotion Recognition (SER) has been investigated
mainly transforming the speech signal into spectrograms that are then
classified using Convolutional Neural Networks pretrained on generic images and
fine tuned with spectrograms. In this paper, we start from the general idea
above and develop a new learning solution for SER, which is based on Compact
Convolutional Transformers (CCTs) combined with a speaker embedding. With CCTs,
the learning power of Vision Transformers (ViT) is combined with a diminished
need for large volume of data as made possible by the convolution. This is
important in SER, where large corpora of data are usually not available. The
speaker embedding allows the network to extract an identity representation of
the speaker, which is then integrated by means of a self-attention mechanism
with the features that the CCT extracts from the spectrogram. Overall, the
solution is capable of operating in real-time showing promising results in a
cross-corpus scenario, where training and test datasets are kept separate.
Experiments have been performed on several benchmarks in a cross-corpus setting
as rarely used in the literature, with results that are comparable or superior
to those obtained with state-of-the-art network architectures. Our code is
available at https://github.com/JabuMlDev/Speaker-VGG-CCT.",,
Improving Speech Prosody of Audiobook Text-to-Speech Synthesis with Acoustic and Textual Contexts,"[arxiv.Result.Author('Detai Xin'), arxiv.Result.Author('Sharath Adavanne'), arxiv.Result.Author('Federico Ang'), arxiv.Result.Author('Ashish Kulkarni'), arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Hiroshi Saruwatari')]",2022-11-04 09:23:02+00:00,"We present a multi-speaker Japanese audiobook text-to-speech (TTS) system
that leverages multimodal context information of preceding acoustic context and
bilateral textual context to improve the prosody of synthetic speech. Previous
work either uses unilateral or single-modality context, which does not fully
represent the context information. The proposed method uses an acoustic context
encoder and a textual context encoder to aggregate context information and
feeds it to the TTS model, which enables the model to predict context-dependent
prosody. We conducted comprehensive objective and subjective evaluations on a
multi-speaker Japanese audiobook dataset. Experimental results demonstrate that
the proposed method significantly outperforms two previous works. Additionally,
we present insights about the different choices of context - modalities,
lateral information and length - for audiobook TTS that have never been
discussed in the literature before.",,
Minimum Latency Training of Sequence Transducers for Streaming End-to-End Speech Recognition,"[arxiv.Result.Author('Yusuke Shinohara'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-04 09:19:59+00:00,"Sequence transducers, such as the RNN-T and the Conformer-T, are one of the
most promising models of end-to-end speech recognition, especially in streaming
scenarios where both latency and accuracy are important. Although various
methods, such as alignment-restricted training and FastEmit, have been studied
to reduce the latency, latency reduction is often accompanied with a
significant degradation in accuracy. We argue that this suboptimal performance
might be caused because none of the prior methods explicitly model and reduce
the latency. In this paper, we propose a new training method to explicitly
model and reduce the latency of sequence transducer models. First, we define
the expected latency at each diagonal line on the lattice, and show that its
gradient can be computed efficiently within the forward-backward algorithm.
Then we augment the transducer loss with this expected latency, so that an
optimal trade-off between latency and accuracy is achieved. Experimental
results on the WSJ dataset show that the proposed minimum latency training
reduces the latency of causal Conformer-T from 220 ms to 27 ms within a WER
degradation of 0.7%, and outperforms conventional alignment-restricted training
(110 ms) and FastEmit (67 ms) methods.",Presented at INTERSPEECH 2022,
Once-for-All Sequence Compression for Self-Supervised Speech Models,"[arxiv.Result.Author('Hsuan-Jui Chen'), arxiv.Result.Author('Yen Meng'), arxiv.Result.Author('Hung-yi Lee')]",2022-11-04 09:19:13+00:00,"The sequence length along the time axis is often the dominant factor of the
computational cost of self-supervised speech models. Works have been proposed
to reduce the sequence length for lowering the computational cost. However,
different downstream tasks have different tolerance of sequence compressing, so
a model that produces a fixed compressing rate may not fit all tasks. In this
work, we introduce a once-for-all (OFA) sequence compression framework for
self-supervised speech models that supports a continuous range of compressing
rates. The framework is evaluated on various tasks, showing marginal
degradation compared to the fixed compressing rate variants with a smooth
performance-efficiency trade-off. We further explore adaptive compressing rate
learning, demonstrating the ability to select task-specific preferred frame
periods without needing a grid search.",Submitted to ICASSP 2023,
Binaural Rendering of Ambisonic Signals by Neural Networks,"[arxiv.Result.Author('Yin Zhu'), arxiv.Result.Author('Qiuqiang Kong'), arxiv.Result.Author('Junjie Shi'), arxiv.Result.Author('Shilei Liu'), arxiv.Result.Author('Xuzhou Ye'), arxiv.Result.Author('Ju-chiang Wang'), arxiv.Result.Author('Junping Zhang')]",2022-11-04 07:57:37+00:00,"Binaural rendering of ambisonic signals is of broad interest to virtual
reality and immersive media. Conventional methods often require manually
measured Head-Related Transfer Functions (HRTFs). To address this issue, we
collect a paired ambisonic-binaural dataset and propose a deep learning
framework in an end-to-end manner. Experimental results show that neural
networks outperform the conventional method in objective metrics and achieve
comparable subjective metrics. To validate the proposed framework, we
experimentally explore different settings of the input features, model
structures, output features, and loss functions. Our proposed system achieves
an SDR of 7.32 and MOSs of 3.83, 3.58, 3.87, 3.58 in quality, timbre,
localization, and immersion dimensions.",,
CochlScene: Acquisition of acoustic scene data using crowdsourcing,"[arxiv.Result.Author('Il-Young Jeong'), arxiv.Result.Author('Jeongsoo Park')]",2022-11-04 07:01:16+00:00,"This paper describes a pipeline for collecting acoustic scene data by using
crowdsourcing. The detailed process of crowdsourcing is explained, including
planning, validation criteria, and actual user interfaces. As a result of data
collection, we present CochlScene, a novel dataset for acoustic scene
classification. Our dataset consists of 76k samples collected from 831
participants in 13 acoustic scenes. We also propose a manual data split of
training, validation, and test sets to increase the reliability of the
evaluation results. Finally, we provide a baseline system for future research.","Accept by APSIPA ASC 2022, 5 pages, 2 figures",
Wireless Deep Speech Semantic Transmission,"[arxiv.Result.Author('Zixuan Xiao'), arxiv.Result.Author('Shengshi Yao'), arxiv.Result.Author('Jincheng Dai'), arxiv.Result.Author('Sixian Wang'), arxiv.Result.Author('Kai Niu'), arxiv.Result.Author('Ping Zhang')]",2022-11-04 06:49:42+00:00,"In this paper, we propose a new class of high-efficiency semantic coded
transmission methods for end-to-end speech transmission over wireless channels.
We name the whole system as deep speech semantic transmission (DSST).
Specifically, we introduce a nonlinear transform to map the speech source to
semantic latent space and feed semantic features into source-channel encoder to
generate the channel-input sequence. Guided by the variational modeling idea,
we build an entropy model on the latent space to estimate the importance
diversity among semantic feature embeddings. Accordingly, these semantic
features of different importance can be allocated with different coding rates
reasonably, which maximizes the system coding gain. Furthermore, we introduce a
channel signal-to-noise ratio (SNR) adaptation mechanism such that a single
model can be applied over various channel states. The end-to-end optimization
of our model leads to a flexible rate-distortion (RD) trade-off, supporting
versatile wireless speech semantic transmission. Experimental results verify
that our DSST system clearly outperforms current engineered speech transmission
systems on both objective and subjective metrics. Compared with existing neural
speech semantic transmission methods, our model saves up to 75% of channel
bandwidth costs when achieving the same quality. An intuitive comparison of
audio demos can be found at https://ximoo123.github.io/DSST.",,
Real-Time Target Sound Extraction,"[arxiv.Result.Author('Bandhav Veluri'), arxiv.Result.Author('Justin Chan'), arxiv.Result.Author('Malek Itani'), arxiv.Result.Author('Tuochao Chen'), arxiv.Result.Author('Takuya Yoshioka'), arxiv.Result.Author('Shyamnath Gollakota')]",2022-11-04 03:51:23+00:00,"We present the first neural network model to achieve real-time and streaming
target sound extraction. To accomplish this, we propose Waveformer, an
encoder-decoder architecture with a stack of dilated causal convolution layers
as the encoder, and a transformer decoder layer as the decoder. This hybrid
architecture uses dilated causal convolutions for processing large receptive
fields in a computationally efficient manner, while also benefiting from the
performance transformer-based architectures provide. Our evaluations show as
much as 2.2-3.3 dB improvement in SI-SNRi compared to the prior models for this
task while having a 1.2-4x smaller model size and a 1.5-2x lower runtime.
Open-source code and datasets: https://github.com/vb000/Waveformer",,
Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects,"[arxiv.Result.Author('Junghyun Koo'), arxiv.Result.Author('Marco A. Martinez-Ramirez'), arxiv.Result.Author('Wei-Hsiang Liao'), arxiv.Result.Author('Stefan Uhlich'), arxiv.Result.Author('Kyogu Lee'), arxiv.Result.Author('Yuki Mitsufuji')]",2022-11-04 03:45:17+00:00,"We propose an end-to-end music mixing style transfer system that converts the
mixing style of an input multitrack to that of a reference song. This is
achieved with an encoder pre-trained with a contrastive objective to extract
only audio effects related information from a reference music recording. All
our models are trained in a self-supervised manner from an already-processed
wet multitrack dataset with an effective data preprocessing method that
alleviates the data scarcity of obtaining unprocessed dry data. We analyze the
proposed encoder for the disentanglement capability of audio effects and also
validate its performance for mixing style transfer through both objective and
subjective evaluations. From the results, we show the proposed system not only
converts the mixing style of multitrack audio close to a reference but is also
robust with mixture-wise style transfer upon using a music source separation
model.",,
Integrated Parameter-Efficient Tuning for General-Purpose Audio Models,"[arxiv.Result.Author('Ju-ho Kim'), arxiv.Result.Author('Jungwoo Heo'), arxiv.Result.Author('Hyun-seo Shin'), arxiv.Result.Author('Chan-yeong Lim'), arxiv.Result.Author('Ha-Jin Yu')]",2022-11-04 02:21:15+00:00,"The advent of hyper-scale and general-purpose pre-trained models is shifting
the paradigm of building task-specific models for target tasks. In the field of
audio research, task-agnostic pre-trained models with high transferability and
adaptability have achieved state-of-the-art performances through fine-tuning
for downstream tasks. Nevertheless, re-training all the parameters of these
massive models entails an enormous amount of time and cost, along with a huge
carbon footprint. To overcome these limitations, the present study explores and
applies efficient transfer learning methods in the audio domain. We also
propose an integrated parameter-efficient tuning (IPET) framework by
aggregating the embedding prompt (a prompt-based learning approach), and the
adapter (an effective transfer learning method). We demonstrate the efficacy of
the proposed framework using two backbone pre-trained audio models with
different characteristics: the audio spectrogram transformer and wav2vec 2.0.
The proposed IPET framework exhibits remarkable performance compared to
fine-tuning method with fewer trainable parameters in four downstream tasks:
sound event classification, music genre classification, keyword spotting, and
speaker verification. Furthermore, the authors identify and analyze the
shortcomings of the IPET framework, providing lessons and research directions
for parameter efficient tuning in the audio domain.","5 pages, 3 figures, submit to ICASSP2023",
Streaming Audio-Visual Speech Recognition with Alignment Regularization,"[arxiv.Result.Author('Pingchuan Ma'), arxiv.Result.Author('Niko Moritz'), arxiv.Result.Author('Stavros Petridis'), arxiv.Result.Author('Christian Fuegen'), arxiv.Result.Author('Maja Pantic')]",2022-11-03 20:20:47+00:00,"Recognizing a word shortly after it is spoken is an important requirement for
automatic speech recognition (ASR) systems in real-world scenarios. As a
result, a large body of work on streaming audio-only ASR models has been
presented in the literature. However, streaming audio-visual automatic speech
recognition (AV-ASR) has received little attention in earlier works. In this
work, we propose a streaming AV-ASR system based on a hybrid connectionist
temporal classification (CTC)/attention neural network architecture. The audio
and the visual encoder neural networks are both based on the conformer
architecture, which is made streamable using chunk-wise self-attention (CSA)
and causal convolution. Streaming recognition with a decoder neural network is
realized by using the triggered attention technique, which performs
time-synchronous decoding with joint CTC/attention scoring. For frame-level ASR
criteria, such as CTC, a synchronized response from the audio and visual
encoders is critical for a joint AV decision making process. In this work, we
propose a novel alignment regularization technique that promotes
synchronization of the audio and visual encoder, which in turn results in
better word error rates (WERs) at all SNR levels for streaming and offline
AV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the
Lip Reading Sentences 3 (LRS3) dataset in an offline and online setup,
respectively, which both present state-of-the-art results when no external
training data are used.",Submitted to ICASSP2023,
Dynamic Kernels and Channel Attention with Multi-Layer Embedding Aggregation for Speaker Verification,"[arxiv.Result.Author('Anna Ollerenshaw'), arxiv.Result.Author('Md Asif Jalal'), arxiv.Result.Author('Thomas Hain')]",2022-11-03 17:13:28+00:00,"State-of-the-art speaker verification frameworks have typically focused on
speech enhancement techniques with increasingly deeper (more layers) and wider
(number of channels) models to improve their verification performance. Instead,
this paper proposes an approach to increase the model resolution capability
using attention-based dynamic kernels in a convolutional neural network to
adapt the model parameters to be feature-conditioned. The attention weights on
the kernels are further distilled by channel attention and multi-layer feature
aggregation to learn global features from speech. This approach provides an
efficient solution to improving representation capacity with lower data
resources. This is due to the self-adaptation to inputs of the structures of
the model parameters. The proposed dynamic convolutional model achieved 1.62\%
EER and 0.18 miniDCF on the VoxCeleb1 test set and has a 17\% relative
improvement compared to the ECAPA-TDNN.",Submitted to ICASSP 2023,
Probing Statistical Representations For End-To-End ASR,"[arxiv.Result.Author('Anna Ollerenshaw'), arxiv.Result.Author('Md Asif Jalal'), arxiv.Result.Author('Thomas Hain')]",2022-11-03 17:08:14+00:00,"End-to-End automatic speech recognition (ASR) models aim to learn a
generalised speech representation to perform recognition. In this domain there
is little research to analyse internal representation dependencies and their
relationship to modelling approaches. This paper investigates cross-domain
language model dependencies within transformer architectures using SVCCA and
uses these insights to exploit modelling approaches. It was found that specific
neural representations within the transformer layers exhibit correlated
behaviour which impacts recognition performance.
  Altogether, this work provides analysis of the modelling approaches affecting
contextual dependencies and ASR performance, and can be used to create or adapt
better performing End-to-End ASR models and also for downstream tasks.",Submitted to ICASSP 2023,
MarginNCE: Robust Sound Localization with a Negative Margin,"[arxiv.Result.Author('Sooyoung Park'), arxiv.Result.Author('Arda Senocak'), arxiv.Result.Author('Joon Son Chung')]",2022-11-03 16:44:14+00:00,"The goal of this work is to localize sound sources in visual scenes with a
self-supervised approach. Contrastive learning in the context of sound source
localization leverages the natural correspondence between audio and visual
signals where the audio-visual pairs from the same source are assumed as
positive, while randomly selected pairs are negatives. However, this approach
brings in noisy correspondences; for example, positive audio and visual pair
signals that may be unrelated to each other, or negative pairs that may contain
semantically similar samples to the positive one. Our key contribution in this
work is to show that using a less strict decision boundary in contrastive
learning can alleviate the effect of noisy correspondences in sound source
localization. We propose a simple yet effective approach by slightly modifying
the contrastive loss with a negative margin. Extensive experimental results
show that our approach gives on-par or better performance than the
state-of-the-art methods. Furthermore, we demonstrate that the introduction of
a negative margin to existing methods results in a consistent improvement in
performance.","Submitted to ICASSP 2023. SOTA performance in Audio-Visual Sound
  Localization. 5 Pages",
HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks,"[arxiv.Result.Author('Filip Szatkowski'), arxiv.Result.Author('Karol J. Piczak'), arxiv.Result.Author('Przemysław Spurek'), arxiv.Result.Author('Jacek Tabor'), arxiv.Result.Author('Tomasz Trzciński')]",2022-11-03 14:20:32+00:00,"Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.",,
Fearless Steps Challenge Phase-1 Evaluation Plan,"[arxiv.Result.Author('Aditya Joglekar'), arxiv.Result.Author('John H. L. Hansen')]",2022-11-03 14:17:43+00:00,"The Fearless Steps Challenge 2019 Phase-1 (FSC-P1) is the inaugural Challenge
of the Fearless Steps Initiative hosted by the Center for Robust Speech Systems
(CRSS) at the University of Texas at Dallas. The goal of this Challenge is to
evaluate the performance of state-of-the-art speech and language systems for
large task-oriented teams with naturalistic audio in challenging environments.
Researchers may select to participate in any single or multiple of these
challenge tasks. Researchers may also choose to employ the FEARLESS STEPS
corpus for other related speech applications. All participants are encouraged
to submit their solutions and results for consideration in the ISCA
INTERSPEECH-2019 special session.","Document Generated in February 2019 for conducting the Fearless Steps
  Challenge Phase-1 and its associated ISCA Interspeech-2019 Special Session",
Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing,"[arxiv.Result.Author('Sofoklis Kakouros'), arxiv.Result.Author('Themos Stafylakis'), arxiv.Result.Author('Ladislav Mosner'), arxiv.Result.Author('Lukas Burget')]",2022-11-03 12:37:59+00:00,"When recognizing emotions from speech, we encounter two common problems: how
to optimally capture emotion-relevant information from the speech signal and
how to best quantify or categorize the noisy subjective emotion labels.
Self-supervised pre-trained representations can robustly capture information
from speech enabling state-of-the-art results in many downstream tasks
including emotion recognition. However, better ways of aggregating the
information across time need to be considered as the relevant emotion
information is likely to appear piecewise and not uniformly across the signal.
For the labels, we need to take into account that there is a substantial degree
of noise that comes from the subjective human annotations. In this paper, we
propose a novel approach to attentive pooling based on correlations between the
representations' coefficients combined with label smoothing, a method aiming to
reduce the confidence of the classifier on the training labels. We evaluate our
proposed approach on the benchmark dataset IEMOCAP, and demonstrate high
performance surpassing that in the literature. The code to reproduce the
results is available at github.com/skakouros/s3prl_attentive_correlation.",Submitted to IEEE-ICASSP 2023,
Iterative autoregression: a novel trick to improve your low-latency speech enhancement model,"[arxiv.Result.Author('Pavel Andreev'), arxiv.Result.Author('Nicholas Babaev'), arxiv.Result.Author('Azat Saginbaev'), arxiv.Result.Author('Ivan Shchekotov')]",2022-11-03 12:32:33+00:00,"Streaming models are an essential component of real-time speech enhancement
tools. The streaming regime constrains speech enhancement models to use only a
tiny context of future information, thus, the low-latency streaming setup is
generally assumed to be challenging and has a significant negative effect on
the model quality. However, due to the sequential nature of streaming
generation, it provides a natural possibility for autoregression, i.e., using
previous predictions when making current ones. In this paper, we present a
simple, yet effective trick for training of autoregressive low-latency speech
enhancement models. We demonstrate that the proposed technique leads to stable
improvement across different architectures and training scenarios.",Submitted to ICASSP 2023,
Hybrid-SD (H_SD): A new hybrid evaluation metric for automatic speech recognition tasks,"[arxiv.Result.Author('Zitha Sasindran'), arxiv.Result.Author('Harsha Yelchuri'), arxiv.Result.Author('Supreeth Rao'), arxiv.Result.Author('T. V. Prabhakar')]",2022-11-03 11:23:36+00:00,"Many studies have examined the shortcomings of word error rate (WER) as an
evaluation metric for automatic speech recognition (ASR) systems, particularly
when used for spoken language understanding tasks such as intent recognition
and dialogue systems. In this paper, we propose Hybrid-SD (H_SD), a new hybrid
evaluation metric for ASR systems that takes into account both semantic
correctness and error rate. To generate sentence dissimilarity scores (SD), we
built a fast and lightweight SNanoBERT model using distillation techniques. Our
experiments show that the SNanoBERT model is 25.9x smaller and 38.8x faster
than SRoBERTa while achieving comparable results on well-known benchmarks.
Hence, making it suitable for deploying with ASR models on edge devices. We
also show that H_SD correlates more strongly with downstream tasks such as
intent recognition and named-entity recognition (NER).",,
Discussion of Features for Acoustic Anomaly Detection under Industrial Disturbing Noise in an End-of-Line Test of Geared Motors,"[arxiv.Result.Author('Peter Wissbrock'), arxiv.Result.Author('David Pelkmann'), arxiv.Result.Author('Yvonne Richter')]",2022-11-03 11:08:41+00:00,"In the end-of-line test of geared motors, the evaluation of product qual-ity
is important. Due to time constraints and the high diversity of variants,
acous-tic measurements are more economical than vibration measurements.
However, the acoustic data is affected by industrial disturbing noise.
Therefore, the aim of this study is to investigate the robustness of features
used for anomaly detection in geared motor end-of-line testing. A real-world
dataset with typical faults and acoustic disturbances is recorded by an
acoustic array. This includes industrial noise from the production and
systematically produced disturbances, used to compare the robustness. Overall,
it is proposed to apply features extracted from a log-envelope spectrum
together with psychoacoustic features. The anomaly de-tection is done by using
the isolation forest or the more universal bagging random miner. Most
disturbances can be circumvented, while the use of a hammer or air pressure
often causes problems. In general, these results are important for condi-tion
monitoring tasks that are based on acoustic or vibration measurements.
Fur-thermore, a real-world problem description is presented to improve common
sig-nal processing and machine learning tasks.",,
A speech corpus for chronic kidney disease,"[arxiv.Result.Author('Jihyun Mun'), arxiv.Result.Author('Sunhee Kim'), arxiv.Result.Author('Myeong Ju Kim'), arxiv.Result.Author('Jiwon Ryu'), arxiv.Result.Author('Sejoong Kim'), arxiv.Result.Author('Minhwa Chung')]",2022-11-03 10:57:48+00:00,"In this study, we present a speech corpus of patients with chronic kidney
disease (CKD) that will be used for research on pathological voice analysis,
automatic illness identification, and severity prediction. This paper
introduces the steps involved in creating this corpus, including the choice of
speech-related parameters and speech lists as well as the recording technique.
The speakers in this corpus, 289 CKD patients with varying degrees of severity
who were categorized based on estimated glomerular filtration rate (eGFR),
delivered sustained vowels, sentence, and paragraph stimuli. This study
compared and analyzed the voice characteristics of CKD patients with those of
the control group; the results revealed differences in voice quality,
phoneme-level pronunciation, prosody, glottal source, and aerodynamic
parameters.",,
Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection,"[arxiv.Result.Author('Peter Wißbrock'), arxiv.Result.Author('Yvonne Richter'), arxiv.Result.Author('David Pelkmann'), arxiv.Result.Author('Zhao Ren'), arxiv.Result.Author('Gregory Palmer')]",2022-11-03 10:56:17+00:00,"Acoustic-based fault detection has a high potential to monitor the health
condition of mechanical parts. However, the background noise of an industrial
environment may negatively influence the performance of fault detection.
Limited attention has been paid to improving the robustness of fault detection
against industrial environmental noise. Therefore, we present the Lenze
production background-noise (LPBN) real-world dataset and an automated and
noise-robust auditory inspection (ARAI) system for the end-of-line inspection
of geared motors. An acoustic array is used to acquire data from motors with a
minor fault, major fault, or which are healthy. A benchmark is provided to
compare the psychoacoustic features with different types of envelope features
based on expert knowledge of the gearbox. To the best of our knowledge, we are
the first to apply time-varying psychoacoustic features for fault detection. We
train a state-of-the-art one-class-classifier, on samples from healthy motors
and separate the faulty ones for fault detection using a threshold. The
best-performing approaches achieve an area under curve of 0.87 (logarithm
envelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).",,
Channel-Aware Pretraining of Joint Encoder-Decoder Self-Supervised Model for Telephonic-Speech ASR,"[arxiv.Result.Author('Vrunda N. Sukhadia'), arxiv.Result.Author('A. Arunkumar'), arxiv.Result.Author('S. Umesh')]",2022-11-03 09:27:16+00:00,"This paper proposes a novel technique to obtain better downstream ASR
performance from a joint encoder-decoder self-supervised model when trained
with speech pooled from two different channels (narrow and wide band). The
joint encoder-decoder self-supervised model extends the HuBERT model with a
Transformer decoder. HuBERT performs clustering of features and predicts the
class of every input frame. In simple pooling, which is our baseline, there is
no way to identify the channel information. To incorporate channel information,
we have proposed non-overlapping cluster IDs for speech from different
channels. Our method gives a relative improvement of ~ 5% over the joint
encoder-decoder self-supervised model built with simple pooling of data, which
serves as our baseline.","5 pages, 5 figures",
Adversarial Data Augmentation Using VAE-GAN for Disordered Speech Recognition,"[arxiv.Result.Author('Zengrui Jin'), arxiv.Result.Author('Xurong Xie'), arxiv.Result.Author('Mengzhe Geng'), arxiv.Result.Author('Tianzi Wang'), arxiv.Result.Author('Shujie Hu'), arxiv.Result.Author('Jiajun Deng'), arxiv.Result.Author('Guinan Li'), arxiv.Result.Author('Xunying Liu')]",2022-11-03 08:38:57+00:00,"Automatic recognition of disordered speech remains a highly challenging task
to date. The underlying neuro-motor conditions, often compounded with
co-occurring physical disabilities, lead to the difficulty in collecting large
quantities of impaired speech required for ASR system development. This paper
presents novel variational auto-encoder generative adversarial network
(VAE-GAN) based personalized disordered speech augmentation approaches that
simultaneously learn to encode, generate and discriminate synthesized impaired
speech. Separate latent features are derived to learn dysarthric speech
characteristics and phoneme context representations. Self-supervised
pre-trained Wav2vec 2.0 embedding features are also incorporated. Experiments
conducted on the UASpeech corpus suggest the proposed adversarial data
augmentation approach consistently outperformed the baseline speed perturbation
and non-VAE GAN augmentation methods with trained hybrid TDNN and End-to-end
Conformer systems. After LHUC speaker adaptation, the best system using VAE-GAN
based augmentation produced an overall WER of 27.78% on the UASpeech test set
of 16 dysarthric speakers, and the lowest published WER of 57.31% on the subset
of speakers with ""Very Low"" intelligibility.",Submitted to ICASSP 2023,
From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction,"[arxiv.Result.Author('Xiuyu Wu'), arxiv.Result.Author('Yunfang Wu')]",2022-11-03 07:30:09+00:00,"Chinese Grammatical Error Correction (CGEC) aims to generate a correct
sentence from an erroneous sequence, where different kinds of errors are mixed.
This paper divides the CGEC task into two steps, namely spelling error
correction and grammatical error correction. Specifically, we propose a novel
zero-shot approach for spelling error correction, which is simple but
effective, obtaining a high precision to avoid error accumulation of the
pipeline structure. To handle grammatical error correction, we design
part-of-speech (POS) features and semantic class features to enhance the neural
network model, and propose an auxiliary task to predict the POS sequence of the
target sentence. Our proposed framework achieves a 42.11 F0.5 score on CGEC
dataset without using any synthetic data or data augmentation methods, which
outperforms the previous state-of-the-art by a wide margin of 1.30 points.
Moreover, our model produces meaningful POS representations that capture
different POS words and convey reasonable POS transition rules.",,
Leveraging Domain Features for Detecting Adversarial Attacks Against Deep Speech Recognition in Noise,"[arxiv.Result.Author('Christian Heider Nielsen'), arxiv.Result.Author('Zheng-Hua Tan')]",2022-11-03 07:25:45+00:00,"In recent years, significant progress has been made in deep model-based
automatic speech recognition (ASR), leading to its widespread deployment in the
real world. At the same time, adversarial attacks against deep ASR systems are
highly successful. Various methods have been proposed to defend ASR systems
from these attacks. However, existing classification based methods focus on the
design of deep learning models while lacking exploration of domain specific
features. This work leverages filter bank-based features to better capture the
characteristics of attacks for improved detection. Furthermore, the paper
analyses the potentials of using speech and non-speech parts separately in
detecting adversarial attacks. In the end, considering adverse environments
where ASR systems may be deployed, we study the impact of acoustic noise of
various types and signal-to-noise ratios. Extensive experiments show that the
inverse filter bank features generally perform better in both clean and noisy
environments, the detection is effective using either speech or non-speech
part, and the acoustic noise can largely degrade the detection performance.",,
Convolution channel separation and frequency sub-bands aggregation for music genre classification,"[arxiv.Result.Author('Jungwoo Heo'), arxiv.Result.Author('Hyun-seo Shin'), arxiv.Result.Author('Ju-ho Kim'), arxiv.Result.Author('Chan-yeong Lim'), arxiv.Result.Author('Ha-Jin Yu')]",2022-11-03 06:03:39+00:00,"In music, short-term features such as pitch and tempo constitute long-term
semantic features such as melody and narrative. A music genre classification
(MGC) system should be able to analyze these features. In this research, we
propose a novel framework that can extract and aggregate both short- and
long-term features hierarchically. Our framework is based on ECAPA-TDNN, where
all the layers that extract short-term features are affected by the layers that
extract long-term features because of the back-propagation training. To prevent
the distortion of short-term features, we devised the convolution channel
separation technique that separates short-term features from long-term feature
extraction paths. To extract more diverse features from our framework, we
incorporated the frequency sub-bands aggregation method, which divides the
input spectrogram along frequency bandwidths and processes each segment. We
evaluated our framework using the Melon Playlist dataset which is a large-scale
dataset containing 600 times more data than GTZAN which is a widely used
dataset in MGC studies. As the result, our framework achieved 70.4% accuracy,
which was improved by 16.9% compared to a conventional framework.",,
"The ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge (ICSRC): Dataset, Tracks, Baseline and Results","[arxiv.Result.Author('Ao Zhang'), arxiv.Result.Author('Fan Yu'), arxiv.Result.Author('Kaixun Huang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Longbiao Wang'), arxiv.Result.Author('Eng Siong Chng'), arxiv.Result.Author('Hui Bu'), arxiv.Result.Author('Binbin Zhang'), arxiv.Result.Author('Wei Chen'), arxiv.Result.Author('Xin Xu')]",2022-11-03 04:45:28+00:00,"This paper summarizes the outcomes from the ISCSLP 2022 Intelligent Cockpit
Speech Recognition Challenge (ICSRC). We first address the necessity of the
challenge and then introduce the associated dataset collected from a new-energy
vehicle (NEV) covering a variety of cockpit acoustic conditions and linguistic
contents. We then describe the track arrangement and the baseline system.
Specifically, we set up two tracks in terms of allowed model/system size to
investigate resource-constrained and -unconstrained setups, targeting to
vehicle embedded as well as cloud ASR systems respectively. Finally we
summarize the challenge results and provide the major observations from the
submitted systems.",Accepted by ISCSLP2022,
Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system,"[arxiv.Result.Author('Li Li'), arxiv.Result.Author('Dongxing Xu'), arxiv.Result.Author('Haoran Wei'), arxiv.Result.Author('Yanhua Long')]",2022-11-03 03:36:51+00:00,"Exploiting effective target modeling units is very important and has always
been a concern in end-to-end automatic speech recognition (ASR). In this work,
we propose a phonetic-assisted multi-target units (PMU) modeling approach, to
enhance the Conformer-Transducer ASR system in a progressive representation
learning manner. Specifically, PMU first uses the pronunciation-assisted
subword modeling (PASM) and byte pair encoding (BPE) to produce
phonetic-induced and text-induced target units separately; Then, three new
frameworks are investigated to enhance the acoustic encoder, including a basic
PMU, a paraCTC and a pcaCTC, they integrate the PASM and BPE units at different
levels for CTC and transducer multi-task training. Experiments on both
LibriSpeech and accented ASR tasks show that, the proposed PMU significantly
outperforms the conventional BPE, it reduces the WER of LibriSpeech clean,
other, and six accented ASR testsets by relative 12.7%, 6.0% and 7.7%,
respectively.","5 pages, 1 figures, submitted to ICASSP 2023",
Losses Can Be Blessings: Routing Self-Supervised Speech Representations Towards Efficient Multilingual and Multitask Speech Processing,"[arxiv.Result.Author('Yonggan Fu'), arxiv.Result.Author('Yang Zhang'), arxiv.Result.Author('Kaizhi Qian'), arxiv.Result.Author('Zhifan Ye'), arxiv.Result.Author('Zhongzhi Yu'), arxiv.Result.Author('Cheng-I Lai'), arxiv.Result.Author('Yingyan Lin')]",2022-11-02 23:47:55+00:00,"Self-supervised learning (SSL) for rich speech representations has achieved
empirical success in low-resource Automatic Speech Recognition (ASR) and other
speech processing tasks, which can mitigate the necessity of a large amount of
transcribed speech and thus has driven a growing demand for on-device ASR and
other speech processing. However, advanced speech SSL models have become
increasingly large, which contradicts the limited on-device resources. This gap
could be more severe in multilingual/multitask scenarios requiring
simultaneously recognizing multiple languages or executing multiple speech
processing tasks. Additionally, strongly overparameterized speech SSL models
tend to suffer from overfitting when being finetuned on low-resource speech
corpus. This work aims to enhance the practical usage of speech SSL models
towards a win-win in both enhanced efficiency and alleviated overfitting via
our proposed S$^3$-Router framework, which for the first time discovers that
simply discarding no more than 10\% of model weights via only finetuning model
connections of speech SSL models can achieve better accuracy over standard
weight finetuning on downstream speech processing tasks. More importantly,
S$^3$-Router can serve as an all-in-one technique to enable (1) a new
finetuning scheme, (2) an efficient multilingual/multitask solution, (3) a
state-of-the-art ASR pruning technique, and (4) a new tool to quantitatively
analyze the learned speech representation. We believe S$^3$-Router has provided
a new perspective for practical deployment of speech SSL models. Our codes are
available at: https://github.com/GATECH-EIC/S3-Router.",Accepted at NeurIPS 2022,
SLICER: Learning universal audio representations using low-resource self-supervised pre-training,"[arxiv.Result.Author('Ashish Seth'), arxiv.Result.Author('Sreyan Ghosh'), arxiv.Result.Author('S. Umesh'), arxiv.Result.Author('Dinesh Manocha')]",2022-11-02 23:45:33+00:00,"We present a new Self-Supervised Learning (SSL) approach to pre-train
encoders on unlabeled audio data that reduces the need for large amounts of
labeled data for audio and speech classification. Our primary aim is to learn
audio representations that can generalize across a large variety of speech and
non-speech tasks in a low-resource un-labeled audio pre-training setting.
Inspired by the recent success of clustering and contrasting learning paradigms
for SSL-based speech representation learning, we propose SLICER (Symmetrical
Learning of Instance and Cluster-level Efficient Representations), which brings
together the best of both clustering and contrasting learning paradigms. We use
a symmetric loss between latent representations from student and teacher
encoders and simultaneously solve instance and cluster-level contrastive
learning tasks. We obtain cluster representations online by just projecting the
input spectrogram into an output subspace with dimensions equal to the number
of clusters. In addition, we propose a novel mel-spectrogram augmentation
procedure, k-mix, based on mixup, which does not require labels and aids
unsupervised representation learning for audio. Overall, SLICER achieves
state-of-the-art results on the LAPE Benchmark \cite{9868132}, significantly
outperforming DeLoRes-M and other prior approaches, which are pre-trained on
$10\times$ larger of unsupervised data. We will make all our codes available on
GitHub.",Submitted to ICASSP 2023,
MAST: Multiscale Audio Spectrogram Transformers,"[arxiv.Result.Author('Sreyan Ghosh'), arxiv.Result.Author('Ashish Seth'), arxiv.Result.Author('S. Umesh'), arxiv.Result.Author('Dinesh Manocha')]",2022-11-02 23:34:12+00:00,"We present Multiscale Audio Spectrogram Transformer (MAST) for audio
classification, which brings the concept of multiscale feature hierarchies to
the Audio Spectrogram Transformer (AST). Given an input audio spectrogram we
first patchify and project it into an initial temporal resolution and embedding
dimension, post which the multiple stages in MAST progressively expand the
embedding dimension while reducing the temporal resolution of the input. We use
a pyramid structure that allows early layers of MAST operating at a high
temporal resolution but low embedding space to model simple low-level acoustic
information and deeper temporally coarse layers to model high-level acoustic
information with high-dimensional embeddings. We also extend our approach to
present a new Self-Supervised Learning (SSL) method called SS-MAST, which
calculates a symmetric contrastive loss between latent representations from a
student and a teacher encoder. In practice, MAST significantly outperforms AST
by an average accuracy of 3.4% across 8 speech and non-speech tasks from the
LAPE Benchmark. Moreover, SS-MAST achieves an absolute average improvement of
2.6% over SSAST for both AST and MAST encoders. We make all our codes available
on GitHub at the time of publication.",Submitted ICASSP 2023,
Phoneme Segmentation Using Self-Supervised Speech Models,"[arxiv.Result.Author('Luke Strgar'), arxiv.Result.Author('David Harwath')]",2022-11-02 19:57:31+00:00,"We apply transfer learning to the task of phoneme segmentation and
demonstrate the utility of representations learned in self-supervised
pre-training for the task. Our model extends transformer-style encoders with
strategically placed convolutions that manipulate features learned in
pre-training. Using the TIMIT and Buckeye corpora we train and test the model
in the supervised and unsupervised settings. The latter case is accomplished by
furnishing a noisy label-set with the predictions of a separate model, it
having been trained in an unsupervised fashion. Results indicate our model
eclipses previous state-of-the-art performance in both settings and on both
datasets. Finally, following observations during published code review and
attempts to reproduce past segmentation results, we find a need to disambiguate
the definition and implementation of widely-used evaluation metrics. We resolve
this ambiguity by delineating two distinct evaluation schemes and describing
their nuances.",Accepted to SLT 2022,
Towards Zero-Shot Code-Switched Speech Recognition,"[arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Matthew Wiesner'), arxiv.Result.Author('Ondrej Klejch'), arxiv.Result.Author('Preethi Jyothi'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-02 19:52:54+00:00,"In this work, we seek to build effective code-switched (CS) automatic speech
recognition systems (ASR) under the zero-shot setting where no transcribed CS
speech data is available for training. Previously proposed frameworks which
conditionally factorize the bilingual task into its constituent monolingual
parts are a promising starting point for leveraging monolingual data
efficiently. However, these methods require the monolingual modules to perform
language segmentation. That is, each monolingual module has to simultaneously
detect CS points and transcribe speech segments of one language while ignoring
those of other languages -- not a trivial task. We propose to simplify each
monolingual module by allowing them to transcribe all speech segments
indiscriminately with a monolingual script (i.e. transliteration). This simple
modification passes the responsibility of CS point detection to subsequent
bilingual modules which determine the final output by considering multiple
monolingual transliterations along with external language model information. We
apply this transliteration-based approach in an end-to-end differentiable
neural network and demonstrate its efficacy for zero-shot CS ASR on
Mandarin-English SEAME test sets.",5 pages,
Variable Attention Masking for Configurable Transformer Transducer Speech Recognition,"[arxiv.Result.Author('Pawel Swietojanski'), arxiv.Result.Author('Stefan Braun'), arxiv.Result.Author('Dogan Can'), arxiv.Result.Author('Thiago Fraga da Silva'), arxiv.Result.Author('Arnab Ghoshal'), arxiv.Result.Author('Takaaki Hori'), arxiv.Result.Author('Roger Hsiao'), arxiv.Result.Author('Henry Mason'), arxiv.Result.Author('Erik McDermott'), arxiv.Result.Author('Honza Silovsky'), arxiv.Result.Author('Ruchir Travadi'), arxiv.Result.Author('Xiaodan Zhuang')]",2022-11-02 19:14:02+00:00,"This work studies the use of attention masking in transformer transducer
based speech recognition for building a single configurable model for different
deployment scenarios. We present a comprehensive set of experiments comparing
fixed masking, where the same attention mask is applied at every frame, with
chunked masking, where the attention mask for each frame is determined by chunk
boundaries, in terms of recognition accuracy and latency. We then explore the
use of variable masking, where the attention masks are sampled from a target
distribution at training time, to build models that can work in different
configurations. Finally, we investigate how a single configurable model can be
used to perform both first pass streaming recognition and second pass acoustic
rescoring. Experiments show that chunked masking achieves a better accuracy vs
latency trade-off compared to fixed masking, both with and without FastEmit. We
also show that variable masking improves the accuracy by up to 8% relative in
the acoustic re-scoring scenario.","5 pages, 4 figures, 2 Tables",
XAI-Increment: A Novel Approach Leveraging LIME Explanations for Improved Incremental Learning,"[arxiv.Result.Author('Arnab Neelim Mazumder'), arxiv.Result.Author('Niall Lyons'), arxiv.Result.Author('Anand Dubey'), arxiv.Result.Author('Ashutosh Pandey'), arxiv.Result.Author('Avik Santra')]",2022-11-02 18:16:17+00:00,"Explainability of neural network prediction is essential to understand
feature importance and gain interpretable insight into neural network
performance. In this work, model explanations are fed back to the feed-forward
training to help the model generalize better. To this extent, a custom weighted
loss where the weights are generated by considering the Euclidean distances
between true LIME (Local Interpretable Model-Agnostic Explanations)
explanations and model-predicted LIME explanations is proposed. Also, in
practical training scenarios, developing a solution that can help the model
learn sequentially without losing information on previous data distribution is
imperative due to the unavailability of all the training data at once. Thus,
the framework known as XAI-Increment incorporates the custom weighted loss
developed with elastic weight consolidation (EWC), to maintain performance in
sequential testing sets. Finally, the training procedure involving the custom
weighted loss shows around 1% accuracy improvement compared to the traditional
loss based training for the keyword spotting task on the Google Speech Commands
dataset and also shows low loss of information when coupled with EWC in the
incremental learning setup.",,
Improving Named Entity Recognition in Telephone Conversations via Effective Active Learning with Human in the Loop,"[arxiv.Result.Author('Md Tahmid Rahman Laskar'), arxiv.Result.Author('Cheng Chen'), arxiv.Result.Author('Xue-Yong Fu'), arxiv.Result.Author('Shashi Bhushan TN')]",2022-11-02 17:55:04+00:00,"Telephone transcription data can be very noisy due to speech recognition
errors, disfluencies, etc. Not only that annotating such data is very
challenging for the annotators, but also such data may have lots of annotation
errors even after the annotation job is completed, resulting in a very poor
model performance. In this paper, we present an active learning framework that
leverages human in the loop learning to identify data samples from the
annotated dataset for re-annotation that are more likely to contain annotation
errors. In this way, we largely reduce the need for data re-annotation for the
whole dataset. We conduct extensive experiments with our proposed approach for
Named Entity Recognition and observe that by re-annotating only about 6%
training instances out of the whole dataset, the F1 score for a certain entity
type can be significantly improved by about 25%.","The final version of this paper will be published in the Proceedings
  of the DaSH Workshop @ EMNLP 2022. This paper is accepted for presentation in
  both DaSH@EMNLP 2022 and HiLL@NIPS 2022",
Predicting phoneme-level prosody latents using AR and flow-based Prior Networks for expressive speech synthesis,"[arxiv.Result.Author('Konstantinos Klapsas'), arxiv.Result.Author('Karolos Nikitaras'), arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Inchul Hwang'), arxiv.Result.Author('Spyros Raptis'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis')]",2022-11-02 17:45:01+00:00,"A large part of the expressive speech synthesis literature focuses on
learning prosodic representations of the speech signal which are then modeled
by a prior distribution during inference. In this paper, we compare different
prior architectures at the task of predicting phoneme level prosodic
representations extracted with an unsupervised FVAE model. We use both
subjective and objective metrics to show that normalizing flow based prior
networks can result in more expressive speech at the cost of a slight drop in
quality. Furthermore, we show that the synthesized speech has higher
variability, for a given text, due to the nature of normalizing flows. We also
propose a Dynamical VAE model, that can generate higher quality speech although
with decreased expressiveness and variability compared to the flow based
models.",Submitted to ICASSP 2023,
Low-Resource Music Genre Classification with Advanced Neural Model Reprogramming,"[arxiv.Result.Author('Yun-Ning Hung'), arxiv.Result.Author('Chao-Han Huck Yang'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Alexander Lerch')]",2022-11-02 17:38:33+00:00,"Transfer learning (TL) approaches have shown promising results when handling
tasks with limited training data. However, considerable memory and
computational resources are often required for fine-tuning pre-trained neural
networks with target domain data. In this work, we introduce a novel method for
leveraging pre-trained models for low-resource (music) classification based on
the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a
pre-trained model from a source domain to a target domain by modifying the
input of a frozen pre-trained model. In addition to the known,
input-independent, reprogramming method, we propose an advanced reprogramming
paradigm: Input-dependent NMR, to increase adaptability to complex input data
such as musical audio. Experimental results suggest that a neural model
pre-trained on large-scale datasets can successfully perform music genre
classification by using this reprogramming method. The two proposed
Input-dependent NMR TL methods outperform fine-tuning-based TL methods on a
small genre classification dataset.","Submitted to ICASSP 2023. Some experimental results were reduced due
  to the space limit. The implementation will be available at
  https://github.com/biboamy/music-repro",
Towards End-to-end Speaker Diarization in the Wild,"[arxiv.Result.Author('Zexu Pan'), arxiv.Result.Author('Gordon Wichern'), arxiv.Result.Author('François G. Germain'), arxiv.Result.Author('Aswin Subramanian'), arxiv.Result.Author('Jonathan Le Roux')]",2022-11-02 17:20:42+00:00,"Speaker diarization algorithms address the ""who spoke when"" problem in audio
recordings. Algorithms trained end-to-end have proven superior to classical
modular-cascaded systems in constrained scenarios with a small number of
speakers. However, their performance for in-the-wild recordings containing more
speakers with shorter utterance lengths remains to be investigated. In this
paper, we address this gap, showing that an attractor-based end-to-end system
can also perform remarkably well in the latter scenario when first pre-trained
on a carefully-designed simulated dataset that matches the distribution of
in-the-wild recordings. We also propose to use an attention mechanism to
increase the network capacity in decoding more speaker attractors, and to
jointly train the attractors on a speaker recognition task to improve the
speaker attractor representation. Even though the model we propose is
audio-only, we find it significantly outperforms both audio-only and
audio-visual baselines on the AVA-AVD benchmark dataset, achieving
state-of-the-art results with an absolute reduction in diarization error of
23.3%.","5 pages, 2 figures, 2 tables. Submitted to ICASSP 2023",
A Quantum Kernel Learning Approach to Acoustic Modeling for Spoken Command Recognition,"[arxiv.Result.Author('Chao-Han Huck Yang'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Nanxin Chen'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Sabato Marco Siniscalchi'), arxiv.Result.Author('Chin-Hui Lee')]",2022-11-02 16:46:23+00:00,"We propose a quantum kernel learning (QKL) framework to address the inherent
data sparsity issues often encountered in training large-scare acoustic models
in low-resource scenarios. We project acoustic features based on
classical-to-quantum feature encoding. Different from existing quantum
convolution techniques, we utilize QKL with features in the quantum space to
design kernel-based classifiers. Experimental results on challenging spoken
command recognition tasks for a few low-resource languages, such as Arabic,
Georgian, Chuvash, and Lithuanian, show that the proposed QKL-based hybrid
approach attains good improvements over existing classical and quantum
solutions.",Submitted to ICASSP 2023,
data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup,"[arxiv.Result.Author('Vasista Sai Lodagala'), arxiv.Result.Author('Sreyan Ghosh'), arxiv.Result.Author('S. Umesh')]",2022-11-02 16:29:59+00:00,"In this paper, we propose a new Self-Supervised Learning (SSL) algorithm
called data2vec-aqc, for speech representation learning from unlabeled speech
data. Our goal is to improve SSL for speech in domains where both unlabeled and
labeled data are limited. Building on the recently introduced data2vec, we
introduce additional modules to the data2vec framework that leverage the
benefit of data augmentations, quantized representations, and clustering. The
interaction between these modules helps solve the cross-contrastive loss as an
additional self-supervised objective. data2vec-aqc achieves up to 14.1% and
20.9% relative WER improvement over the existing state-of-the-art data2vec
system on the test-clean and test-other sets, respectively, of LibriSpeech,
without the use of any language model. Our proposed model also achieves up to
17.8% relative WER improvement over the baseline data2vec when fine-tuned on
Switchboard data.","Submitted to ICASSP 2023. arXiv admin note: text overlap with
  arXiv:2210.02592",
Audio Language Modeling using Perceptually-Guided Discrete Representations,"[arxiv.Result.Author('Felix Kreuk'), arxiv.Result.Author('Yaniv Taigman'), arxiv.Result.Author('Adam Polyak'), arxiv.Result.Author('Jade Copet'), arxiv.Result.Author('Gabriel Synnaeve'), arxiv.Result.Author('Alexandre Défossez'), arxiv.Result.Author('Yossi Adi')]",2022-11-02 16:02:45+00:00,"In this work, we study the task of Audio Language Modeling, in which we aim
at learning probabilistic models for audio that can be used for generation and
completion. We use a state-of-the-art perceptually-guided audio compression
model, to encode audio to discrete representations. Next, we train a
transformer-based causal language model using these representations. At
inference time, we perform audio auto-completion by encoding an audio prompt as
a discrete sequence, feeding it to the audio language model, sampling from the
model, and synthesizing the corresponding time-domain signal. We evaluate the
quality of samples generated by our method on Audioset, the largest dataset for
general audio to date, and show that it is superior to the evaluated baseline
audio encoders. We additionally provide an extensive analysis to better
understand the trade-off between audio-quality and language-modeling
capabilities. Samples:link.",,
Analysis of Noisy-target Training for DNN-based speech enhancement,"[arxiv.Result.Author('Takuya Fujimura'), arxiv.Result.Author('Tomoki Toda')]",2022-11-02 15:21:28+00:00,"Deep neural network (DNN)-based speech enhancement usually uses a clean
speech as a training target. However, it is hard to collect large amounts of
clean speech because the recording is very costly. In other words, the
performance of current speech enhancement has been limited by the amount of
training data. To relax this limitation, Noisy-target Training (NyTT) that
utilizes noisy speech as a training target has been proposed. Although it has
been experimentally shown that NyTT can train a DNN without clean speech, a
detailed analysis has not been conducted and its behavior has not been
understood well. In this paper, we conduct various analyses to deepen our
understanding of NyTT. In addition, based on the property of NyTT, we propose a
refined method that is comparable to the method using clean speech.
Furthermore, we show that we can improve the performance by using a huge amount
of noisy speech with clean speech.",Submitted to ICASSP 2023,
Inference and Denoise: Causal Inference-based Neural Speech Enhancement,"[arxiv.Result.Author('Tsun-An Hsieh'), arxiv.Result.Author('Chao-Han Huck Yang'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Sabato Marco Siniscalchi'), arxiv.Result.Author('Yu Tsao')]",2022-11-02 15:03:50+00:00,"This study addresses the speech enhancement (SE) task within the causal
inference paradigm by modeling the noise presence as an intervention. Based on
the potential outcome framework, the proposed causal inference-based speech
enhancement (CISE) separates clean and noisy frames in an intervened noisy
speech using a noise detector and assigns both sets of frames to two mask-based
enhancement modules (EMs) to perform noise-conditional SE. Specifically, we use
the presence of noise as guidance for EM selection during training, and the
noise detector selects the enhancement module according to the prediction of
the presence of noise for each frame. Moreover, we derived a SE-specific
average treatment effect to quantify the causal effect adequately. Experimental
evidence demonstrates that CISE outperforms a non-causal mask-based SE approach
in the studied settings and has better performance and efficiency than more
complex SE models.",Submitted to ICASSP 2023,
"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval","[arxiv.Result.Author('Layne Berry'), arxiv.Result.Author('Yi-Jen Shih'), arxiv.Result.Author('Hsuan-Fu Wang'), arxiv.Result.Author('Heng-Jui Chang'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('David Harwath')]",2022-11-02 14:54:45+00:00,"This work investigates the use of large-scale, pre-trained models (CLIP and
HuBERT) for multilingual speech-image retrieval. For non-English speech-image
retrieval, we outperform the current state-of-the-art performance by a wide
margin when training separate models for each language, and show that a single
model which processes speech in all three languages still achieves retrieval
scores comparable with the prior state-of-the-art. We identify key differences
in model behavior and performance between English and non-English settings,
presumably attributable to the English-only pre-training of CLIP and HuBERT.
Finally, we show that our models can be used for mono- and cross-lingual
speech-text retrieval and cross-lingual speech-speech retrieval, despite never
having seen any parallel speech-text or speech-speech data during training.",Submitted to ICASSP 2023,
I4U System Description for NIST SRE'20 CTS Challenge,"[arxiv.Result.Author('Kong Aik Lee'), arxiv.Result.Author('Tomi Kinnunen'), arxiv.Result.Author('Daniele Colibro'), arxiv.Result.Author('Claudio Vair'), arxiv.Result.Author('Andreas Nautsch'), arxiv.Result.Author('Hanwu Sun'), arxiv.Result.Author('Liang He'), arxiv.Result.Author('Tianyu Liang'), arxiv.Result.Author('Qiongqiong Wang'), arxiv.Result.Author('Mickael Rouvier'), arxiv.Result.Author('Pierre-Michel Bousquet'), arxiv.Result.Author('Rohan Kumar Das'), arxiv.Result.Author('Ignacio Viñals Bailo'), arxiv.Result.Author('Meng Liu'), arxiv.Result.Author('Héctor Deldago'), arxiv.Result.Author('Xuechen Liu'), arxiv.Result.Author('Md Sahidullah'), arxiv.Result.Author('Sandro Cumani'), arxiv.Result.Author('Boning Zhang'), arxiv.Result.Author('Koji Okabe'), arxiv.Result.Author('Hitoshi Yamamoto'), arxiv.Result.Author('Ruijie Tao'), arxiv.Result.Author('Haizhou Li'), arxiv.Result.Author('Alfonso Ortega Giménez'), arxiv.Result.Author('Longbiao Wang'), arxiv.Result.Author('Luis Buera')]",2022-11-02 13:04:27+00:00,"This manuscript describes the I4U submission to the 2020 NIST Speaker
Recognition Evaluation (SRE'20) Conversational Telephone Speech (CTS)
Challenge. The I4U's submission was resulted from active collaboration among
researchers across eight research teams - I$^2$R (Singapore), UEF (Finland),
VALPT (Italy, Spain), NEC (Japan), THUEE (China), LIA (France), NUS
(Singapore), INRIA (France) and TJU (China). The submission was based on the
fusion of top performing sub-systems and sub-fusion systems contributed by
individual teams. Efforts have been spent on the use of common development and
validation sets, submission schedule and milestone, minimizing inconsistency in
trial list and score file format across sites.","SRE 2021, NIST Speaker Recognition Evaluation Workshop, CTS Speaker
  Recognition Challenge, 14-12 December 2021",
Transformer-based encoder-encoder architecture for Spoken Term Detection,"[arxiv.Result.Author('Jan Švec'), arxiv.Result.Author('Luboš Šmídl'), arxiv.Result.Author('Jan Lehečka')]",2022-11-02 13:03:15+00:00,"The paper presents a method for spoken term detection based on the
Transformer architecture. We propose the encoder-encoder architecture employing
two BERT-like encoders with additional modifications, including convolutional
and upsampling layers, attention masking, and shared parameters. The encoders
project a recognized hypothesis and a searched term into a shared embedding
space, where the score of the putative hit is computed using the calibrated dot
product. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the
proposed system outperformed a baseline method based on deep LSTMs on the
English and Czech STD datasets based on USC Shoah Foundation Visual History
Archive (MALACH).",Submitted to ICASSP 2023,
DSPGAN: a GAN-based universal vocoder for high-fidelity TTS by time-frequency domain supervision from DSP,"[arxiv.Result.Author('Kun Song'), arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Yi Lei'), arxiv.Result.Author('Jian Cong'), arxiv.Result.Author('Hanzhao Li'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Gang He'), arxiv.Result.Author('Jinfeng Bai')]",2022-11-02 12:56:56+00:00,"Recent development of neural vocoders based on the generative adversarial
neural network (GAN) has shown their advantages of generating raw waveform
conditioned on mel-spectrogram with fast inference speed and lightweight
networks. Whereas, it is still challenging to train a universal neural vocoder
that can synthesize high-fidelity speech from various scenarios with unseen
speakers, languages, and speaking styles. In this paper, we propose DSPGAN, a
GAN-based universal vocoder for high-fidelity speech synthesis by applying the
time-frequency domain supervision from digital signal processing (DSP). To
eliminate the mismatch problem caused by the ground-truth spectrograms in
training phase and the predicted spectrograms in inference phase, we leverage
the mel-spectrogram extracted from the waveform generated by a DSP module,
rather than the predicted mel-spectrogram from the Text-to-Speech (TTS)
acoustic model, as the time-frequency domain supervision to the GAN-based
vocoder. We also utilize sine excitation as the time-domain supervision to
improve the harmonic modeling and eliminate various artifacts of the GAN-based
vocoder. Experimental results show that DSPGAN significantly outperforms the
compared approaches and can generate high-fidelity speech based on diverse data
in TTS.",Submitted to ICASSP 2023,
Intermediate Fine-Tuning Using Imperfect Synthetic Speech for Improving Electrolaryngeal Speech Recognition,"[arxiv.Result.Author('Lester Phillip Violeta'), arxiv.Result.Author('Ding Ma'), arxiv.Result.Author('Wen-Chin Huang'), arxiv.Result.Author('Tomoki Toda')]",2022-11-02 12:32:26+00:00,"Research on automatic speech recognition (ASR) systems for electrolaryngeal
speakers has been relatively unexplored due to small datasets. When training
data is lacking in ASR, a large-scale pretraining and fine tuning framework is
often sufficient to achieve high recognition rates; however, in
electrolaryngeal speech, the domain shift between the pretraining and
fine-tuning data is too large to overcome, limiting the maximum improvement of
recognition rates. To resolve this, we propose an intermediate fine-tuning step
that uses imperfect synthetic speech to close the domain shift gap between the
pretraining and target data. Despite the imperfect synthetic data, we show the
effectiveness of this on electrolaryngeal speech datasets, with improvements of
6.1% over the baseline that did not use imperfect synthetic speech. Results
show how the intermediate fine-tuning stage focuses on learning the high-level
inherent features of the imperfect synthetic data rather than the low-level
features such as intelligibility.",Submitted to ICASSP 2023,
Monolingual Recognizers Fusion for Code-switching Speech Recognition,"[arxiv.Result.Author('Tongtong Song'), arxiv.Result.Author('Qiang Xu'), arxiv.Result.Author('Haoyu Lu'), arxiv.Result.Author('Longbiao Wang'), arxiv.Result.Author('Hao Shi'), arxiv.Result.Author('Yuqin Lin'), arxiv.Result.Author('Yanbing Yang'), arxiv.Result.Author('Jianwu Dang')]",2022-11-02 11:24:26+00:00,"The bi-encoder structure has been intensively investigated in code-switching
(CS) automatic speech recognition (ASR). However, most existing methods require
the structures of two monolingual ASR models (MAMs) should be the same and only
use the encoder of MAMs. This leads to the problem that pre-trained MAMs cannot
be timely and fully used for CS ASR. In this paper, we propose a monolingual
recognizers fusion method for CS ASR. It has two stages: the speech awareness
(SA) stage and the language fusion (LF) stage. In the SA stage, acoustic
features are mapped to two language-specific predictions by two independent
MAMs. To keep the MAMs focused on their own language, we further extend the
language-aware training strategy for the MAMs. In the LF stage, the BELM fuses
two language-specific predictions to get the final prediction. Moreover, we
propose a text simulation strategy to simplify the training process of the BELM
and reduce reliance on CS data. Experiments on a Mandarin-English corpus show
the efficiency of the proposed method. The mix error rate is significantly
reduced on the test set after using open-source pre-trained MAMs.",Submitted to ICASSP2023,
Singing Voice Synthesis with Vibrato Modeling and Latent Energy Representation,"[arxiv.Result.Author('Yingjie Song'), arxiv.Result.Author('Wei Song'), arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Zhengchen Zhang'), arxiv.Result.Author('Dan Zeng'), arxiv.Result.Author('Zhi Liu'), arxiv.Result.Author('Yang Yu')]",2022-11-02 09:58:25+00:00,"This paper proposes an expressive singing voice synthesis system by
introducing explicit vibrato modeling and latent energy representation. Vibrato
is essential to the naturalness of synthesized sound, due to the inherent
characteristics of human singing. Hence, a deep learning-based vibrato model is
introduced in this paper to control the vibrato's likeliness, rate, depth and
phase in singing, where the vibrato likeliness represents the existence
probability of vibrato and it would help improve the singing voice's
naturalness. Actually, there is no annotated label about vibrato likeliness in
existing singing corpus. We adopt a novel vibrato likeliness labeling method to
label the vibrato likeliness automatically. Meanwhile, the power spectrogram of
audio contains rich information that can improve the expressiveness of singing.
An autoencoder-based latent energy bottleneck feature is proposed for
expressive singing voice synthesis. Experimental results on the open dataset
NUS48E show that both the vibrato modeling and the latent energy representation
could significantly improve the expressiveness of singing voice. The audio
samples are shown in the demo website.",,
Fast and efficient speech enhancement with variational autoencoders,"[arxiv.Result.Author('Mostafa Sadeghi'), arxiv.Result.Author('Romain Serizel')]",2022-11-02 09:52:13+00:00,"Unsupervised speech enhancement based on variational autoencoders has shown
promising performance compared with the commonly used supervised methods. This
approach involves the use of a pre-trained deep speech prior along with a
parametric noise model, where the noise parameters are learned from the noisy
speech signal with an expectationmaximization (EM)-based method. The E-step
involves an intractable latent posterior distribution. Existing algorithms to
solve this step are either based on computationally heavy Monte Carlo Markov
Chain sampling methods and variational inference, or inefficient
optimization-based methods. In this paper, we propose a new approach based on
Langevin dynamics that generates multiple sequences of samples and comes with a
total variation-based regularization to incorporate temporal correlations of
latent vectors. Our experiments demonstrate that the developed framework makes
an effective compromise between computational efficiency and enhancement
quality, and outperforms existing methods.",,
Weighted variance variational autoencoder for speech enhancement,"[arxiv.Result.Author('Ali Golmakani'), arxiv.Result.Author('Mostafa Sadeghi'), arxiv.Result.Author('Xavier Alameda-Pineda'), arxiv.Result.Author('Romain Serizel')]",2022-11-02 09:51:15+00:00,"We address speech enhancement based on variational autoencoders, which
involves learning a speech prior distribution in the time-frequency (TF)
domain. A zero-mean complexvalued Gaussian distribution is usually assumed for
the generative model, where the speech information is encoded in the variance
as a function of a latent variable. While this is the commonly used approach,
in this paper we propose a weighted variance generative model, where the
contribution of each TF point in parameter learning is weighted. We impose a
Gamma prior distribution on the weights, which would effectively lead to a
Student's t-distribution instead of Gaussian for speech modeling. We develop
efficient training and speech enhancement algorithms based on the proposed
generative model. Our experimental results on spectrogram modeling and speech
enhancement demonstrate the effectiveness and robustness of the proposed
approach compared to the standard unweighted variance model.",,
Audio-visual speech enhancement with a deep Kalman filter generative model,"[arxiv.Result.Author('Ali Golmakani'), arxiv.Result.Author('Mostafa Sadeghi'), arxiv.Result.Author('Romain Serizel')]",2022-11-02 09:50:08+00:00,"Deep latent variable generative models based on variational autoencoder (VAE)
have shown promising performance for audiovisual speech enhancement (AVSE). The
underlying idea is to learn a VAEbased audiovisual prior distribution for clean
speech data, and then combine it with a statistical noise model to recover a
speech signal from a noisy audio recording and video (lip images) of the target
speaker. Existing generative models developed for AVSE do not take into account
the sequential nature of speech data, which prevents them from fully
incorporating the power of visual data. In this paper, we present an
audiovisual deep Kalman filter (AV-DKF) generative model which assumes a
first-order Markov chain model for the latent variables and effectively fuses
audiovisual data. Moreover, we develop an efficient inference methodology to
estimate speech signals at test time. We conduct a set of experiments to
compare different variants of generative models for speech enhancement. The
results demonstrate the superiority of the AV-DKF model compared with both its
audio-only version and the non-sequential audio-only and audiovisual VAE-based
models.",,
SpectroMap: Peak detection algorithm for audio fingerprinting,[arxiv.Result.Author('Aarón López-García')],2022-11-02 09:40:22+00:00,"We present SpectroMap, an open source GitHub repository for audio
fingerprinting written in Python programming language. It is composed of a peak
search algorithm that extracts topological prominences from a spectrogram via
time-frequency bands. In this paper, we introduce the algorithm functioning
with two experimental applications in a high-quality urban sound dataset and
environmental audio recordings to describe how it works and how effective it is
in handling the input data.","7 pages, 3 figures",
Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation,"[arxiv.Result.Author('Rao Ma'), arxiv.Result.Author('Xiaobo Wu'), arxiv.Result.Author('Jin Qiu'), arxiv.Result.Author('Yanan Qin'), arxiv.Result.Author('Haihua Xu'), arxiv.Result.Author('Peihao Wu'), arxiv.Result.Author('Zejun Ma')]",2022-11-02 09:15:20+00:00,"ASR model deployment environment is ever-changing, and the incoming speech
can be switched across different domains during a session. This brings a
challenge for effective domain adaptation when only target domain text data is
available, and our objective is to obtain obviously improved performance on the
target domain while the performance on the general domain is less undermined.
In this paper, we propose an adaptive LM fusion approach called internal
language model estimation based adaptive domain adaptation (ILME-ADA). To
realize such an ILME-ADA, an interpolated log-likelihood score is calculated
based on the maximum of the scores from the internal LM and the external LM
(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method
with both RNN-T and LAS modeling frameworks employing neural network and n-gram
LMs as ELMs respectively on two domain specific (target) test sets. The
proposed method can achieve significantly better performance on the target test
sets while it gets minimal performance degradation on the general test set,
compared with both shallow and ILME-based LM fusion methods.",submitted to ICASSP 2023,
Multi-Speaker Multi-Style Speech Synthesis with Timbre and Style Disentanglement,"[arxiv.Result.Author('Wei Song'), arxiv.Result.Author('Yanghao Yue'), arxiv.Result.Author('Ya-jie Zhang'), arxiv.Result.Author('Zhengchen Zhang'), arxiv.Result.Author('Youzheng Wu'), arxiv.Result.Author('Xiaodong He')]",2022-11-02 09:13:49+00:00,"Disentanglement of a speaker's timbre and style is very important for style
transfer in multi-speaker multi-style text-to-speech (TTS) scenarios. With the
disentanglement of timbres and styles, TTS systems could synthesize expressive
speech for a given speaker with any style which has been seen in the training
corpus. However, there are still some shortcomings with the current research on
timbre and style disentanglement. The current method either requires
single-speaker multi-style recordings, which are difficult and expensive to
collect, or uses a complex network and complicated training method, which is
difficult to reproduce and control the style transfer behavior. To improve the
disentanglement effectiveness of timbres and styles, and to remove the reliance
on single-speaker multi-style corpus, a simple but effective timbre and style
disentanglement method is proposed in this paper. The FastSpeech2 network is
employed as the backbone network, with explicit duration, pitch, and energy
trajectory to represent the style. Each speaker's data is considered as a
separate and isolated style, then a speaker embedding and a style embedding are
added to the FastSpeech2 network to learn disentangled representations.
Utterance level pitch and energy normalization are utilized to improve the
decoupling effect. Experimental results demonstrate that the proposed model
could synthesize speech with any style seen during training with high style
similarity while maintaining very high speaker similarity.",,
Adversarial Guitar Amplifier Modelling With Unpaired Data,"[arxiv.Result.Author('Alec Wright'), arxiv.Result.Author('Vesa Välimäki'), arxiv.Result.Author('Lauri Juvela')]",2022-11-02 08:06:14+00:00,"We propose an audio effects processing framework that learns to emulate a
target electric guitar tone from a recording. We train a deep neural network
using an adversarial approach, with the goal of transforming the timbre of a
guitar, into the timbre of another guitar after audio effects processing has
been applied, for example, by a guitar amplifier. The model training requires
no paired data, and the resulting model emulates the target timbre well whilst
being capable of real-time processing on a modern personal computer. To verify
our approach we present two experiments, one which carries out unpaired
training using paired data, allowing us to monitor training via objective
metrics, and another that uses fully unpaired data, corresponding to a
realistic scenario where a user wants to emulate a guitar timbre only using
audio data from a recording. Our listening test results confirm that the models
are perceptually convincing.",,
Fast-U2++: Fast and Accurate End-to-End Speech Recognition in Joint CTC/Attention Frames,"[arxiv.Result.Author('Chengdong Liang'), arxiv.Result.Author('Xiao-Lei Zhang'), arxiv.Result.Author('BinBin Zhang'), arxiv.Result.Author('Di Wu'), arxiv.Result.Author('Shengqiang Li'), arxiv.Result.Author('Xingchen Song'), arxiv.Result.Author('Zhendong Peng'), arxiv.Result.Author('Fuping Pan')]",2022-11-02 08:01:52+00:00,"Recently, the unified streaming and non-streaming two-pass (U2/U2++)
end-to-end model for speech recognition has shown great performance in terms of
streaming capability, accuracy and latency. In this paper, we present
fast-U2++, an enhanced version of U2++ to further reduce partial latency. The
core idea of fast-U2++ is to output partial results of the bottom layers in its
encoder with a small chunk, while using a large chunk in the top layers of its
encoder to compensate the performance degradation caused by the small chunk.
Moreover, we use knowledge distillation method to reduce the token emission
latency. We present extensive experiments on Aishell-1 dataset. Experiments and
ablation studies show that compared to U2++, fast-U2++ reduces model latency
from 320ms to 80ms, and achieves a character error rate (CER) of 5.06% with a
streaming setup.","5 pages, 3 figures",
SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory,"[arxiv.Result.Author('Se Jin Park'), arxiv.Result.Author('Minsu Kim'), arxiv.Result.Author('Joanna Hong'), arxiv.Result.Author('Jeongsoo Choi'), arxiv.Result.Author('Yong Man Ro')]",2022-11-02 07:17:49+00:00,"The challenge of talking face generation from speech lies in aligning two
different modal information, audio and video, such that the mouth region
corresponds to input audio. Previous methods either exploit audio-visual
representation learning or leverage intermediate structural information such as
landmarks and 3D models. However, they struggle to synthesize fine details of
the lips varying at the phoneme level as they do not sufficiently provide
visual information of the lips at the video synthesis step. To overcome this
limitation, our work proposes Audio-Lip Memory that brings in visual
information of the mouth region corresponding to input audio and enforces
fine-grained audio-visual coherence. It stores lip motion features from
sequential ground truth images in the value memory and aligns them with
corresponding audio features so that they can be retrieved using audio input at
inference time. Therefore, using the retrieved lip motion features as visual
hints, it can easily correlate audio with visual dynamics in the synthesis
step. By analyzing the memory, we demonstrate that unique lip features are
stored in each memory slot at the phoneme level, capturing subtle lip motion
based on memory addressing. In addition, we introduce visual-visual
synchronization loss which can enhance lip-syncing performance when used along
with audio-visual synchronization loss in our model. Extensive experiments are
performed to verify that our method generates high-quality video with mouth
shapes that best align with the input audio, outperforming previous
state-of-the-art methods.",Accepted at AAAI 2022 (Oral),
SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation,"[arxiv.Result.Author('Yassine El Kheir'), arxiv.Result.Author('Shammur Absar Chowdhury'), arxiv.Result.Author('Hamdy Mubarak'), arxiv.Result.Author('Shazia Afzal'), arxiv.Result.Author('Ahmed Ali')]",2022-11-02 07:13:30+00:00,"One of the biggest challenges in designing mispronunciation detection models
is the unavailability of labeled L2 speech data. To overcome such data
scarcity, we introduce SpeechBlender -- a fine-grained data augmentation
pipeline for generating mispronunciation errors. The SpeechBlender utilizes
varieties of masks to target different regions of a phonetic unit, and use the
mixing factors to linearly interpolate raw speech signals while generating
erroneous pronunciation instances. The masks facilitate smooth blending of the
signals, thus generating more effective samples than the `Cut/Paste' method. We
show the effectiveness of our augmentation technique in a phoneme-level
pronunciation quality assessment task, leveraging only a good pronunciation
dataset. With SpeechBlender augmentation, we observed a 3% and 2% increase in
Pearson correlation coefficient (PCC) compared to no-augmentation and goodness
of pronunciation augmentation scenarios respectively for Speechocean762
testset. Moreover, a 2% rise in PCC is observed when comparing our single-task
phoneme-level mispronunciation detection model with a multi-task learning model
using multiple-granularity information.","5 pages, submitted to ICASSP 2023",
SIMD-size aware weight regularization for fast neural vocoding on CPU,"[arxiv.Result.Author('Hiroki Kanagawa'), arxiv.Result.Author('Yusuke Ijima')]",2022-11-02 05:43:53+00:00,"This paper proposes weight regularization for a faster neural vocoder.
Pruning time-consuming DNN modules is a promising way to realize a real-time
vocoder on a CPU (e.g. WaveRNN, LPCNet). Regularization that encourages
sparsity is also effective in avoiding the quality degradation created by
pruning. However, the orders of weight matrices must be contiguous in SIMD size
for fast vocoding. To ensure this order, we propose explicit SIMD size aware
regularization. Our proposed method reshapes a weight matrix into a tensor so
that the weights are aligned by group size in advance, and then computes the
group Lasso-like regularization loss. Experiments on 70% sparse subband WaveRNN
show that pruning in conventional Lasso and column-wise group Lasso degrades
the synthetic speech's naturalness. The vocoder with proposed regularization 1)
achieves comparable naturalness to that without pruning and 2) performs
meaningfully faster than other conventional vocoders using regularization.",Accepted to SLT 2022,
Factorized Blank Thresholding for Improved Runtime Efficiency of Neural Transducers,"[arxiv.Result.Author('Duc Le'), arxiv.Result.Author('Frank Seide'), arxiv.Result.Author('Yuhao Wang'), arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Kjell Schubert'), arxiv.Result.Author('Ozlem Kalinli'), arxiv.Result.Author('Michael L. Seltzer')]",2022-11-02 05:42:53+00:00,"We show how factoring the RNN-T's output distribution can significantly
reduce the computation cost and power consumption for on-device ASR inference
with no loss in accuracy. With the rise in popularity of neural-transducer type
models like the RNN-T for on-device ASR, optimizing RNN-T's runtime efficiency
is of great interest. While previous work has primarily focused on the
optimization of RNN-T's acoustic encoder and predictor, this paper focuses the
attention on the joiner. We show that despite being only a small part of RNN-T,
the joiner has a large impact on the overall model's runtime efficiency. We
propose to factorize the joiner into blank and non-blank portions for the
purpose of skipping the more expensive non-blank computation when the blank
probability exceeds a certain threshold. Since the blank probability can be
computed very efficiently and the RNN-T output is dominated by blanks, our
proposed method leads to a 26-30% decoding speed-up and 43-53% reduction in
on-device power consumption, all the while incurring no accuracy degradation
and being relatively simple to implement.",Submitted to ICASSP 2023,
Pop2Piano : Pop Audio-based Piano Cover Generation,"[arxiv.Result.Author('Jongho Choi'), arxiv.Result.Author('Kyogu Lee')]",2022-11-02 05:42:22+00:00,"The piano cover of pop music is widely enjoyed by people. However, the
generation task of the pop piano cover is still understudied. This is partly
due to the lack of synchronized {Pop, Piano Cover} data pairs, which made it
challenging to apply the latest data-intensive deep learning-based methods. To
leverage the power of the data-driven approach, we make a large amount of
paired and synchronized {pop, piano cover} data using an automated pipeline. In
this paper, we present Pop2Piano, a Transformer network that generates piano
covers given waveforms of pop music. To the best of our knowledge, this is the
first model to directly generate a piano cover from pop audio without melody
and chord extraction modules. We show that Pop2Piano trained with our dataset
can generate plausible piano covers.",,
Neural Fourier Shift for Binaural Speech Rendering,"[arxiv.Result.Author('Jin Woo Lee'), arxiv.Result.Author('Kyogu Lee')]",2022-11-02 04:55:09+00:00,"We present a neural network for rendering binaural speech from given monaural
audio, position, and orientation of the source. Most of the previous works have
focused on synthesizing binaural speeches by conditioning the positions and
orientations in the feature space of convolutional neural networks. These
synthesis approaches are powerful in estimating the target binaural speeches
even for in-the-wild data but are difficult to generalize for rendering the
audio from out-of-distribution domains. To alleviate this, we propose Neural
Fourier Shift (NFS), a novel network architecture that enables binaural speech
rendering in the Fourier space. Specifically, utilizing a geometric time delay
based on the distance between the source and the receiver, NFS is trained to
predict the delays and scales of various early reflections. NFS is efficient in
both memory and computational cost, is interpretable, and operates
independently of the source domain by its design. With up to 25 times lighter
memory and 6 times fewer calculations, the experimental results show that NFS
outperforms the previous studies on the benchmark dataset.",Submitted to ICASSP 2023,
Conversation-oriented ASR with multi-look-ahead CBS architecture,"[arxiv.Result.Author('Huaibo Zhao'), arxiv.Result.Author('Shinya Fujie'), arxiv.Result.Author('Tetsuji Ogawa'), arxiv.Result.Author('Jin Sakuma'), arxiv.Result.Author('Yusuke Kida'), arxiv.Result.Author('Tetsunori Kobayashi')]",2022-11-02 03:58:56+00:00,"During conversations, humans are capable of inferring the intention of the
speaker at any point of the speech to prepare the following action promptly.
Such ability is also the key for conversational systems to achieve rhythmic and
natural conversation. To perform this, the automatic speech recognition (ASR)
used for transcribing the speech in real-time must achieve high accuracy
without delay. In streaming ASR, high accuracy is assured by attending to
look-ahead frames, which leads to delay increments. To tackle this trade-off
issue, we propose a multiple latency streaming ASR to achieve high accuracy
with zero look-ahead. The proposed system contains two encoders that operate in
parallel, where a primary encoder generates accurate outputs utilizing
look-ahead frames, and the auxiliary encoder recognizes the look-ahead portion
of the primary encoder without look-ahead. The proposed system is constructed
based on contextual block streaming (CBS) architecture, which leverages block
processing and has a high affinity for the multiple latency architecture.
Various methods are also studied for architecting the system, including
shifting the network to perform as different encoders; as well as generating
both encoders' outputs in one encoding pass.",Submitted to ICASSP2023,
More Speaking or More Speakers?,"[arxiv.Result.Author('Dan Berrebbi'), arxiv.Result.Author('Ronan Collobert'), arxiv.Result.Author('Navdeep Jaitly'), arxiv.Result.Author('Tatiana Likhomanenko')]",2022-11-02 03:50:40+00:00,"Self-training (ST) and self-supervised learning (SSL) methods have
demonstrated strong improvements in automatic speech recognition (ASR). In
spite of these advances, to the best of our knowledge, there is no analysis of
how the composition of the labelled and unlabelled datasets used in these
methods affects the results. In this work we aim to analyse the effect of
numbers of speakers in the training data on a recent SSL algorithm (wav2vec
2.0), and a recent ST algorithm (slimIPL). We perform a systematic analysis on
both labeled and unlabeled data by varying the number of speakers while keeping
the number of hours fixed and vice versa. Our findings suggest that SSL
requires a large amount of unlabeled data to produce high accuracy results,
while ST requires a sufficient number of speakers in the labelled data,
especially in the low-regime setting. In this manner these two approaches
improve supervised learning in different regimes of dataset composition.",,
LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification,"[arxiv.Result.Author('Xing Chen'), arxiv.Result.Author('Jie Wang'), arxiv.Result.Author('Xiao-Lei Zhang'), arxiv.Result.Author('Wei-Qiang Zhang'), arxiv.Result.Author('Kunde Yang')]",2022-11-02 02:03:53+00:00,"Although the security of automatic speaker verification (ASV) is seriously
threatened by recently emerged adversarial attacks, there have been some
countermeasures to alleviate the threat. However, many defense approaches not
only require the prior knowledge of the attackers but also possess weak
interpretability. To address this issue, in this paper, we propose an
attacker-independent and interpretable method, named learnable mask detector
(LMD), to separate adversarial examples from the genuine ones. It utilizes
score variation as an indicator to detect adversarial examples, where the score
variation is the absolute discrepancy between the ASV scores of an original
audio recording and its transformed audio synthesized from its masked complex
spectrogram. A core component of the score variation detector is to generate
the masked spectrogram by a neural network. The neural network needs only
genuine examples for training, which makes it an attacker-independent approach.
Its interpretability lies that the neural network is trained to minimize the
score variation of the targeted ASV, and maximize the number of the masked
spectrogram bins of the genuine training examples. Its foundation is based on
the observation that, masking out the vast majority of the spectrogram bins
with little speaker information will inevitably introduce a large score
variation to the adversarial example, and a small score variation to the
genuine example. Experimental results with 12 attackers and two representative
ASV systems show that our proposed method outperforms five state-of-the-art
baselines. The extensive experimental results can also be a benchmark for the
detection-based ASV defenses.","13 pages, 9 figures",
Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022,"[arxiv.Result.Author('Zhengyang Chen'), arxiv.Result.Author('Bing Han'), arxiv.Result.Author('Xu Xiang'), arxiv.Result.Author('Houjun Huang'), arxiv.Result.Author('Bei Liu'), arxiv.Result.Author('Yanmin Qian')]",2022-11-02 01:33:23+00:00,"Different speaker recognition challenges have been held to assess the speaker
verification system in the wild and probe the performance limit. Voxceleb
Speaker Recognition Challenge (VoxSRC), based on the voxceleb, is the most
popular. Besides, another challenge called CN-Celeb Speaker Recognition
Challenge (CNSRC) is also held this year, which is based on the Chinese
celebrity multi-genre dataset CN-Celeb. This year, our team participated in
both speaker verification closed tracks in CNSRC 2022 and VoxSRC 2022, and
achieved the 1st place and 3rd place respectively. In most system reports, the
authors usually only provide a description of their systems but lack an
effective analysis of their methods. In this paper, we will outline how to
build a strong speaker verification challenge system and give a detailed
analysis of each method compared with some other popular technical means.",,
InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss,"[arxiv.Result.Author('Yosuke Higuchi'), arxiv.Result.Author('Tetsuji Ogawa'), arxiv.Result.Author('Tetsunori Kobayashi'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-02 00:18:25+00:00,"This paper presents InterMPL, a semi-supervised learning method of end-to-end
automatic speech recognition (ASR) that performs pseudo-labeling (PL) with
intermediate supervision. Momentum PL (MPL) trains a connectionist temporal
classification (CTC)-based model on unlabeled data by continuously generating
pseudo-labels on the fly and improving their quality. In contrast to
autoregressive formulations, such as the attention-based encoder-decoder and
transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in
general, owing to its simple/fast inference algorithm and robustness against
generating collapsed labels. However, CTC generally yields inferior performance
than the autoregressive models due to the conditional independence assumption,
thereby limiting the performance of MPL. We propose to enhance MPL by
introducing intermediate loss, inspired by the recent advances in CTC-based
modeling. Specifically, we focus on self-conditional and hierarchical
conditional CTC, that apply auxiliary CTC losses to intermediate layers such
that the conditional independence assumption is explicitly relaxed. We also
explore how pseudo-labels should be generated and used as supervision for
intermediate losses. Experimental results in different semi-supervised settings
demonstrate that the proposed approach outperforms MPL and improves an ASR
model by up to a 12.1% absolute performance gain. In addition, our detailed
analysis validates the importance of the intermediate loss.",Submitted to ICASSP2023,
Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild,"[arxiv.Result.Author('Jose Vargas-Quiros'), arxiv.Result.Author('Laura Cabrera-Quiros'), arxiv.Result.Author('Catharine Oertel'), arxiv.Result.Author('Hayley Hung')]",2022-11-02 00:18:08+00:00,"Laughter is considered one of the most overt signals of joy. Laughter is
well-recognized as a multimodal phenomenon but is most commonly detected by
sensing the sound of laughter. It is unclear how perception and annotation of
laughter differ when annotated from other modalities like video, via the body
movements of laughter. In this paper we take a first step in this direction by
asking if and how well laughter can be annotated when only audio, only video
(containing full body movement information) or audiovisual modalities are
available to annotators. We ask whether annotations of laughter are congruent
across modalities, and compare the effect that labeling modality has on machine
learning model performance. We compare annotations and models for laughter
detection, intensity estimation, and segmentation, three tasks common in
previous studies of laughter. Our analysis of more than 4000 annotations
acquired from 48 annotators revealed evidence for incongruity in the perception
of laughter, and its intensity between modalities. Further analysis of
annotations against consolidated audiovisual reference annotations revealed
that recall was lower on average for video when compared to the audio
condition, but tended to increase with the intensity of the laughter samples.
Our machine learning experiments compared the performance of state-of-the-art
unimodal (audio-based, video-based and acceleration-based) and multi-modal
models for different combinations of input modalities, training label modality,
and testing label modality. Models with video and acceleration inputs had
similar performance regardless of training label modality, suggesting that it
may be entirely appropriate to train models for laughter detection from body
movements using video-acquired labels, despite their lower inter-rater
agreement.",,
BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder,"[arxiv.Result.Author('Yosuke Higuchi'), arxiv.Result.Author('Tetsuji Ogawa'), arxiv.Result.Author('Tetsunori Kobayashi'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-02 00:10:43+00:00,"We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech
recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced
encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR
has been actively studied, aiming to utilize versatile linguistic knowledge for
generating accurate text. One crucial factor that makes this integration
challenging lies in the vocabulary mismatch; the vocabulary constructed for a
pre-trained LM is generally too large for E2E-ASR training and is likely to
have a mismatch against a target ASR domain. To overcome such an issue, we
propose BECTRA, an extended version of our previous BERT-CTC, that realizes
BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based
model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder
using a vocabulary suitable for a target task. With the combination of the
transducer and BERT-CTC, we also propose a novel inference algorithm for taking
advantage of both autoregressive and non-autoregressive decoding. Experimental
results on several ASR tasks, varying in amounts of data, speaking styles, and
languages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing
with the vocabulary mismatch while exploiting BERT knowledge.",Submitted to ICASSP2023,
Unified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems,"[arxiv.Result.Author('Shaan Bijwadia'), arxiv.Result.Author('Shuo-yiin Chang'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Tara Sainath'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Yanzhang He')]",2022-11-01 23:43:15+00:00,"Automatic speech recognition (ASR) systems typically rely on an external
endpointer (EP) model to identify speech boundaries. In this work, we propose a
method to jointly train the ASR and EP tasks in a single end-to-end (E2E)
multitask model, improving EP quality by optionally leveraging information from
the ASR audio encoder. We introduce a ""switch"" connection, which trains the EP
to consume either the audio frames directly or low-level latent representations
from the ASR model. This results in a single E2E model that can be used during
inference to perform frame filtering at low cost, and also make high quality
end-of-query (EOQ) predictions based on ongoing ASR computation. We present
results on a voice search test set showing that, compared to separate
single-task models, this approach reduces median endpoint latency by 120 ms
(30.8% reduction), and 90th percentile latency by 170 ms (23.0% reduction),
without regressing word error rate. For continuous recognition, WER improves by
10.6% (relative).",To be published in Spoken Language Technology Workshop (SLT) 2022,
Comparision Of Adversarial And Non-Adversarial LSTM Music Generative Models,"[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Anna Sergeevna Bosman'), arxiv.Result.Author('Johan Pieter De Villiers')]",2022-11-01 20:23:49+00:00,"Algorithmic music composition is a way of composing musical pieces with
minimal to no human intervention. While recurrent neural networks are
traditionally applied to many sequence-to-sequence prediction tasks, including
successful implementations of music composition, their standard supervised
learning approach based on input-to-output mapping leads to a lack of note
variety. These models can therefore be seen as potentially unsuitable for tasks
such as music generation. Generative adversarial networks learn the generative
distribution of data and lead to varied samples. This work implements and
compares adversarial and non-adversarial training of recurrent neural network
music composers on MIDI data. The resulting music samples are evaluated by
human listeners, their preferences recorded. The evaluation indicates that
adversarial training produces more aesthetically pleasing music.","Submitted to a 2023 conference, 20 pages, 13 figures",
SCA: Streaming Cross-attention Alignment for Echo Cancellation,"[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Yangyang Shi'), arxiv.Result.Author('Yun Li'), arxiv.Result.Author('Kaustubh Kalgaonkar'), arxiv.Result.Author('Sriram Srinivasan'), arxiv.Result.Author('Xin Lei')]",2022-11-01 17:01:50+00:00,"End-to-End deep learning has shown promising results for speech enhancement
tasks, such as noise suppression, dereverberation, and speech separation.
However, most state-of-the-art methods for echo cancellation are either
classical DSP-based or hybrid DSP-ML algorithms. Components such as the delay
estimator and adaptive linear filter are based on traditional signal processing
concepts, and deep learning algorithms typically only serve to replace the
non-linear residual echo suppressor. This paper introduces an end-to-end echo
cancellation network with a streaming cross-attention alignment (SCA). Our
proposed method can handle unaligned inputs without requiring external
alignment and generate high-quality speech without echoes. At the same time,
the end-to-end algorithm simplifies the current echo cancellation pipeline for
time-variant echo path cases. We test our proposed method on the ICASSP2022 and
Interspeech2021 Microsoft deep echo cancellation challenge evaluation dataset,
where our method outperforms some of the other hybrid and end-to-end methods.",,
T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5,"[arxiv.Result.Author('Chan-Jan Hsu'), arxiv.Result.Author('Ho-Lam Chung'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Yu Tsao')]",2022-11-01 17:00:23+00:00,"In Spoken language understanding (SLU), a natural solution is concatenating
pre-trained speech models (e.g. HuBERT) and pretrained language models (PLM,
e.g. T5). Most previous works use pretrained language models with subword-based
tokenization. However, the granularity of input units affects the alignment of
speech model outputs and language model inputs, and PLM with character-based
tokenization is underexplored. In this work, we conduct extensive studies on
how PLMs with different tokenization strategies affect spoken language
understanding task including spoken question answering (SQA) and speech
translation (ST). We further extend the idea to create T5lephone(pronounced as
telephone), a variant of T5 that is pretrained using phonemicized text. We
initialize T5lephone with existing PLMs to pretrain it using relatively
lightweight computational resources. We reached state-of-the-art on NMSQA, and
the T5lephone model exceeds T5 with other types of units on end-to-end SQA and
ST.",,
Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers,"[arxiv.Result.Author('Cheng-Ping Hsieh'), arxiv.Result.Author('Subhankar Ghosh'), arxiv.Result.Author('Boris Ginsburg')]",2022-11-01 16:59:54+00:00,"Fine-tuning is a popular method for adapting text-to-speech (TTS) models to
new speakers. However this approach has some challenges. Usually fine-tuning
requires several hours of high quality speech per speaker. There is also that
fine-tuning will negatively affect the quality of speech synthesis for
previously learnt speakers. In this paper we propose an alternative approach
for TTS adaptation based on using parameter-efficient adapter modules. In the
proposed approach, a few small adapter modules are added to the original
network. The original weights are frozen, and only the adapters are fine-tuned
on speech for new speaker. The parameter-efficient fine-tuning approach will
produce a new model with high level of parameter sharing with original model.
Our experiments on LibriTTS, HiFi-TTS and VCTK datasets validate the
effectiveness of adapter-based method through objective and subjective metrics.",Submitted to ICASSP 2023,
No-audio speaking status detection in crowded settings via visual pose-based filtering and wearable acceleration,"[arxiv.Result.Author('Jose Vargas-Quiros'), arxiv.Result.Author('Laura Cabrera-Quiros'), arxiv.Result.Author('Hayley Hung')]",2022-11-01 15:55:48+00:00,"Recognizing who is speaking in a crowded scene is a key challenge towards the
understanding of the social interactions going on within. Detecting speaking
status from body movement alone opens the door for the analysis of social
scenes in which personal audio is not obtainable. Video and wearable sensors
make it possible recognize speaking in an unobtrusive, privacy-preserving way.
When considering the video modality, in action recognition problems, a bounding
box is traditionally used to localize and segment out the target subject, to
then recognize the action taking place within it. However, cross-contamination,
occlusion, and the articulated nature of the human body, make this approach
challenging in a crowded scene. Here, we leverage articulated body poses for
subject localization and in the subsequent speech detection stage. We show that
the selection of local features around pose keypoints has a positive effect on
generalization performance while also significantly reducing the number of
local features considered, making for a more efficient method. Using two
in-the-wild datasets with different viewpoints of subjects, we investigate the
role of cross-contamination in this effect. We additionally make use of
acceleration measured through wearable sensors for the same task, and present a
multimodal approach combining both methods.",,
Reducing Two-Way Ranging Variance by Signal-Timing Optimization,"[arxiv.Result.Author('Mohammed Ayman Shalaby'), arxiv.Result.Author('Charles Champagne Cossette'), arxiv.Result.Author('James Richard Forbes'), arxiv.Result.Author('Jerome Le Ny')]",2022-11-01 15:43:05+00:00,"Time-of-flight-based range measurements among transceivers with different
clocks requires ranging protocols that accommodate for the varying rates of the
clocks. Double-sided two-way ranging (DS-TWR) has recently been widely adopted
as a standard protocol due to its accuracy; however, the precision of DS-TWR
has not been clearly addressed. In this paper, an analytical model of the
variance of DS-TWR is derived as a function of the user-programmed response
delays. Consequently, this allows formulating an optimization problem over the
response delays in order to maximize the information gained from range
measurements by addressing the effect of varying the response delays on the
precision and frequency of the measurements. The derived analytical variance
model and proposed optimization formulation are validated experimentally with 2
ranging UWB transceivers, where 29 million range measurements are collected.","5 pages, 4 figures, submitted to 2023 International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)",
Avoid Overthinking in Self-Supervised Models for Speech Recognition,"[arxiv.Result.Author('Dan Berrebbi'), arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Shinji Watanabe')]",2022-11-01 15:26:46+00:00,"Self-supervised learning (SSL) models reshaped our approach to speech,
language and vision. However their huge size and the opaque relations between
their layers and tasks result in slow inference and network overthinking, where
predictions made from the last layer of large models is worse than those made
from intermediate layers. Early exit (EE) strategies can solve both issues by
dynamically reducing computations at inference time for certain samples.
Although popular for classification tasks in vision and language, EE has seen
less use for sequence-to-sequence speech recognition (ASR) tasks where outputs
from early layers are often degenerate. This challenge is further compounded
when speech SSL models are applied on out-of-distribution (OOD) data. This
paper first shows that SSL models do overthinking in ASR. We then motivate
further research in EE by computing an optimal bound for performance versus
speed trade-offs. To approach this bound we propose two new strategies for ASR:
(1) we adapt the recently proposed patience strategy to ASR; and (2) we design
a new EE strategy specific to ASR that performs better than all strategies
previously introduced.",,
Learning utterance-level representations through token-level acoustic latents prediction for Expressive Speech Synthesis,"[arxiv.Result.Author('Karolos Nikitaras'), arxiv.Result.Author('Konstantinos Klapsas'), arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('Georgia Maniati'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Inchul Hwang'), arxiv.Result.Author('Spyros Raptis'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis')]",2022-11-01 15:17:25+00:00,"This paper proposes an Expressive Speech Synthesis model that utilizes
token-level latent prosodic variables in order to capture and control
utterance-level attributes, such as character acting voice and speaking style.
Current works aim to explicitly factorize such fine-grained and utterance-level
speech attributes into different representations extracted by modules that
operate in the corresponding level. We show that the fine-grained latent space
also captures coarse-grained information, which is more evident as the
dimension of latent space increases in order to capture diverse prosodic
representations. Therefore, a trade-off arises between the diversity of the
token-level and utterance-level representations and their disentanglement. We
alleviate this issue by first capturing rich speech attributes into a
token-level latent space and then, separately train a prior network that given
the input text, learns utterance-level representations in order to predict the
phoneme-level, posterior latents extracted during the previous step. Both
qualitative and quantitative evaluations are used to demonstrate the
effectiveness of the proposed approach. Audio samples are available in our demo
page.",Submitted to ICASSP 2023,
TrimTail: Low-Latency Streaming ASR with Simple but Effective Spectrogram-Level Length Penalty,"[arxiv.Result.Author('Xingchen Song'), arxiv.Result.Author('Di Wu'), arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Binbin Zhang'), arxiv.Result.Author('Yuekai Zhang'), arxiv.Result.Author('Zhendong Peng'), arxiv.Result.Author('Wenpeng Li'), arxiv.Result.Author('Fuping Pan'), arxiv.Result.Author('Changbao Zhu')]",2022-11-01 15:12:34+00:00,"In this paper, we present TrimTail, a simple but effective emission
regularization method to improve the latency of streaming ASR models. The core
idea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,
see Fig. 1-(b)) directly on the spectrogram of input utterances, which does not
require any alignment. We demonstrate that TrimTail is computationally cheap
and can be applied online and optimized with any training loss or any model
architecture on any dataset without any extra effort by applying it on various
end-to-end streaming ASR networks either trained with CTC loss [1] or
Transducer loss [2]. We achieve 100 $\sim$ 200ms latency reduction with equal
or even better accuracy on both Aishell-1 and Librispeech. Moreover, by using
TrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive
Delay (USD) with an accuracy loss of less than 0.2.",submitted to ICASSP 2023,
A Comparative Study on multichannel Speaker-attributed automatic speech recognition in Multi-party Meetings,"[arxiv.Result.Author('Mohan Shi'), arxiv.Result.Author('Jie Zhang'), arxiv.Result.Author('Zhihao Du'), arxiv.Result.Author('Fan Yu'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Li-Rong Dai')]",2022-11-01 14:58:27+00:00,"Speaker-attributed automatic speech recognition (SA-ASR) in multiparty
meeting scenarios is one of the most valuable and challenging ASR task. It was
shown that single-channel frame-level diarization with serialized output
training (SC-FD-SOT), single-channel word-level diarization with SOT
(SC-WD-SOT) and joint training of single-channel target-speaker separation and
ASR (SC-TS-ASR) can be exploited to partially solve this problem. SC-FD-SOT
obtains the speaker-attributed transcriptions by aligning the speaker
diarization results with the ASR hypotheses, SC-WD-SOT uses word-level
diarization to get rid of the alignment dependence on timestamps, and SC-TS-ASR
jointly trains target-speaker separation and ASR modules, which achieves the
best performance. In this paper, we propose three corresponding multichannel
(MC) SA-ASR approaches, namely MC-FD-SOT, MC-WD-SOT and MC-TS-ASR. For
different tasks/models, different multichannel data fusion strategies are
considered, including channel-level cross-channel attention for MC-FD-SOT,
frame-level cross-channel attention for MC-WD-SOT and neural beamforming for
MC-TS-ASR. Experimental results on the AliMeeting corpus reveal that our
proposed multichannel SA-ASR models can consistently outperform the
corresponding single-channel counterparts in terms of the speaker-dependent
character error rate (SD-CER).",,
Modelling black-box audio effects with time-varying feature modulation,"[arxiv.Result.Author('Marco Comunità'), arxiv.Result.Author('Christian J. Steinmetz'), arxiv.Result.Author('Huy Phan'), arxiv.Result.Author('Joshua D. Reiss')]",2022-11-01 14:41:57+00:00,"Deep learning approaches for black-box modelling of audio effects have shown
promise, however, the majority of existing work focuses on nonlinear effects
with behaviour on relatively short time-scales, such as guitar amplifiers and
distortion. While recurrent and convolutional architectures can theoretically
be extended to capture behaviour at longer time scales, we show that simply
scaling the width, depth, or dilation factor of existing architectures does not
result in satisfactory performance when modelling audio effects such as fuzz
and dynamic range compression. To address this, we propose the integration of
time-varying feature-wise linear modulation into existing temporal
convolutional backbones, an approach that enables learnable adaptation of the
intermediate activations. We demonstrate that our approach more accurately
captures long-range dependencies for a range of fuzz and compressor
implementations across both time and frequency domain metrics. We provide sound
examples, source code, and pretrained models to faciliate reproducibility.",,
Adapting self-supervised models to multi-talker speech recognition using speaker embeddings,"[arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Desh Raj'), arxiv.Result.Author('Paola García'), arxiv.Result.Author('Sanjeev Khudanpur')]",2022-11-01 14:16:16+00:00,"Self-supervised learning (SSL) methods which learn representations of data
without explicit supervision have gained popularity in speech-processing tasks,
particularly for single-talker applications. However, these models often have
degraded performance for multi-talker scenarios -- possibly due to the domain
mismatch -- which severely limits their use for such applications. In this
paper, we investigate the adaptation of upstream SSL models to the multi-talker
automatic speech recognition (ASR) task under two conditions. First, when
segmented utterances are given, we show that adding a target speaker extraction
(TSE) module based on enrollment embeddings is complementary to mixture-aware
pre-training. Second, for unsegmented mixtures, we propose a novel joint
speaker modeling (JSM) approach, which aggregates information from all speakers
in the mixture through their embeddings. With controlled experiments on
Libri2Mix, we show that using speaker embeddings provides relative WER
improvements of 9.1% and 42.1% over strong baselines for the segmented and
unsegmented cases, respectively. We also demonstrate the effectiveness of our
models for real conversational mixtures through experiments on the AMI dataset.",submitted to ICASSP 2023,
Metric Learning for User-defined Keyword Spotting,"[arxiv.Result.Author('Jaemin Jung'), arxiv.Result.Author('Youkyum Kim'), arxiv.Result.Author('Jihwan Park'), arxiv.Result.Author('Youshin Lim'), arxiv.Result.Author('Byeong-Yeol Kim'), arxiv.Result.Author('Youngjoon Jang'), arxiv.Result.Author('Joon Son Chung')]",2022-11-01 13:08:55+00:00,"The goal of this work is to detect new spoken terms defined by users. While
most previous works address Keyword Spotting (KWS) as a closed-set
classification problem, this limits their transferability to unseen terms. The
ability to define custom keywords has advantages in terms of user experience.
  In this paper, we propose a metric learning-based training strategy for
user-defined keyword spotting. In particular, we make the following
contributions: (1) we construct a large-scale keyword dataset with an existing
speech corpus and propose a filtering method to remove data that degrade model
training; (2) we propose a metric learning-based two-stage training strategy,
and demonstrate that the proposed method improves the performance on the
user-defined keyword spotting task by enriching their representations; (3) to
facilitate the fair comparison in the user-defined KWS field, we propose
unified evaluation protocol and metrics.
  Our proposed system does not require an incremental training on the
user-defined keywords, and outperforms previous works by a significant margin
on the Google Speech Commands dataset using the proposed as well as the
existing metrics.",,
Disentangled representation learning for multilingual speaker recognition,"[arxiv.Result.Author('Kihyun Nam'), arxiv.Result.Author('Youkyum Kim'), arxiv.Result.Author('Hee Soo Heo'), arxiv.Result.Author('Jee-weon Jung'), arxiv.Result.Author('Joon Son Chung')]",2022-11-01 13:00:07+00:00,"The goal of this paper is to train speaker embeddings that are robust to
bilingual speaking scenario. The majority of the world's population speak at
least two languages; however, most speaker recognition systems fail to
recognise the same speaker when speaking in different languages.
  Popular speaker recognition evaluation sets do not consider the bilingual
scenario, making it difficult to analyse the effect of bilingual speakers on
speaker recognition performance. This paper proposes a new large-scale
evaluation set derived from VoxCeleb that considers bilingual scenarios. We
also introduce a representation learning strategy, which disentangles language
information from speaker representation to account for the bilingual scenario.
This language-disentangled representation learning strategy can be adapted to
existing models with small changes to the training pipeline.
  Experimental results demonstrate that the baseline models suffer significant
performance degradation when evaluated on the proposed bilingual test set. On
the contrary, the model trained with the proposed disentanglement strategy
shows significant improvement under the bilingual evaluation scenario while
simultaneously retaining competitive performance on existing monolingual test
sets.",,
Generating Gender-Ambiguous Text-to-Speech Voices,"[arxiv.Result.Author('Konstantinos Markopoulos'), arxiv.Result.Author('Georgia Maniati'), arxiv.Result.Author('Georgios Vamvoukakis'), arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('Karolos Nikitaras'), arxiv.Result.Author('Konstantinos Klapsas'), arxiv.Result.Author('Georgios Vardaxoglou'), arxiv.Result.Author('Panos Kakoulidis'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Inchul Hwang'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis'), arxiv.Result.Author('Spyros Raptis')]",2022-11-01 10:40:24+00:00,"The gender of a voice assistant or any voice user interface is a central
element of its perceived identity. While a female voice is a common choice,
there is an increasing interest in alternative approaches where the gender is
ambiguous rather than clearly identifying as female or male. This work
addresses the task of generating gender-ambiguous text-to-speech (TTS) voices
that do not correspond to any existing person. This is accomplished by sampling
from a latent speaker embeddings' space that was formed while training a
multilingual, multi-speaker TTS system on data from multiple male and female
speakers. Various options are investigated regarding the sampling process. In
our experiments, the effects of different sampling choices on the gender
ambiguity and the naturalness of the resulting voices are evaluated. The
proposed method is shown able to efficiently generate novel speakers that are
superior to a baseline averaged speaker embedding. To our knowledge, this is
the first systematic approach that can reliably generate a range of
gender-ambiguous voices to meet diverse user requirements.",Submitted to ICASSP 2023,
Understanding Acoustic Patterns of Human Teachers Demonstrating Manipulation Tasks to Robots,"[arxiv.Result.Author('Akanksha Saran'), arxiv.Result.Author('Kush Desai'), arxiv.Result.Author('Mai Lee Chang'), arxiv.Result.Author('Rudolf Lioutikov'), arxiv.Result.Author('Andrea Thomaz'), arxiv.Result.Author('Scott Niekum')]",2022-11-01 09:52:48+00:00,"Humans use audio signals in the form of spoken language or verbal reactions
effectively when teaching new skills or tasks to other humans. While
demonstrations allow humans to teach robots in a natural way, learning from
trajectories alone does not leverage other available modalities including audio
from human teachers. To effectively utilize audio cues accompanying human
demonstrations, first it is important to understand what kind of information is
present and conveyed by such cues. This work characterizes audio from human
teachers demonstrating multi-step manipulation tasks to a situated Sawyer robot
using three feature types: (1) duration of speech used, (2) expressiveness in
speech or prosody, and (3) semantic content of speech. We analyze these
features along four dimensions and find that teachers convey similar semantic
concepts via spoken words for different conditions of (1) demonstration types,
(2) audio usage instructions, (3) subtasks, and (4) errors during
demonstrations. However, differentiating properties of speech in terms of
duration and expressiveness are present along the four dimensions, highlighting
that human audio carries rich information, potentially beneficial for
technological advancement of robot learning from demonstration methods.",IROS 2022,
Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features,"[arxiv.Result.Author('Alexandra Vioni'), arxiv.Result.Author('Georgia Maniati'), arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Inchul Hwang'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis')]",2022-11-01 09:18:50+00:00,"Current state-of-the-art methods for automatic synthetic speech evaluation
are based on MOS prediction neural models. Such MOS prediction models include
MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies
on a pretrained self-supervised learning model that directly uses the speech
signal as input. In modern high-quality neural TTS systems, prosodic
appropriateness with regard to the spoken content is a decisive factor for
speech naturalness. For this reason, we propose to include prosodic and
linguistic features as additional inputs in MOS prediction systems, and
evaluate their impact on the prediction outcome. We consider phoneme level F0
and duration features as prosodic inputs, as well as Tacotron encoder outputs,
POS tags and BERT embeddings as higher-level linguistic inputs. All MOS
prediction systems are trained on SOMOS, a neural TTS-only dataset with
crowdsourced naturalness MOS evaluations. Results show that the proposed
additional features are beneficial in the MOS prediction task, by improving the
predicted MOS scores' correlation with the ground truths, both at
utterance-level and system-level predictions.",Submitted to ICASSP 2023,
Speech-text based multi-modal training with bidirectional attention for improved speech recognition,"[arxiv.Result.Author('Yuhang Yang'), arxiv.Result.Author('Haihua Xu'), arxiv.Result.Author('Hao Huang'), arxiv.Result.Author('Eng Siong Chng'), arxiv.Result.Author('Sheng Li')]",2022-11-01 08:25:11+00:00,"To let the state-of-the-art end-to-end ASR model enjoy data efficiency, as
well as much more unpaired text data by multi-modal training, one needs to
address two problems: 1) the synchronicity of feature sampling rates between
speech and language (aka text data); 2) the homogeneity of the learned
representations from two encoders. In this paper we propose to employ a novel
bidirectional attention mechanism (BiAM) to jointly learn both ASR encoder
(bottom layers) and text encoder with a multi-modal learning method. The BiAM
is to facilitate feature sampling rate exchange, realizing the quality of the
transformed features for the one kind to be measured in another space, with
diversified objective functions. As a result, the speech representations are
enriched with more linguistic information, while the representations generated
by the text encoder are more similar to corresponding speech ones, and
therefore the shared ASR models are more amenable for unpaired text data
pretraining. To validate the efficacy of the proposed method, we perform two
categories of experiments with or without extra unpaired text data.
Experimental results on Librispeech corpus show it can achieve up to 6.15% word
error rate reduction (WERR) with only paired data learning, while 9.23% WERR
when more unpaired text data is employed.","5 pages, 3 figures, 3 tables",
Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages,"[arxiv.Result.Author('Anusha Prakash'), arxiv.Result.Author('Arun Kumar'), arxiv.Result.Author('Ashish Seth'), arxiv.Result.Author('Bhagyashree Mukherjee'), arxiv.Result.Author('Ishika Gupta'), arxiv.Result.Author('Jom Kuriakose'), arxiv.Result.Author('Jordan Fernandes'), arxiv.Result.Author('K V Vikram'), arxiv.Result.Author('Mano Ranjith Kumar M'), arxiv.Result.Author('Metilda Sagaya Mary'), arxiv.Result.Author('Mohammad Wajahat'), arxiv.Result.Author('Mohana N'), arxiv.Result.Author('Mudit Batra'), arxiv.Result.Author('Navina K'), arxiv.Result.Author('Nihal John George'), arxiv.Result.Author('Nithya Ravi'), arxiv.Result.Author('Pruthwik Mishra'), arxiv.Result.Author('Sudhanshu Srivastava'), arxiv.Result.Author('Vasista Sai Lodagala'), arxiv.Result.Author('Vandan Mujadia'), arxiv.Result.Author('Kada Sai Venkata Vineeth'), arxiv.Result.Author('Vrunda Sukhadia'), arxiv.Result.Author('Dipti Sharma'), arxiv.Result.Author('Hema Murthy'), arxiv.Result.Author('Pushpak Bhattacharya'), arxiv.Result.Author('S Umesh'), arxiv.Result.Author('Rajeev Sangal')]",2022-11-01 07:06:29+00:00,"Cross-lingual dubbing of lecture videos requires the transcription of the
original audio, correction and removal of disfluencies, domain term discovery,
text-to-text translation into the target language, chunking of text using
target language rhythm, text-to-speech synthesis followed by isochronous
lipsyncing to the original video. This task becomes challenging when the source
and target languages belong to different language families, resulting in
differences in generated audio duration. This is further compounded by the
original speaker's rhythm, especially for extempore speech. This paper
describes the challenges in regenerating English lecture videos in Indian
languages semi-automatically. A prototype is developed for dubbing lectures
into 9 Indian languages. A mean-opinion-score (MOS) is obtained for two
languages, Hindi and Tamil, on two different courses. The output video is
compared with the original video in terms of MOS (1-5) and lip synchronisation
with scores of 4.09 and 3.74, respectively. The human effort also reduces by
75%.",,
Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection,"[arxiv.Result.Author('Jiyun Kim'), arxiv.Result.Author('Byounghan Lee'), arxiv.Result.Author('Kyung-Ah Sohn')]",2022-11-01 03:16:36+00:00,"In a hate speech detection model, we should consider two critical aspects in
addition to detection performance-bias and explainability. Hate speech cannot
be identified based solely on the presence of specific words: the model should
be able to reason like humans and be explainable. To improve the performance
concerning the two aspects, we propose Masked Rationale Prediction (MRP) as an
intermediate task. MRP is a task to predict the masked human
rationales-snippets of a sentence that are grounds for human judgment-by
referring to surrounding tokens combined with their unmasked rationales. As the
model learns its reasoning ability based on rationales by MRP, it performs hate
speech detection robustly in terms of bias and explainability. The proposed
method generally achieves state-of-the-art performance in various metrics,
demonstrating its effectiveness for hate speech detection.","10 pages, 4 figures, 3 tables. Accepted at COLING 2022",
Waveform Boundary Detection for Partially Spoofed Audio,"[arxiv.Result.Author('Zexin Cai'), arxiv.Result.Author('Weiqing Wang'), arxiv.Result.Author('Ming Li')]",2022-11-01 02:31:54+00:00,"The present paper proposes a waveform boundary detection system for audio
spoofing attacks containing partially manipulated segments. Partially
spoofed/fake audio, where part of the utterance is replaced, either with
synthetic or natural audio clips, has recently been reported as one scenario of
audio deepfakes. As deepfakes can be a threat to social security, the detection
of such spoofing audio is essential. Accordingly, we propose to address the
problem with a deep learning-based frame-level detection system that can detect
partially spoofed audio and locate the manipulated pieces. Our proposed method
is trained and evaluated on data provided by the ADD2022 Challenge. We evaluate
our detection model concerning various acoustic features and network
configurations. As a result, our detection system achieves an equal error rate
(EER) of 6.58% on the ADD2022 challenge test set, which is the best performance
in partially spoofed audio detection systems that can locate manipulated clips.",,
SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation,"[arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Yi Ren'), arxiv.Result.Author('Kejun Zhang'), arxiv.Result.Author('Shuicheng Yan')]",2022-11-01 02:10:25+00:00,"While deep generative models have empowered music generation, it remains a
challenging and under-explored problem to edit an existing musical piece at
fine granularity. In this paper, we propose SDMuse, a unified Stochastic
Differential Music editing and generation framework, which can not only compose
a whole musical piece from scratch, but also modify existing musical pieces in
many ways, such as combination, continuation, inpainting, and style
transferring. The proposed SDMuse follows a two-stage pipeline to achieve music
generation and editing on top of a hybrid representation including pianoroll
and MIDI-event. In particular, SDMuse first generates/edits pianoroll by
iteratively denoising through a stochastic differential equation (SDE) based on
a diffusion model generative prior, and then refines the generated pianoroll
and predicts MIDI-event tokens auto-regressively. We evaluate the generated
music of our method on ailabs1k7 pop music dataset in terms of quality and
controllability on various music editing and generation tasks. Experimental
results demonstrate the effectiveness of our proposed stochastic differential
music editing and generation process, as well as the hybrid representations.",,
Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition,"[arxiv.Result.Author('Suyoun Kim'), arxiv.Result.Author('Ke Li'), arxiv.Result.Author('Lucas Kabela'), arxiv.Result.Author('Rongqing Huang'), arxiv.Result.Author('Jiedan Zhu'), arxiv.Result.Author('Ozlem Kalinli'), arxiv.Result.Author('Duc Le')]",2022-10-31 22:38:28+00:00,"Recently, there has been an increasing interest in two-pass streaming
end-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring
model on top of the conventional 1st-pass streaming ASR model to improve
recognition accuracy while keeping latency low. One of the latest 2nd-pass
rescoring model, Transformer Rescorer, takes the n-best initial outputs and
audio embeddings from the 1st-pass model, and then choose the best output by
re-scoring the n-best initial outputs. However, training this Transformer
Rescorer requires expensive paired audio-text training data because the model
uses audio embeddings as input. In this work, we present our Joint Audio/Text
training method for Transformer Rescorer, to leverage unpaired text-only data
which is relatively cheaper than paired audio-text data. We evaluate
Transformer Rescorer with our Joint Audio/Text training on Librispeech dataset
as well as our large-scale in-house dataset and show that our training method
can improve word error rate (WER) significantly compared to standard
Transformer Rescorer without requiring any extra model parameters or latency.",,Findings of EMNLP 2022 short
Design Considerations For Hypothesis Rejection Modules In Spoken Language Understanding Systems,"[arxiv.Result.Author('Aman Alok'), arxiv.Result.Author('Rahul Gupta'), arxiv.Result.Author('Shankar Ananthakrishnan')]",2022-10-31 21:16:23+00:00,"Spoken Language Understanding (SLU) systems typically consist of a set of
machine learning models that operate in conjunction to produce an SLU
hypothesis. The generated hypothesis is then sent to downstream components for
further action. However, it is desirable to discard an incorrect hypothesis
before sending it downstream. In this work, we present two designs for SLU
hypothesis rejection modules: (i) scheme R1 that performs rejection on domain
specific SLU hypothesis and, (ii) scheme R2 that performs rejection on
hypothesis generated from the overall SLU system. Hypothesis rejection modules
in both schemes reject/accept a hypothesis based on features drawn from the
utterance directed to the SLU system, the associated SLU hypothesis and SLU
confidence score. Our experiments suggest that both the schemes yield similar
results (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with
the best performing systems using all the available features. We argue that
while either of the rejection schemes can be chosen over the other, they carry
some inherent differences which need to be considered while making this choice.
Additionally, we incorporate ASR features in the rejection module (obtaining an
1.9% FRR @ 3.8% FAR) and analyze the improvements.","5 pages. ICASSP 2020 - 2020 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)",
Active Learning of Non-semantic Speech Tasks with Pretrained Models,"[arxiv.Result.Author('Harlin Lee'), arxiv.Result.Author('Aaqib Saeed'), arxiv.Result.Author('Andrea L. Bertozzi')]",2022-10-31 20:08:55+00:00,"Pretraining neural networks with massive unlabeled datasets has become
popular as it equips the deep models with a better prior to solve downstream
tasks. However, this approach generally assumes that for downstream tasks, we
have access to annotated data of sufficient size. In this work, we propose
ALOE, a novel system for improving the data- and label-efficiency of
non-semantic speech tasks with active learning (AL). ALOE uses pre-trained
models in conjunction with active learning to label data incrementally and
learns classifiers for downstream tasks, thereby mitigating the need to acquire
labeled data beforehand. We demonstrate the effectiveness of ALOE on a wide
range of tasks, uncertainty-based acquisition functions, and model
architectures. Training a linear classifier on top of a frozen encoder with
ALOE is shown to achieve performance similar to several baselines that utilize
the entire labeled data.",,
Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search,"[arxiv.Result.Author('Zihan Wang'), arxiv.Result.Author('Qi Meng'), arxiv.Result.Author('HaiFeng Lan'), arxiv.Result.Author('XinRui Zhang'), arxiv.Result.Author('KeHao Guo'), arxiv.Result.Author('Akshat Gupta')]",2022-10-31 19:55:33+00:00,"Speech emotion recognition (SER) classifies audio into emotion categories
such as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion
Recognition (SER) is a common application for popular languages, it continues
to be a problem for low-resourced languages, i.e., languages with no pretrained
speech-to-text recognition models. This paper firstly proposes a
language-specific model that extract emotional information from multiple
pre-trained speech models, and then designs a multi-domain model that
simultaneously performs SER for various languages. Our multidomain model
employs a multi-gating mechanism to generate unique weighted feature
combination for each language, and also searches for specific neural network
structure for each language through a neural architecture search module. In
addition, we introduce a contrastive auxiliary loss to build more separable
representations for audio data. Our experiments show that our model raises the
state-of-the-art accuracy by 3% for German and 14.3% for French.",,
Textless Direct Speech-to-Speech Translation with Discrete Speech Representation,"[arxiv.Result.Author('Xinjian Li'), arxiv.Result.Author('Ye Jia'), arxiv.Result.Author('Chung-Cheng Chiu')]",2022-10-31 19:48:38+00:00,"Research on speech-to-speech translation (S2ST) has progressed rapidly in
recent years. Many end-to-end systems have been proposed and show advantages
over conventional cascade systems, which are often composed of recognition,
translation and synthesis sub-systems. However, most of the end-to-end systems
still rely on intermediate textual supervision during training, which makes it
infeasible to work for languages without written forms. In this work, we
propose a novel model, Textless Translatotron, which is based on Translatotron
2, for training an end-to-end direct S2ST model without any textual
supervision. Instead of jointly training with an auxiliary task predicting
target phonemes as in Translatotron 2, the proposed model uses an auxiliary
task predicting discrete speech representations which are obtained from learned
or random speech quantizers. When a speech encoder pre-trained with
unsupervised speech data is used for both models, the proposed model obtains
translation quality nearly on-par with Translatotron 2 on the multilingual
CVSS-C corpus as well as the bilingual Fisher Spanish-English corpus. On the
latter, it outperforms the prior state-of-the-art textless model by +18.5 BLEU.",,
ImagineNET: Target Speaker Extraction with Intermittent Visual Cue through Embedding Inpainting,"[arxiv.Result.Author('Zexu Pan'), arxiv.Result.Author('Wupeng Wang'), arxiv.Result.Author('Marvin Borsdorf'), arxiv.Result.Author('Haizhou Li')]",2022-10-31 19:29:29+00:00,"The speaker extraction technique seeks to single out the voice of a target
speaker from the interfering voices in a speech mixture. Typically an auxiliary
reference of the target speaker is used to form voluntary attention. Either a
pre-recorded utterance or a synchronized lip movement in a video clip can serve
as the auxiliary reference. The use of visual cue is not only feasible, but
also effective due to its noise robustness, and becoming popular. However, it
is difficult to guarantee that such parallel visual cue is always available in
real-world applications where visual occlusion or intermittent communication
can occur. In this paper, we study the audio-visual speaker extraction
algorithms with intermittent visual cue. We propose a joint speaker extraction
and visual embedding inpainting framework to explore the mutual benefits. To
encourage the interaction between the two tasks, they are performed alternately
with an interlacing structure and optimized jointly. We also propose two types
of visual inpainting losses and study our proposed method with two types of
popularly used visual embeddings. The experimental results show that we
outperform the baseline in terms of signal quality, perceptual quality, and
intelligibility.",,
An analysis of degenerating speech due to progressive dysarthria on ASR performance,"[arxiv.Result.Author('Katrin Tomanek'), arxiv.Result.Author('Katie Seaver'), arxiv.Result.Author('Pan-Pan Jiang'), arxiv.Result.Author('Richard Cave'), arxiv.Result.Author('Lauren Harrel'), arxiv.Result.Author('Jordan R. Green')]",2022-10-31 18:42:41+00:00,"Although personalized automatic speech recognition (ASR) models have recently
been designed to recognize even severely impaired speech, model performance may
degrade over time for persons with degenerating speech. The aims of this study
were to (1) analyze the change of performance of ASR over time in individuals
with degrading speech, and (2) explore mitigation strategies to optimize
recognition throughout disease progression. Speech was recorded by four
individuals with degrading speech due to amyotrophic lateral sclerosis (ALS).
Word error rates (WER) across recording sessions were computed for three ASR
models: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent
(A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance
of all three models degraded significantly over time as speech became more
impaired, but the performance of the A-SD model improved markedly when it was
updated with recordings from the severe stages of speech progression. Recording
additional utterances early in the disease before speech degraded significantly
did not improve the performance of A-SD models. Overall, our findings emphasize
the importance of continuous recording (and model retraining) when providing
personalized models for individuals with progressive speech impairments.",Submitted to ICASSP 2023,
Domain Curricula for Code-Switched MT at MixMT 2022,"[arxiv.Result.Author('Lekan Raheem'), arxiv.Result.Author('Maab Elrashid')]",2022-10-31 16:41:57+00:00,"In multilingual colloquial settings, it is a habitual occurrence to compose
expressions of text or speech containing tokens or phrases of different
languages, a phenomenon popularly known as code-switching or code-mixing (CMX).
We present our approach and results for the Code-mixed Machine Translation
(MixMT) shared task at WMT 2022: the task consists of two subtasks, monolingual
to code-mixed machine translation (Subtask-1) and code-mixed to monolingual
machine translation (Subtask-2). Most non-synthetic code-mixed data are from
social media but gathering a significant amount of this kind of data would be
laborious and this form of data has more writing variation than other domains,
so for both subtasks, we experimented with data schedules for out-of-domain
data. We jointly learn multiple domains of text by pretraining and fine-tuning,
combined with a sentence alignment objective. We found that switching between
domains caused improved performance in the domains seen earliest during
training, but depleted the performance on the remaining domains. A continuous
training run with strategically dispensed data of different domains showed a
significantly improved performance over fine-tuning.","The paper composed of 6 pages, it contains 3 figures and 4 tables, it
  has been accepted at the EMNLP 2022, SEVENTH CONFERENCE ON MACHINE
  TRANSLATION (WMT22)",
Audio-Visual Speech Enhancement and Separation by Leveraging Multi-Modal Self-Supervised Embeddings,"[arxiv.Result.Author('I-Chun Chern'), arxiv.Result.Author('Kuo-Hsuan Hung'), arxiv.Result.Author('Yi-Ting Chen'), arxiv.Result.Author('Tassadaq Hussain'), arxiv.Result.Author('Mandar Gogate'), arxiv.Result.Author('Amir Hussain'), arxiv.Result.Author('Yu Tsao'), arxiv.Result.Author('Jen-Cheng Hou')]",2022-10-31 16:30:10+00:00,"AV-HuBERT, a multi-modal self-supervised learning model, has been shown to be
effective for categorical problems such as automatic speech recognition and
lip-reading. This suggests that useful audio-visual speech representations can
be obtained via utilizing multi-modal self-supervised embeddings. Nevertheless,
it is unclear if such representations can be generalized to solve real-world
multi-modal AV regression tasks, such as audio-visual speech enhancement (AVSE)
and audio-visual speech separation (AVSS). In this study, we leveraged the
pre-trained AV-HuBERT model followed by an SE module for AVSE and AVSS.
Comparative experimental results demonstrate that our proposed model performs
better than the state-of-the-art AVSE and traditional audio-only SE models. In
summary, our results confirm the effectiveness of our proposed model for the
AVSS task with proper fine-tuning strategies, demonstrating that multi-modal
self-supervised embeddings obtained from AV-HUBERT can be generalized to
audio-visual regression tasks.",Under peer review,
Analysis and Detection of Singing Techniques in Repertoires of J-POP Solo Singers,"[arxiv.Result.Author('Yuya Yamamoto'), arxiv.Result.Author('Juhan Nam'), arxiv.Result.Author('Hiroko Terasawa')]",2022-10-31 14:45:01+00:00,"In this paper, we focus on singing techniques within the scope of music
information retrieval research. We investigate how singers use singing
techniques using real-world recordings of famous solo singers in Japanese
popular music songs (J-POP). First, we built a new dataset of singing
techniques. The dataset consists of 168 commercial J-POP songs, and each song
is annotated using various singing techniques with timestamps and vocal pitch
contours. We also present descriptive statistics of singing techniques on the
dataset to clarify what and how often singing techniques appear. We further
explored the difficulty of the automatic detection of singing techniques using
previously proposed machine learning techniques. In the detection, we also
investigate the effectiveness of auxiliary information (i.e., pitch and
distribution of label duration), not only providing the baseline. The best
result achieves 40.4% at macro-average F-measure on nine-way multi-class
detection. We provide the annotation of the dataset and its detail on the
appendix website 0 .","Accepted at ISMIR 2022, appendix website:
  https://yamathcy.github.io/ISMIR2022J-POP/",
Robust MelGAN: A robust universal neural vocoder for high-fidelity TTS,"[arxiv.Result.Author('Kun Song'), arxiv.Result.Author('Jian Cong'), arxiv.Result.Author('Xinsheng Wang'), arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Ning Jiang'), arxiv.Result.Author('Haiying Wu')]",2022-10-31 14:24:10+00:00,"In current two-stage neural text-to-speech (TTS) paradigm, it is ideal to
have a universal neural vocoder, once trained, which is robust to imperfect
mel-spectrogram predicted from the acoustic model. To this end, we propose
Robust MelGAN vocoder by solving the original multi-band MelGAN's metallic
sound problem and increasing its generalization ability. Specifically, we
introduce a fine-grained network dropout strategy to the generator. With a
specifically designed over-smooth handler which separates speech signal intro
periodic and aperiodic components, we only perform network dropout to the
aperodic components, which alleviates metallic sounding and maintains good
speaker similarity. To further improve generalization ability, we introduce
several data augmentation methods to augment fake data in the discriminator,
including harmonic shift, harmonic noise and phase noise. Experiments show that
Robust MelGAN can be used as a universal vocoder, significantly improving sound
quality in TTS systems built on various types of data.",Accepted by ISCSLP 2022,
From Nano to Macro: Overview of the IEEE Bio Image and Signal Processing Technical Committee,"[arxiv.Result.Author('Selin Aviyente'), arxiv.Result.Author('Alejandro Frangi'), arxiv.Result.Author('Erik Meijering'), arxiv.Result.Author('Arrate Muñoz-Barrutia'), arxiv.Result.Author('Michael Liebling'), arxiv.Result.Author('Dimitri Van De Ville'), arxiv.Result.Author('Jean-Christophe Olivo-Marin'), arxiv.Result.Author('Jelena Kovačević'), arxiv.Result.Author('Michael Unser')]",2022-10-31 14:12:11+00:00,"The Bio Image and Signal Processing (BISP) Technical Committee (TC) of the
IEEE Signal Processing Society (SPS) promotes activities within the broad
technical field of biomedical image and signal processing. Areas of interest
include medical and biological imaging, digital pathology, molecular imaging,
microscopy, and associated computational imaging, image analysis, and
image-guided treatment, alongside physiological signal processing,
computational biology, and bioinformatics. BISP has 40 members and covers a
wide range of EDICS, including CIS-MI: Medical Imaging, BIO-MIA: Medical Image
Analysis, BIO-BI: Biological Imaging, BIO: Biomedical Signal Processing,
BIO-BCI: Brain/Human-Computer Interfaces, and BIO-INFR: Bioinformatics. BISP
plays a central role in the organization of the IEEE International Symposium on
Biomedical Imaging (ISBI) and contributes to the technical sessions at the IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP),
and the IEEE International Conference on Image Processing (ICIP). In this
paper, we provide a brief history of the TC, review the technological and
methodological contributions its community delivered, and highlight promising
new directions we anticipate.",,
VoicePrivacy 2022 System Description: Speaker Anonymization with Feature-matched F0 Trajectories,"[arxiv.Result.Author('Ünal Ege Gaznepoglu'), arxiv.Result.Author('Anna Leschanowsky'), arxiv.Result.Author('Nils Peters')]",2022-10-31 13:59:27+00:00,"We introduce a novel method to improve the performance of the VoicePrivacy
Challenge 2022 baseline B1 variants. Among the known deficiencies of
x-vector-based anonymization systems is the insufficient disentangling of the
input features. In particular, the fundamental frequency (F0) trajectories,
which are used for voice synthesis without any modifications. Especially in
cross-gender conversion, this situation causes unnatural sounding voices,
increases word error rates (WERs), and personal information leakage. Our
submission overcomes this problem by synthesizing an F0 trajectory, which
better harmonizes with the anonymized x-vector. We utilized a low-complexity
deep neural network to estimate an appropriate F0 value per frame, using the
linguistic content from the bottleneck features (BN) and the anonymized
x-vector. Our approach results in a significantly improved anonymization system
and increased naturalness of the synthesized voice. Consequently, our results
suggest that F0 extraction is not required for voice anonymization.","4 pages, 4 figures, 2 tables, submitted to VoicePrivacy Challenge
  2022",
Diffusion-based Generative Speech Source Separation,"[arxiv.Result.Author('Robin Scheibler'), arxiv.Result.Author('Youna Ji'), arxiv.Result.Author('Soo-Whan Chung'), arxiv.Result.Author('Jaeuk Byun'), arxiv.Result.Author('Soyeon Choe'), arxiv.Result.Author('Min-Seok Choi')]",2022-10-31 13:46:55+00:00,"We propose DiffSep, a new single channel source separation method based on
score-matching of a stochastic differential equation (SDE). We craft a tailored
continuous time diffusion-mixing process starting from the separated sources
and converging to a Gaussian distribution centered on their mixture. This
formulation lets us apply the machinery of score-based generative modelling.
First, we train a neural network to approximate the score function of the
marginal probabilities or the diffusion-mixing process. Then, we use it to
solve the reverse time SDE that progressively separates the sources starting
from their mixture. We propose a modified training strategy to handle model
mismatch and source permutation ambiguity. Experiments on the WSJ0 2mix dataset
demonstrate the potential of the method. Furthermore, the method is also
suitable for speech enhancement and shows performance competitive with prior
work on the VoiceBank-DEMAND dataset.","5 pages, 3 figures, 2 tables. Submitted to ICASSP 2023",
Model Compression for DNN-Based Text-Independent Speaker Verification Using Weight Quantization,"[arxiv.Result.Author('Jingyu Li'), arxiv.Result.Author('Zhaoyang Zhang'), arxiv.Result.Author('Jiong Wang'), arxiv.Result.Author('Tan Lee')]",2022-10-31 13:46:47+00:00,"DNN-based models achieve high performance in the speaker verification (SV)
task with substantial computation costs. The model size is an essential concern
in applying models on resource-constrained devices, while model compression for
SV models has not been studied extensively in previous works. Weight
quantization is exploited to compress DNN-based speaker embedding extraction
models in this paper. Uniform and Powers-of-Two quantization are utilized in
the experiments. The results on VoxCeleb show that the weight quantization can
decrease the size of ECAPA-TDNN and ResNet by 4 times with insignificant
performance decline. The quantized 4-bit ResNet achieves similar performance to
the original model with an 8 times smaller size. We empirically show that the
performance of ECAPA-TDNN is more sensitive than ResNet to quantization due to
the difference in weight distribution. The experiments on CN-Celeb also
demonstrate that quantized models are robust for SV in the language mismatch
scenario.",Some results are not correct,
Convolution-Based Channel-Frequency Attention for Text-Independent Speaker Verification,"[arxiv.Result.Author('Jingyu Li'), arxiv.Result.Author('Yusheng Tian'), arxiv.Result.Author('Tan Lee')]",2022-10-31 13:34:22+00:00,"Deep convolutional neural networks (CNNs) have been applied to extracting
speaker embeddings with significant success in speaker verification.
Incorporating the attention mechanism has shown to be effective in improving
the model performance. This paper presents an efficient two-dimensional
convolution-based attention module, namely C2D-Att. The interaction between the
convolution channel and frequency is involved in the attention calculation by
lightweight convolution layers. This requires only a small number of
parameters. Fine-grained attention weights are produced to represent channel
and frequency-specific information. The weights are imposed on the input
features to improve the representation ability for speaker modeling. The
C2D-Att is integrated into a modified version of ResNet for speaker embedding
extraction. Experiments are conducted on VoxCeleb datasets. The results show
that C2DAtt is effective in generating discriminative attention maps and
outperforms other attention methods. The proposed model shows robust
performance with different scales of model size and achieves state-of-the-art
results.",,
AccentSpeech: Learning Accent from Crowd-sourced Data for Target Speaker TTS with Accents,"[arxiv.Result.Author('Yongmao Zhang'), arxiv.Result.Author('Zhichao Wang'), arxiv.Result.Author('Peiji Yang'), arxiv.Result.Author('Hongshen Sun'), arxiv.Result.Author('Zhisheng Wang'), arxiv.Result.Author('Lei Xie')]",2022-10-31 13:31:17+00:00,"Learning accent from crowd-sourced data is a feasible way to achieve a target
speaker TTS system that can synthesize accent speech. To this end, there are
two challenging problems to be solved. First, direct use of the poor acoustic
quality crowd-sourced data and the target speaker data in accent transfer will
apparently lead to synthetic speech with degraded quality. To mitigate this
problem, we take a bottleneck feature (BN) based TTS approach, in which TTS is
decomposed into a Text-to-BN (T2BN) module to learn accent and a BN-to-Mel
(BN2Mel) module to learn speaker timbre, where neural network based BN feature
serves as the intermediate representation that are robust to noise
interference. Second, direct training T2BN using the crowd-sourced data in the
two-stage system will produce accent speech of target speaker with poor
prosody. This is because the the crowd-sourced recordings are contributed from
the ordinary unprofessional speakers. To tackle this problem, we update the
two-stage approach to a novel three-stage approach, where T2BN and BN2Mel are
trained using the high-quality target speaker data and a new BN-to-BN module is
plugged in between the two modules to perform accent transfer. To train the
BN2BN module, the parallel unaccented and accented BN features are obtained by
a proposed data augmentation procedure. Finally the proposed three-stage
approach manages to produce accent speech for the target speaker with good
prosody, as the prosody pattern is inherited from the professional target
speaker and accent transfer is achieved by the BN2BN module at the same time.
The proposed approach, named as AccentSpeech, is validated in a Mandarin TTS
accent transfer task.",Accepted by ISCSLP2022,
Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation,"[arxiv.Result.Author('Nikolaos Ellinas'), arxiv.Result.Author('Georgios Vamvoukakis'), arxiv.Result.Author('Konstantinos Markopoulos'), arxiv.Result.Author('Georgia Maniati'), arxiv.Result.Author('Panos Kakoulidis'), arxiv.Result.Author('June Sig Sung'), arxiv.Result.Author('Inchul Hwang'), arxiv.Result.Author('Spyros Raptis'), arxiv.Result.Author('Aimilios Chalamandaris'), arxiv.Result.Author('Pirros Tsiakoulis')]",2022-10-31 12:44:53+00:00,"This paper presents a method for end-to-end cross-lingual text-to-speech
(TTS) which aims to preserve the target language's pronunciation regardless of
the original speaker's language. The model used is based on a non-attentive
Tacotron architecture, where the decoder has been replaced with a normalizing
flow network conditioned on the speaker identity, allowing both TTS and voice
conversion (VC) to be performed by the same model due to the inherent
linguistic content and speaker identity disentanglement. When used in a
cross-lingual setting, acoustic features are initially produced with a native
speaker of the target language and then voice conversion is applied by the same
model in order to convert these features to the target speaker's voice. We
verify through objective and subjective evaluations that our method can have
benefits compared to baseline cross-lingual synthesis. By including speakers
averaging 7.5 minutes of speech, we also present positive results on
low-resource scenarios.",Submitted to ICASSP 2023,
Combining Automatic Speaker Verification and Prosody Analysis for Synthetic Speech Detection,"[arxiv.Result.Author('Luigi Attorresi'), arxiv.Result.Author('Davide Salvi'), arxiv.Result.Author('Clara Borrelli'), arxiv.Result.Author('Paolo Bestagini'), arxiv.Result.Author('Stefano Tubaro')]",2022-10-31 11:03:03+00:00,"The rapid spread of media content synthesis technology and the potentially
damaging impact of audio and video deepfakes on people's lives have raised the
need to implement systems able to detect these forgeries automatically. In this
work we present a novel approach for synthetic speech detection, exploiting the
combination of two high-level semantic properties of the human voice. On one
side, we focus on speaker identity cues and represent them as speaker
embeddings extracted using a state-of-the-art method for the automatic speaker
verification task. On the other side, voice prosody, intended as variations in
rhythm, pitch or accent in speech, is extracted through a specialized encoder.
We show that the combination of these two embeddings fed to a supervised binary
classifier allows the detection of deepfake speech generated with both
Text-to-Speech and Voice Conversion techniques. Our results show improvements
over the considered baselines, good generalization properties over multiple
datasets and robustness to audio compression.",,
DiaCorrect: End-to-end error correction for speaker diarization,"[arxiv.Result.Author('Jiangyu Han'), arxiv.Result.Author('Yuhang Cao'), arxiv.Result.Author('Heng Lu'), arxiv.Result.Author('Yanhua Long')]",2022-10-31 10:12:32+00:00,"In recent years, speaker diarization has attracted widespread attention. To
achieve better performance, some studies propose to diarize speech in multiple
stages. Although these methods might bring additional benefits, most of them
are quite complex. Motivated by spelling correction in automatic speech
recognition (ASR), in this paper, we propose an end-to-end error correction
framework, termed DiaCorrect, to refine the initial diarization results in a
simple but efficient way. By exploiting the acoustic interactions between input
mixture and its corresponding speaker activity, DiaCorrect could automatically
adapt the initial speaker activity to minimize the diarization errors. Without
bells and whistles, experiments on LibriSpeech based 2-speaker meeting-like
data show that, the self-attentitive end-to-end neural diarization (SA-EEND)
baseline with DiaCorrect could reduce its diarization error rate (DER) by over
62.4% from 12.31% to 4.63%. Our source code is available online at
https://github.com/jyhan03/diacorrect.",submitted to ICASSP,
Self-Supervised Hierarchical Metrical Structure Modeling,"[arxiv.Result.Author('Junyan Jiang'), arxiv.Result.Author('Gus Xia')]",2022-10-31 10:05:19+00:00,"We propose a novel method to model hierarchical metrical structures for both
symbolic music and audio signals in a self-supervised manner with minimal
domain knowledge. The model trains and inferences on beat-aligned music signals
and predicts an 8-layer hierarchical metrical tree from beat, measure to the
section level. The training procedure does not require any hierarchical
metrical labeling except for beats, purely relying on the nature of metrical
regularity and inter-voice consistency as inductive biases. We show in
experiments that the method achieves comparable performance with supervised
baselines on multiple metrical structure analysis tasks on both symbolic music
and audio signals. All demos, source code and pre-trained models are publicly
available on GitHub.",,
Minimum Processing Near-end Listening Enhancement,"[arxiv.Result.Author('Andreas Jonas Fuglsig'), arxiv.Result.Author('Jesper Jensen'), arxiv.Result.Author('Zheng-Hua Tan'), arxiv.Result.Author('Lars Søndergaard Bertelsen'), arxiv.Result.Author('Jens Christian Lindof'), arxiv.Result.Author('Jan Østergaard')]",2022-10-31 09:08:00+00:00,"The intelligibility and quality of speech from a mobile phone or public
announcement system are often affected by background noise in the listening
environment. By pre-processing the speech signal it is possible to improve the
speech intelligibility and quality -- this is known as near-end listening
enhancement (NLE). Although, existing NLE techniques are able to greatly
increase intelligibility in harsh noise environments, in favorable noise
conditions the intelligibility of speech reaches a ceiling where it cannot be
further enhanced. Actually, the focus of existing methods solely on improving
the intelligibility causes unnecessary processing of the speech signal and
leads to speech distortions and quality degradations. In this paper, we provide
a new rationale for NLE, where the target speech is minimally processed in
terms of a processing penalty, provided that a certain performance constraint,
e.g., intelligibility, is satisfied. We present a closed-form solution for the
case where the performance criterion is an intelligibility estimator based on
the approximated speech intelligibility index and the processing penalty is the
mean-square error between the processed and the clean speech. This produces an
NLE method that adapts to changing noise conditions via a simple gain rule by
limiting the processing to the minimum necessary to achieve a desired
intelligibility, while at the same time focusing on quality in favorable noise
situations by minimizing the amount of speech distortions. Through simulation
studies, we show the proposed method attains speech quality on par or better
than existing methods in both objective measurements and subjective listening
tests, whilst still sustaining objective speech intelligibility performance on
par with existing methods.",,
The Importance of Accurate Alignments in End-to-End Speech Synthesis,"[arxiv.Result.Author('Anusha Prakash'), arxiv.Result.Author('Hema A Murthy')]",2022-10-31 09:05:51+00:00,"Unit selection synthesis systems required accurate segmentation and labeling
of the speech signal owing to the concatenative nature. Hidden Markov
model-based speech synthesis accommodates some transcription errors, but it was
later shown that accurate transcriptions yield highly intelligible speech with
smaller amounts of training data. With the arrival of end-to-end (E2E) systems,
it was observed that very good quality speech could be synthesised with large
amounts of data. As end-to-end synthesis progressed from Tacotron to
FastSpeech2, it has become imminent that features that represent prosody are
important for good-quality synthesis. In particular, durations of the sub-word
units are important. Variants of FastSpeech use a teacher model or forced
alignments to obtain good-quality synthesis. In this paper, we focus on
duration prediction, using signal processing cues in tandem with forced
alignment to produce accurate phone durations during training.
  The current work aims to highlight the importance of accurate alignments for
good-quality synthesis. An attempt is made to train the E2E systems with
accurately labeled data, and compare the same with approximately labeled data.",Version 1 uploaded,
Audio Time-Scale Modification with Temporal Compressing Networks,"[arxiv.Result.Author('Ernie Chu'), arxiv.Result.Author('Ju-Ting Chen'), arxiv.Result.Author('Chia-Ping Chen')]",2022-10-31 09:04:33+00:00,"We proposed a novel approach in the field of time-scale modification on audio
signals. While traditional methods use the framing technique, spectral approach
uses the short-time Fourier transform to preserve the frequency during temporal
stretching. TSM-Net, our neural-network model encodes the raw audio into a
high-level latent representation. We call it Neuralgram, in which one vector
represents 1024 audio samples. It is inspired by the framing technique but
addresses the clipping artifacts. The Neuralgram is a two-dimensional matrix
with real values, we can apply some existing image resizing techniques on the
Neuralgram and decode it using our neural decoder to obtain the time-scaled
audio. Both the encoder and decoder are trained with GANs, which shows fair
generalization ability on the scaled Neuralgrams. Our method yields little
artifacts and opens a new possibility in the research of modern time-scale
modification. The audio samples can be found on
https://ernestchu.github.io/tsm-net-demo/",,
Improving Audio-Language Learning with MixGen and Multi-Level Test-Time Augmentation,"[arxiv.Result.Author('Eungbeom Kim'), arxiv.Result.Author('Jinhee Kim'), arxiv.Result.Author('Yoori Oh'), arxiv.Result.Author('Kyungsu Kim'), arxiv.Result.Author('Minju Park'), arxiv.Result.Author('Jaeheon Sim'), arxiv.Result.Author('Jinwoo Lee'), arxiv.Result.Author('Kyogu Lee')]",2022-10-31 08:50:52+00:00,"In this paper, we propose two novel augmentation methods 1) audio-language
MixGen (AL-MixGen) and 2) multi-level test-time augmentation (Multi-TTA) for
audio-language learning. Inspired by MixGen, which is originally applied to
vision-language learning, we introduce an augmentation method for the
audio-language domain. We also explore the impact of test-time augmentations
and present Multi-TTA which generalizes test-time augmentation over multiple
layers of a deep learning model. Incorporating AL-MixGen and Multi-TTA into the
baseline achieves 47.5 SPIDEr on audio captioning, which is an +18.2% over the
baseline and outperforms the state-of-the-art approach with a 5x smaller model.
In audio-text retrieval, the proposed methods surpass the baseline performance
as well.",Submitted to ICASSP 2023,
Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data,"[arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Shilin Zhou'), arxiv.Result.Author('Chen Gong'), arxiv.Result.Author('Zhenghua Li'), arxiv.Result.Author('Zhefeng Wang'), arxiv.Result.Author('Baoxing Huai'), arxiv.Result.Author('Min Zhang')]",2022-10-31 08:02:21+00:00,"Chinese word segmentation (CWS) models have achieved very high performance
when the training data is sufficient and in-domain. However, the performance
drops drastically when shifting to cross-domain and low-resource scenarios due
to data sparseness issues. Considering that constructing large-scale manually
annotated data is time-consuming and labor-intensive, in this work, we for the
first time propose to mine word boundary information from pauses in speech to
efficiently obtain large-scale CWS naturally annotated data. We present a
simple yet effective complete-then-train method to utilize these natural
annotations from speech for CWS model training. Extensive experiments
demonstrate that the CWS performance in cross-domain and low-resource scenarios
can be significantly improved by leveraging our naturally annotated data
extracted from speech.",Submitted to ICASSP2023,
Magnitude or Phase? A Two Stage Algorithm for Dereverberation,"[arxiv.Result.Author('Ayal Schwartz'), arxiv.Result.Author('Sharon Gannot'), arxiv.Result.Author('Shlomo E. Chazan')]",2022-10-31 07:56:48+00:00,"In this work we present a new single-microphone speech dereverberation
algorithm. First, a performance analysis is presented to interpret that
algorithms focused on improving solely magnitude or phase are not good enough.
Furthermore, we demonstrate that few objective measurements have high
correlation with the clean magnitude while others with the clean phase.
Consequently ,we propose a new architecture which consists of two sub-models,
each of which is responsible for a different task. The first model estimates
the clean magnitude given the noisy input. The enhanced magnitude together with
the noisy-input phase are then used as inputs to the second model to estimate
the real and imaginary portions of the dereverberated signal. A training scheme
including pre-training and fine-tuning is presented in the paper. We evaluate
our proposed approach using data from the REVERB challenge and compare our
results to other methods. We demonstrate consistent improvements in all
measures, which can be attributed to the improved estimates of both the
magnitude and the phase.",,
Fast and parallel decoding for transducer,"[arxiv.Result.Author('Wei Kang'), arxiv.Result.Author('Liyong Guo'), arxiv.Result.Author('Fangjun Kuang'), arxiv.Result.Author('Long Lin'), arxiv.Result.Author('Mingshuang Luo'), arxiv.Result.Author('Zengwei Yao'), arxiv.Result.Author('Xiaoyu Yang'), arxiv.Result.Author('Piotr Żelasko'), arxiv.Result.Author('Daniel Povey')]",2022-10-31 07:46:10+00:00,"The transducer architecture is becoming increasingly popular in the field of
speech recognition, because it is naturally streaming as well as high in
accuracy. One of the drawbacks of transducer is that it is difficult to decode
in a fast and parallel way due to an unconstrained number of symbols that can
be emitted per time step. In this work, we introduce a constrained version of
transducer loss to learn strictly monotonic alignments between the sequences;
we also improve the standard greedy search and beam search algorithms by
limiting the number of symbols that can be emitted per time step in transducer
decoding, making it more efficient to decode in parallel with batches.
Furthermore, we propose an finite state automaton-based (FSA) parallel beam
search algorithm that can run with graphs on GPU efficiently. The experiment
results show that we achieve slight word error rate (WER) improvement as well
as significant speedup in decoding. Our work is open-sourced and publicly
available\footnote{https://github.com/k2-fsa/icefall}.","Submitted to 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing",
Delay-penalized transducer for low-latency streaming ASR,"[arxiv.Result.Author('Wei Kang'), arxiv.Result.Author('Zengwei Yao'), arxiv.Result.Author('Fangjun Kuang'), arxiv.Result.Author('Liyong Guo'), arxiv.Result.Author('Xiaoyu Yang'), arxiv.Result.Author('Long lin'), arxiv.Result.Author('Piotr Żelasko'), arxiv.Result.Author('Daniel Povey')]",2022-10-31 07:03:50+00:00,"In streaming automatic speech recognition (ASR), it is desirable to reduce
latency as much as possible while having minimum impact on recognition
accuracy. Although a few existing methods are able to achieve this goal, they
are difficult to implement due to their dependency on external alignments. In
this paper, we propose a simple way to penalize symbol delay in transducer
model, so that we can balance the trade-off between symbol delay and accuracy
for streaming models without external alignments. Specifically, our method adds
a small constant times (T/2 - t), where T is the number of frames and t is the
current frame, to all the non-blank log-probabilities (after normalization)
that are fed into the two dimensional transducer recursion. For both streaming
Conformer models and unidirectional long short-term memory (LSTM) models,
experimental results show that it can significantly reduce the symbol delay
with an acceptable performance degradation. Our method achieves similar
delay-accuracy trade-off to the previously published FastEmit, but we believe
our method is preferable because it has a better justification: it is
equivalent to penalizing the average symbol delay. Our work is open-sourced and
publicly available (https://github.com/k2-fsa/k2).","Submitted to 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing",
Predicting Multi-Codebook Vector Quantization Indexes for Knowledge Distillation,"[arxiv.Result.Author('Liyong Guo'), arxiv.Result.Author('Xiaoyu Yang'), arxiv.Result.Author('Quandong Wang'), arxiv.Result.Author('Yuxiang Kong'), arxiv.Result.Author('Zengwei Yao'), arxiv.Result.Author('Fan Cui'), arxiv.Result.Author('Fangjun Kuang'), arxiv.Result.Author('Wei Kang'), arxiv.Result.Author('Long Lin'), arxiv.Result.Author('Mingshuang Luo'), arxiv.Result.Author('Piotr Zelasko'), arxiv.Result.Author('Daniel Povey')]",2022-10-31 07:03:17+00:00,"Knowledge distillation(KD) is a common approach to improve model performance
in automatic speech recognition (ASR), where a student model is trained to
imitate the output behaviour of a teacher model. However, traditional KD
methods suffer from teacher label storage issue, especially when the training
corpora are large. Although on-the-fly teacher label generation tackles this
issue, the training speed is significantly slower as the teacher model has to
be evaluated every batch. In this paper, we reformulate the generation of
teacher label as a codec problem. We propose a novel Multi-codebook Vector
Quantization (MVQ) approach that compresses teacher embeddings to codebook
indexes (CI). Based on this, a KD training framework (MVQ-KD) is proposed where
a student model predicts the CI generated from the embeddings of a
self-supervised pre-trained teacher model. Experiments on the LibriSpeech
clean-100 hour show that MVQ-KD framework achieves comparable performance as
traditional KD methods (l1, l2), while requiring 256 times less storage. When
the full LibriSpeech dataset is used, MVQ-KD framework results in 13.8% and
8.2% relative word error rate reductions (WERRs) for non -streaming transducer
on test-clean and test-other and 4.0% and 4.9% for streaming transducer. The
implementation of this work is already released as a part of the open-source
project icefall.",Submitted to ICASSP 2022,
Structured State Space Decoder for Speech Recognition and Synthesis,"[arxiv.Result.Author('Koichi Miyazaki'), arxiv.Result.Author('Masato Murata'), arxiv.Result.Author('Tomoki Koriyama')]",2022-10-31 06:54:23+00:00,"Automatic speech recognition (ASR) systems developed in recent years have
shown promising results with self-attention models (e.g., Transformer and
Conformer), which are replacing conventional recurrent neural networks.
Meanwhile, a structured state space model (S4) has been recently proposed,
producing promising results for various long-sequence modeling tasks, including
raw speech classification. The S4 model can be trained in parallel, same as the
Transformer model. In this study, we applied S4 as a decoder for ASR and
text-to-speech (TTS) tasks by comparing it with the Transformer decoder. For
the ASR task, our experimental results demonstrate that the proposed model
achieves a competitive word error rate (WER) of 1.88%/4.25% on LibriSpeech
test-clean/test-other set and a character error rate (CER) of 3.80%/2.63%/2.98%
on the CSJ eval1/eval2/eval3 set. Furthermore, the proposed model is more
robust than the standard Transformer model, particularly for long-form speech
on both the datasets. For the TTS task, the proposed method outperforms the
Transformer baseline.",Submitted to ICASSP 2023,
FusionFormer: Fusing Operations in Transformer for Efficient Streaming Speech Recognition,"[arxiv.Result.Author('Xingchen Song'), arxiv.Result.Author('Di Wu'), arxiv.Result.Author('Binbin Zhang'), arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Wenpeng Li'), arxiv.Result.Author('Dongfang Li'), arxiv.Result.Author('Pengshen Zhang'), arxiv.Result.Author('Zhendong Peng'), arxiv.Result.Author('Fuping Pan'), arxiv.Result.Author('Changbao Zhu'), arxiv.Result.Author('Zhongqin Wu')]",2022-10-31 06:01:02+00:00,"The recently proposed Conformer architecture which combines convolution with
attention to capture both local and global dependencies has become the
\textit{de facto} backbone model for Automatic Speech Recognition~(ASR).
Inherited from the Natural Language Processing (NLP) tasks, the architecture
takes Layer Normalization~(LN) as a default normalization technique. However,
through a series of systematic studies, we find that LN might take 10\% of the
inference time despite that it only contributes to 0.1\% of the FLOPs. This
motivates us to replace LN with other normalization techniques, e.g., Batch
Normalization~(BN), to speed up inference with the help of operator fusion
methods and the avoidance of calculating the mean and variance statistics
during inference. After examining several plain attempts which directly remove
all LN layers or replace them with BN in the same place, we find that the
divergence issue is mainly caused by the unstable layer output. We therefore
propose to append a BN layer to each linear or convolution layer where
stabilized training results are observed. We also propose to simplify the
activations in Conformer, such as Swish and GLU, by replacing them with ReLU.
All these exchanged modules can be fused into the weights of the adjacent
linear/convolution layers and hence have zero inference cost. Therefore, we
name it FusionFormer. Our experiments indicate that FusionFormer is as
effective as the LN-based Conformer and is about 10\% faster.","8 pages, plus 3 appendix",
Modular Hybrid Autoregressive Transducer,"[arxiv.Result.Author('Zhong Meng'), arxiv.Result.Author('Tongzhou Chen'), arxiv.Result.Author('Rohit Prabhavalkar'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Gary Wang'), arxiv.Result.Author('Kartik Audhkhasi'), arxiv.Result.Author('Jesse Emond'), arxiv.Result.Author('Trevor Strohman'), arxiv.Result.Author('Bhuvana Ramabhadran'), arxiv.Result.Author('W. Ronny Huang'), arxiv.Result.Author('Ehsan Variani'), arxiv.Result.Author('Yinghui Huang'), arxiv.Result.Author('Pedro J. Moreno')]",2022-10-31 03:56:37+00:00,"Text-only adaptation of a transducer model remains challenging for end-to-end
speech recognition since the transducer has no clearly separated acoustic model
(AM), language model (LM) or blank model. In this work, we propose a modular
hybrid autoregressive transducer (MHAT) that has structurally separated label
and blank decoders to predict label and blank distributions, respectively,
along with a shared acoustic encoder. The encoder and label decoder outputs are
directly projected to AM and internal LM scores and then added to compute label
posteriors. We train MHAT with an internal LM loss and a HAT loss to ensure
that its internal LM becomes a standalone neural LM that can be effectively
adapted to text. Moreover, text adaptation of MHAT fosters a much better LM
fusion than internal LM subtraction-based methods. On Google's large-scale
production data, a multi-domain MHAT adapted with 100B sentences achieves
relative WER reductions of up to 12.4% without LM fusion and 21.5% with LM
fusion from 400K-hour trained HAT.","8 pages, 1 figure, SLT 2022","2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar"
Joint Pre-Training with Speech and Bilingual Text for Direct Speech to Speech Translation,"[arxiv.Result.Author('Kun Wei'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Ziqiang Zhang'), arxiv.Result.Author('Liping Chen'), arxiv.Result.Author('Shujie Liu'), arxiv.Result.Author('Lei He'), arxiv.Result.Author('Jinyu Li'), arxiv.Result.Author('Furu Wei')]",2022-10-31 02:55:51+00:00,"Direct speech-to-speech translation (S2ST) is an attractive research topic
with many advantages compared to cascaded S2ST. However, direct S2ST suffers
from the data scarcity problem because the corpora from speech of the source
language to speech of the target language are very rare. To address this issue,
we propose in this paper a Speech2S model, which is jointly pre-trained with
unpaired speech and bilingual text data for direct speech-to-speech translation
tasks. By effectively leveraging the paired text data, Speech2S is capable of
modeling the cross-lingual speech conversion from source to target language. We
verify the performance of the proposed Speech2S on Europarl-ST and VoxPopuli
datasets. Experimental results demonstrate that Speech2S gets an improvement of
about 5 BLEU scores compared to encoder-only pre-training models, and achieves
a competitive or even better performance than existing state-of-the-art
models1.",Submitted to ICASSP 2023,
Blank Collapse: Compressing CTC emission for the faster decoding,"[arxiv.Result.Author('Minkyu Jung'), arxiv.Result.Author('Ohhyeok Kwon'), arxiv.Result.Author('Seunghyun Seo'), arxiv.Result.Author('Soonshin Seo')]",2022-10-31 02:12:51+00:00,"Connectionist Temporal Classification (CTC) model is a very efficient method
for modeling sequences, especially for speech data. In order to use CTC model
as an Automatic Speech Recognition (ASR) task, the beam search decoding with an
external language model like n-gram LM is necessary to obtain reasonable
results. In this paper we analyze the blank label in CTC beam search deeply and
propose a very simple method to reduce the amount of calculation resulting in
faster beam search decoding speed. With this method, we can get up to 78%
faster decoding speed than ordinary beam search decoding with a very small loss
of accuracy in LibriSpeech datasets. We prove this method is effective not only
practically by experiments but also theoretically by mathematical reasoning. We
also observe that this reduction is more obvious if the accuracy of the model
is higher.",Submitted to ICASSP 2023,
Wespeaker: A Research and Production oriented Speaker Embedding Learning Toolkit,"[arxiv.Result.Author('Hongji Wang'), arxiv.Result.Author('Chengdong Liang'), arxiv.Result.Author('Shuai Wang'), arxiv.Result.Author('Zhengyang Chen'), arxiv.Result.Author('Binbin Zhang'), arxiv.Result.Author('Xu Xiang'), arxiv.Result.Author('Yanlei Deng'), arxiv.Result.Author('Yanmin Qian')]",2022-10-31 02:11:58+00:00,"Speaker modeling is essential for many related tasks, such as speaker
recognition and speaker diarization. The dominant modeling approach is
fixed-dimensional vector representation, i.e., speaker embedding. This paper
introduces a research and production oriented speaker embedding learning
toolkit, Wespeaker. Wespeaker contains the implementation of scalable data
management, state-of-the-art speaker embedding models, loss functions, and
scoring back-ends, with highly competitive results achieved by structured
recipes which were adopted in the winning systems in several speaker
verification challenges. The application to other downstream tasks such as
speaker diarization is also exhibited in the related recipe. Moreover, CPU- and
GPU-compatible deployment codes are integrated for production-oriented
development. The toolkit is publicly available at
https://github.com/wenet-e2e/wespeaker.",,
"Learning to Defer to Multiple Experts: Consistent Surrogate Losses, Confidence Calibration, and Conformal Ensembles","[arxiv.Result.Author('Rajeev Verma'), arxiv.Result.Author('Daniel Barrejón'), arxiv.Result.Author('Eric Nalisnick')]",2022-10-30 21:27:29+00:00,"We study the statistical properties of learning to defer (L2D) to multiple
experts. In particular, we address the open problems of deriving a consistent
surrogate loss, confidence calibration, and principled ensembling of experts.
Firstly, we derive two consistent surrogates -- one based on a softmax
parameterization, the other on a one-vs-all (OvA) parameterization -- that are
analogous to the single expert losses proposed by Mozannar and Sontag (2020)
and Verma and Nalisnick (2022), respectively. We then study the frameworks'
ability to estimate P( m_j = y | x ), the probability that the jth expert will
correctly predict the label for x. Theory shows the softmax-based loss causes
mis-calibration to propagate between the estimates while the OvA-based loss
does not (though in practice, we find there are trade offs). Lastly, we propose
a conformal inference technique that chooses a subset of experts to query when
the system defers. We perform empirical validation on tasks for galaxy, skin
lesion, and hate speech classification.",First two authors contributed equally,
Real-Time MRI Video synthesis from time aligned phonemes with sequence-to-sequence networks,"[arxiv.Result.Author('Sathvik Udupa'), arxiv.Result.Author('Prasanta Kumar Ghosh')]",2022-10-30 16:45:59+00:00,"Real-Time Magnetic resonance imaging (rtMRI) of the midsagittal plane of the
mouth is of interest for speech production research. In this work, we focus on
estimating utterance level rtMRI video from the spoken phoneme sequence. We
obtain time-aligned phonemes from forced alignment, to obtain frame-level
phoneme sequences which are aligned with rtMRI frames. We propose a
sequence-to-sequence learning model with a transformer phoneme encoder and
convolutional frame decoder. We then modify the learning by using intermediary
features obtained from sampling from a pretrained phoneme-conditioned
variational autoencoder (CVAE). We train on 8 subjects in a subject-specific
manner and demonstrate the performance with a subjective test. We also use an
auxiliary task of air tissue boundary (ATB) segmentation to obtain the
objective scores on the proposed models. We show that the proposed method is
able to generate realistic rtMRI video for unseen utterances, and adding CVAE
is beneficial for learning the sequence-to-sequence mapping for subjects where
the mapping is hard to learn.",submitted to ICASSP 2023,
Improved acoustic-to-articulatory inversion using representations from pretrained self-supervised learning models,"[arxiv.Result.Author('Sathvik Udupa'), arxiv.Result.Author('Siddarth C'), arxiv.Result.Author('Prasanta Kumar Ghosh')]",2022-10-30 16:24:02+00:00,"In this work, we investigate the effectiveness of pretrained Self-Supervised
Learning (SSL) features for learning the mapping for acoustic to articulatory
inversion (AAI). Signal processing-based acoustic features such as MFCCs have
been predominantly used for the AAI task with deep neural networks. With SSL
features working well for various other speech tasks such as speech
recognition, emotion classification, etc., we experiment with its efficacy for
AAI. We train on SSL features with transformer neural networks-based AAI models
of 3 different model complexities and compare its performance with MFCCs in
subject-specific (SS), pooled and fine-tuned (FT) configurations with data from
10 subjects, and evaluate with correlation coefficient (CC) score on the unseen
sentence test set. We find that acoustic feature reconstruction objective-based
SSL features such as TERA and DeCoAR work well for AAI, with SS CCs of these
SSL features reaching close to the best FT CCs of MFCC. We also find the
results consistent across different model sizes.",submitted to ICASSP 2023,
TT-Net: Dual-path transformer based sound field translation in the spherical harmonic domain,"[arxiv.Result.Author('Yiwen Wang'), arxiv.Result.Author('Zijian Lan'), arxiv.Result.Author('Xihong Wu'), arxiv.Result.Author('Tianshu Qu')]",2022-10-30 14:16:48+00:00,"In the current method for the sound field translation tasks based on
spherical harmonic (SH) analysis, the solution based on the additive theorem
usually faces the problem of singular values caused by large matrix condition
numbers. The influence of different distances and frequencies of the spherical
radial function on the stability of the translation matrix will affect the
accuracy of the SH coefficients at the selected point. Due to the problems
mentioned above, we propose a neural network scheme based on the dual-path
transformer. More specifically, the dual-path network is constructed by the
self-attention module along the two dimensions of the frequency and order axes.
The transform-average-concatenate layer and upscaling layer are introduced in
the network, which provides solutions for multiple sampling points and
upscaling. Numerical simulation results indicate that both the working
frequency range and the distance range of the translation are extended. More
accurate higher-order SH coefficients are obtained with the proposed dual-path
network.",Submitted to ICASSP 2023,
SRTNet: Time Domain Speech Enhancement Via Stochastic Refinement,"[arxiv.Result.Author('Zhibin Qiu'), arxiv.Result.Author('Mengfan Fu'), arxiv.Result.Author('Yinfeng Yu'), arxiv.Result.Author('LiLi Yin'), arxiv.Result.Author('Fuchun Sun'), arxiv.Result.Author('Hao Huang')]",2022-10-30 10:36:11+00:00,"Diffusion model, as a new generative model which is very popular in image
generation and audio synthesis, is rarely used in speech enhancement. In this
paper, we use the diffusion model as a module for stochastic refinement. We
propose SRTNet, a novel method for speech enhancement via Stochastic Refinement
in complete Time domain. Specifically, we design a joint network consisting of
a deterministic module and a stochastic module, which makes up the
``enhance-and-refine'' paradigm. We theoretically demonstrate the feasibility
of our method and experimentally prove that our method achieves faster
training, faster sampling and higher quality. Our code and enhanced samples are
available at https://github.com/zhibinQiu/SRTNet.git.",,
Adaptive Speech Quality Aware Complex Neural Network for Acoustic Echo Cancellation with Supervised Contrastive Learning,"[arxiv.Result.Author('Bozhong Liu'), arxiv.Result.Author('Xiaoxi Yu'), arxiv.Result.Author('Hantao Huang')]",2022-10-30 09:42:03+00:00,"Acoustic echo cancellation (AEC) is designed to remove echoes, reverberation,
and unwanted added sounds from the microphone signal while maintaining the
quality of the near-end speaker's speech. This paper proposes adaptive speech
quality complex neural networks to focus on specific tasks for real-time
acoustic echo cancellation. In specific, we propose a complex modularize neural
network with different stages to focus on feature extraction, acoustic
separation, and mask optimization receptively. Furthermore, we adopt the
contrastive learning framework and novel speech quality aware loss functions to
further improve the performance. The model is trained with 72 hours for
pre-training and then 72 hours for fine-tuning. The proposed model outperforms
the state-of-the-art performance.","Submitted to International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2023. Under review",
Symmetric Saliency-based Adversarial Attack To Speaker Identification,"[arxiv.Result.Author('Jiadi Yao'), arxiv.Result.Author('Xing Chen'), arxiv.Result.Author('Xiao-Lei Zhang'), arxiv.Result.Author('Wei-Qiang Zhang'), arxiv.Result.Author('Kunde Yang')]",2022-10-30 08:54:02+00:00,"Adversarial attack approaches to speaker identification either need high
computational cost or are not very effective, to our knowledge. To address this
issue, in this paper, we propose a novel generation-network-based approach,
called symmetric saliency-based encoder-decoder (SSED), to generate adversarial
voice examples to speaker identification. It contains two novel components.
First, it uses a novel saliency map decoder to learn the importance of speech
samples to the decision of a targeted speaker identification system, so as to
make the attacker focus on generating artificial noise to the important
samples. It also proposes an angular loss function to push the speaker
embedding far away from the source speaker. Our experimental results
demonstrate that the proposed SSED yields the state-of-the-art performance,
i.e. over 97% targeted attack success rate and a signal-to-noise level of over
39 dB on both the open-set and close-set speaker identification tasks, with a
low computational cost.",,
token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text,"[arxiv.Result.Author('Xianghu Yue'), arxiv.Result.Author('Junyi Ao'), arxiv.Result.Author('Xiaoxue Gao'), arxiv.Result.Author('Haizhou Li')]",2022-10-30 06:38:19+00:00,"Self-supervised pre-training has been successful in both text and speech
processing. Speech and text offer different but complementary information. The
question is whether we are able to perform a speech-text joint pre-training on
unpaired speech and text. In this paper, we take the idea of self-supervised
pre-training one step further and propose token2vec, a novel joint pre-training
framework for unpaired speech and text based on discrete representations of
speech. Firstly, due to the distinct characteristics between speech and text
modalities, where speech is continuous while text is discrete, we first
discretize speech into a sequence of discrete speech tokens to solve the
modality mismatch problem. Secondly, to solve the length mismatch problem,
where the speech sequence is usually much longer than text sequence, we convert
the words of text into phoneme sequences and randomly repeat each phoneme in
the sequences. Finally, we feed the discrete speech and text tokens into a
modality-agnostic Transformer encoder and pre-train with token-level masking
language modeling (tMLM). Experiments show that token2vec is significantly
superior to various speech-only pre-training baselines, with up to 17.7%
relative WER reduction. Token2vec model is also validated on a non-ASR task,
i.e., spoken intent classification, and shows good transferability.",Submitted to ICASSP 2023,
WeKws: A production first small-footprint end-to-end Keyword Spotting Toolkit,"[arxiv.Result.Author('Jie Wang'), arxiv.Result.Author('Menglong Xu'), arxiv.Result.Author('Jingyong Hou'), arxiv.Result.Author('Binbin Zhang'), arxiv.Result.Author('Xiao-Lei Zhang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Fuping Pan')]",2022-10-30 04:45:07+00:00,"Keyword spotting (KWS) enables speech-based user interaction and gradually
becomes an indispensable component of smart devices. Recently, end-to-end (E2E)
methods have become the most popular approach for on-device KWS tasks. However,
there is still a gap between the research and deployment of E2E KWS methods. In
this paper, we introduce WeKws, a production-quality, easy-to-build, and
convenient-to-be-applied E2E KWS toolkit. WeKws contains the implementations of
several state-of-the-art backbone networks, making it achieve highly
competitive results on three publicly available datasets. To make WeKws a pure
E2E toolkit, we utilize a refined max-pooling loss to make the model learn the
ending position of the keyword by itself, which significantly simplifies the
training pipeline and makes WeKws very efficient to be applied in real-world
scenarios. The toolkit is publicly available at
https://github.com/wenet-e2e/wekws.",,
DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set,"[arxiv.Result.Author('Arunkumar A'), arxiv.Result.Author('Mudit Batra'), arxiv.Result.Author('Umesh S')]",2022-10-30 04:01:26+00:00,"In a multilingual country like India, multilingual Automatic Speech
Recognition (ASR) systems have much scope. Multilingual ASR systems exhibit
many advantages like scalability, maintainability, and improved performance
over the monolingual ASR systems. However, building multilingual systems for
Indian languages is challenging since different languages use different scripts
for writing. On the other hand, Indian languages share a lot of common sounds.
Common Label Set (CLS) exploits this idea and maps graphemes of various
languages with similar sounds to common labels. Since Indian languages are
mostly phonetic, building a parser to convert from native script to CLS is
easy. In this paper, we explore various approaches to build multilingual ASR
models. We also propose a novel architecture called Encoder-Decoder-Decoder for
building multilingual systems that use both CLS and native script labels. We
also analyzed the effectiveness of CLS-based multilingual systems combined with
machine transliteration.",Submitted to ICASSP 2023,
Improvements to Embedding-Matching Acoustic-to-Word ASR Using Multiple-Hypothesis Pronunciation-Based Embeddings,"[arxiv.Result.Author('Hao Yen'), arxiv.Result.Author('Woojay Jeon')]",2022-10-30 02:37:55+00:00,"In embedding-matching acoustic-to-word (A2W) ASR, every word in the
vocabulary is represented by a fixed-dimension embedding vector that can be
added or removed independently of the rest of the system. The approach is
potentially an elegant solution for the dynamic out-of-vocabulary (OOV) words
problem, where speaker- and context-dependent named entities like contact names
must be incorporated into the ASR on-the-fly for every speech utterance at
testing time. Challenges still remain, however, in improving the overall
accuracy of embedding-matching A2W. In this paper, we contribute two methods
that improve the accuracy of embedding-matching A2W. First, we propose
internally producing multiple embeddings, instead of a single embedding, at
each instance in time, which allows the A2W model to propose a richer set of
hypotheses over multiple time segments in the audio. Second, we propose using
word pronunciation embeddings rather than word orthography embeddings to reduce
ambiguities introduced by words that have more than one sound. We show that the
above ideas give significant accuracy improvement, with the same training data
and nearly identical model size, in scenarios where dynamic OOV words play a
crucial role. On a dataset of various queries to a speech-based digital
assistant that include many user-dependent contact names, we observe up to 18%
decrease in word error rate using the proposed improvements.",,
Recursive Estimation of User Intent from Noninvasive Electroencephalography using Discriminative Models,"[arxiv.Result.Author('Niklas Smedemark-Margulies'), arxiv.Result.Author('Basak Celik'), arxiv.Result.Author('Tales Imbiriba'), arxiv.Result.Author('Aziz Kocanaogullari'), arxiv.Result.Author('Deniz Erdogmus')]",2022-10-29 21:19:08+00:00,"We study the problem of inferring user intent from noninvasive
electroencephalography (EEG) to restore communication for people with severe
speech and physical impairments (SSPI). The focus of this work is improving the
estimation of posterior symbol probabilities in a typing task. At each
iteration of the typing procedure, a subset of symbols is chosen for the next
query based on the current probability estimate. Evidence about the user's
response is collected from event-related potentials (ERP) in order to update
symbol probabilities, until one symbol exceeds a predefined confidence
threshold. We provide a graphical model describing this task, and derive a
recursive Bayesian update rule based on a discriminative probability over label
vectors for each query, which we approximate using a neural network classifier.
We evaluate the proposed method in a simulated typing task and show that it
outperforms previous approaches based on generative modeling.","5 pages, 2 figures",
BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model,"[arxiv.Result.Author('Yosuke Higuchi'), arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Tetsuji Ogawa'), arxiv.Result.Author('Tetsunori Kobayashi'), arxiv.Result.Author('Shinji Watanabe')]",2022-10-29 18:19:44+00:00,"This paper presents BERT-CTC, a novel formulation of end-to-end speech
recognition that adapts BERT for connectionist temporal classification (CTC).
Our formulation relaxes the conditional independence assumptions used in
conventional CTC and incorporates linguistic knowledge through the explicit
output dependency obtained by BERT contextual embedding. BERT-CTC attends to
the full contexts of the input and hypothesized output sequences via the
self-attention mechanism. This mechanism encourages a model to learn
inner/inter-dependencies between the audio and token representations while
maintaining CTC's training efficiency. During inference, BERT-CTC combines a
mask-predict algorithm with CTC decoding, which iteratively refines an output
sequence. The experimental results reveal that BERT-CTC improves over
conventional approaches across variations in speaking styles and languages.
Finally, we show that the semantic representations in BERT-CTC are beneficial
towards downstream spoken language understanding tasks.",Accepted to Findings of EMNLP2022,
Learning Dependencies of Discrete Speech Representations with Neural Hidden Markov Models,"[arxiv.Result.Author('Sung-Lin Yeh'), arxiv.Result.Author('Hao Tang')]",2022-10-29 17:46:58+00:00,"While discrete latent variable models have had great success in
self-supervised learning, most models assume that frames are independent. Due
to the segmental nature of phonemes in speech perception, modeling dependencies
among latent variables at the frame level can potentially improve the learned
representations on phonetic-related tasks. In this work, we assume Markovian
dependencies among latent variables, and propose to learn speech
representations with neural hidden Markov models. Our general framework allows
us to compare to self-supervised models that assume independence, while keeping
the number of parameters fixed. The added dependencies improve the
accessibility of phonetic information, phonetic segmentation, and the cluster
purity of phones, showcasing the benefit of the assumed dependencies.",,
XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers,"[arxiv.Result.Author('Roshan Sharma'), arxiv.Result.Author('Bhiksha Raj')]",2022-10-29 16:21:30+00:00,"Transformers are among the state of the art for many tasks in speech, vision,
and natural language processing, among others. Self-attentions, which are
crucial contributors to this performance have quadratic computational
complexity, which makes training on longer input sequences challenging. Prior
work has produced state-of-the-art transformer variants with linear attention,
however, current models sacrifice performance to achieve efficient
implementations. In this work, we develop a novel linear transformer by
examining the properties of the key-query product within self-attentions. Our
model outperforms state of the art approaches on speech recognition and speech
summarization, resulting in 1 % absolute WER improvement on the Librispeech-100
speech recognition benchmark and a new INTERVIEW speech recognition benchmark,
and 5 points on ROUGE for summarization with How2.",Under review at ICASSP 2023,
Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition,"[arxiv.Result.Author('Roshan Sharma'), arxiv.Result.Author('Hira Dhamyal'), arxiv.Result.Author('Bhiksha Raj'), arxiv.Result.Author('Rita Singh')]",2022-10-29 16:12:31+00:00,"Traditionally, in paralinguistic analysis for emotion detection from speech,
emotions have been identified with discrete or dimensional (continuous-valued)
labels. Accordingly, models that have been proposed for emotion detection use
one or the other of these label types. However, psychologists like Russell and
Plutchik have proposed theories and models that unite these views, maintaining
that these representations have shared and complementary information. This
paper is an attempt to validate these viewpoints computationally. To this end,
we propose a model to jointly predict continuous and discrete emotional
attributes and show how the relationship between these can be utilized to
improve the robustness and performance of emotion recognition tasks. Our
approach comprises multi-task and hierarchical multi-task learning frameworks
that jointly model the relationships between continuous-valued and discrete
emotion labels. Experimental results on two widely used datasets (IEMOCAP and
MSPPodcast) for speech-based emotion recognition show that our model results in
statistically significant improvements in performance over strong baselines
with non-unified approaches. We also demonstrate that using one type of label
(discrete or continuous-valued) for training improves recognition performance
in tasks that use the other type of label. Experimental results and reasoning
for this approach (called the mismatched training approach) are also presented.",Under Review at ICASSP 2023,
Speaker Representation Learning via Contrastive Loss with Maximal Speaker Separability,"[arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Man-Wai Mak')]",2022-10-29 16:01:29+00:00,"A great challenge in speaker representation learning using deep models is to
design learning objectives that can enhance the discrimination of unseen
speakers under unseen domains. This work proposes a supervised contrastive
learning objective to learn a speaker embedding space by effectively leveraging
the label information in the training data. In such a space, utterance pairs
spoken by the same or similar speakers will stay close, while utterance pairs
spoken by different speakers will be far apart. For each training speaker, we
perform random data augmentation on their utterances to form positive pairs,
and utterances from different speakers form negative pairs. To maximize speaker
separability in the embedding space, we incorporate the additive angular-margin
loss into the contrastive learning objective. Experimental results on CN-Celeb
show that this new learning objective can cause ECAPA-TDNN to find an embedding
space that exhibits great speaker discrimination. The contrastive learning
objective is easy to implement, and we provide PyTorch code at
https://github.com/shanmon110/AAMSupCon.","Accept by APSIPA ASC 2022, 6 pages, 2 figures",
Discriminative Speaker Representation via Contrastive Learning with Class-Aware Attention in Angular Space,"[arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Man-Wai Mak'), arxiv.Result.Author('Helen Mei-Ling Meng')]",2022-10-29 14:55:24+00:00,"The challenges in applying contrastive learning to speaker verification (SV)
are that the softmax-based contrastive loss lacks discriminative power and that
the hard negative pairs can easily influence learning. To overcome these
challenges, we propose a contrastive learning SV framework incorporating an
additive angular margin into the supervised contrastive loss. The margin
improves the speaker representation's discrimination ability. We introduce a
class-aware attention mechanism through which hard negative samples contribute
less significantly to the supervised contrastive loss. We also employed a
gradient-based multi-objective optimization approach to balance the
classification and contrastive loss. Experimental results on CN-Celeb and
Voxceleb1 show that this new learning objective can cause the encoder to find
an embedding space that exhibits great speaker discrimination across languages.","Submitted to ICASSP 2023, 5 pages, 2 figures",
Application of Knowledge Distillation to Multi-task Speech Representation Learning,"[arxiv.Result.Author('Mine Kerpicci'), arxiv.Result.Author('Van Nguyen'), arxiv.Result.Author('Shuhua Zhang'), arxiv.Result.Author('Erik Visser')]",2022-10-29 14:22:43+00:00,"Model architectures such as wav2vec 2.0 and HuBERT have been proposed to
learn speech representations from audio waveforms in a self-supervised manner.
When these models are combined with downstream tasks such as speech
recognition, they have been shown to provide state-of-the-art performance.
However, these models use a large number of parameters, the smallest version of
which has about 95 million parameters. This constitutes a challenge for edge AI
device deployments. In this paper, we use knowledge distillation to reduce the
original model size by about 75% while maintaining similar performance levels.
Moreover, we use wav2vec 2.0 and HuBERT models for distillation and present a
comprehensive performance analysis through our experiments where we fine-tune
the distilled models on single task and multi-task frameworks separately. In
particular, our experiments show that fine-tuning the distilled models on
keyword spotting and speaker verification tasks result in only 0.1% accuracy
and 0.9% equal error rate degradations, respectively.","Speech representation learning, multitask learning, wav2vec, HuBERT,
  knowledge distillation",
Relating Human Perception of Musicality to Prediction in a Predictive Coding Model,"[arxiv.Result.Author('Nikolas McNeal'), arxiv.Result.Author('Jennifer Huang'), arxiv.Result.Author('Aniekan Umoren'), arxiv.Result.Author('Shuqi Dai'), arxiv.Result.Author('Roger Dannenberg'), arxiv.Result.Author('Richard Randall'), arxiv.Result.Author('Tai Sing Lee')]",2022-10-29 12:20:01+00:00,"We explore the use of a neural network inspired by predictive coding for
modeling human music perception. This network was developed based on the
computational neuroscience theory of recurrent interactions in the hierarchical
visual cortex. When trained with video data using self-supervised learning, the
model manifests behaviors consistent with human visual illusions. Here, we
adapt this network to model the hierarchical auditory system and investigate
whether it will make similar choices to humans regarding the musicality of a
set of random pitch sequences. When the model is trained with a large corpus of
instrumental classical music and popular melodies rendered as mel spectrograms,
it exhibits greater prediction errors for random pitch sequences that are rated
less musical by human subjects. We found that the prediction error depends on
the amount of information regarding the subsequent note, the pitch interval,
and the temporal context. Our findings suggest that predictability is
correlated with human perception of musicality and that a predictive coding
neural network trained on music can be used to characterize the features and
motifs contributing to human perception of music.","5 pages, 5 figures, currently in peer review",
End-to-end Spoken Language Understanding with Tree-constrained Pointer Generator,"[arxiv.Result.Author('Guangzhi Sun'), arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Philip C. Woodland')]",2022-10-29 10:03:56+00:00,"End-to-end spoken language understanding (SLU) suffers from the long-tail
word problem. This paper exploits contextual biasing, a technique to improve
the speech recognition of rare words, in end-to-end SLU systems. Specifically,
a tree-constrained pointer generator (TCPGen), a powerful and efficient biasing
model component, is studied, which leverages a slot shortlist with
corresponding entities to extract biasing lists. Meanwhile, to bias the SLU
model output slot distribution, a slot probability biasing (SPB) mechanism is
proposed to calculate a slot distribution from TCPGen. Experiments on the SLURP
dataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially
on unseen entities. On a new split by holding out 5 slot types for the test,
TCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%
compared to baselines which can not deal with it. In addition to slot filling,
the intent classification accuracy was also improved.","5 pages, submitted to ICASSP 2023",
Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection,"[arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Jiajun Deng'), arxiv.Result.Author('Tianzi Wang'), arxiv.Result.Author('Bo Zheng'), arxiv.Result.Author('Shoukang Hu'), arxiv.Result.Author('Xunying Liu'), arxiv.Result.Author('Helen Meng')]",2022-10-29 09:18:41+00:00,"Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating
preventive care and to delay further progression. Speech based automatic AD
screening systems provide a non-intrusive and more scalable alternative to
other clinical screening techniques. Textual embedding features produced by
pre-trained language models (PLMs) such as BERT are widely used in such
systems. However, PLM domain fine-tuning is commonly based on the masked word
or sentence prediction costs that are inconsistent with the back-end AD
detection task. To this end, this paper investigates the use of prompt-based
fine-tuning of PLMs that consistently uses AD classification errors as the
training objective function. Disfluency features based on hesitation or pause
filler token frequencies are further incorporated into prompt phrases during
PLM fine-tuning. The exploit of the complementarity between BERT or RoBERTa
based PLMs that are either prompt learning fine-tuned, or optimized using
conventional masked word or sentence prediction costs, decision voting based
system combination between them is further applied. Mean, standard deviation
and the maximum among accuracy scores over 15 experiment runs are adopted as
performance measurements for the AD detection system. Mean detection accuracy
of 84.20% (with std 2.09%, best 87.5%) and 82.64% (with std 4.0%, best 89.58%)
were obtained using manual and ASR speech transcripts respectively on the
ADReSS20 test set consisting of 48 elderly speakers.",Submitted to ICASSP 2023,
Phonemic Representation and Transcription for Speech to Text Applications for Under-resourced Indigenous African Languages: The Case of Kiswahili,"[arxiv.Result.Author('Ebbie Awino'), arxiv.Result.Author('Lilian Wanzare'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Barack Wanjawa'), arxiv.Result.Author('Edward Ombui'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Benard Okal')]",2022-10-29 09:04:09+00:00,"Building automatic speech recognition (ASR) systems is a challenging task,
especially for under-resourced languages that need to construct corpora nearly
from scratch and lack sufficient training data. It has emerged that several
African indigenous languages, including Kiswahili, are technologically
under-resourced. ASR systems are crucial, particularly for the hearing-impaired
persons who can benefit from having transcripts in their native languages.
However, the absence of transcribed speech datasets has complicated efforts to
develop ASR models for these indigenous languages. This paper explores the
transcription process and the development of a Kiswahili speech corpus, which
includes both read-out texts and spontaneous speech data from native Kiswahili
speakers. The study also discusses the vowels and consonants in Kiswahili and
provides an updated Kiswahili phoneme dictionary for the ASR model that was
created using the CMU Sphinx speech recognition toolbox, an open-source speech
recognition toolkit. The ASR model was trained using an extended phonetic set
that yielded a WER and SER of 18.87% and 49.5%, respectively, an improved
performance than previous similar research for under-resourced languages.",,
Articulatory Representation Learning Via Joint Factor Analysis and Neural Matrix Factorization,"[arxiv.Result.Author('Jiachen Lian'), arxiv.Result.Author('Alan W Black'), arxiv.Result.Author('Yijing Lu'), arxiv.Result.Author('Louis Goldstein'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Gopala K. Anumanchipalli')]",2022-10-29 05:28:54+00:00,"Articulatory representation learning is the fundamental research in modeling
neural speech production system. Our previous work has established a deep
paradigm to decompose the articulatory kinematics data into gestures, which
explicitly model the phonological and linguistic structure encoded with human
speech production mechanism, and corresponding gestural scores. We continue
with this line of work by raising two concerns: (1) The articulators are
entangled together in the original algorithm such that some of the articulators
do not leverage effective moving patterns, which limits the interpretability of
both gestures and gestural scores; (2) The EMA data is sparsely sampled from
articulators, which limits the intelligibility of learned representations. In
this work, we propose a novel articulatory representation decomposition
algorithm that takes the advantage of guided factor analysis to derive the
articulatory-specific factors and factor scores. A neural convolutive matrix
factorization algorithm is then employed on the factor scores to derive the new
gestures and gestural scores. We experiment with the rtMRI corpus that captures
the fine-grained vocal tract contours. Both subjective and objective evaluation
results suggest that the newly proposed system delivers the articulatory
representations that are intelligible, generalizable, efficient and
interpretable.",Submitted to 2023 ICASSP,
Accelerating RNN-T Training and Inference Using CTC guidance,"[arxiv.Result.Author('Yongqiang Wang'), arxiv.Result.Author('Zhehuai Chen'), arxiv.Result.Author('Chengjian Zheng'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Wei Han'), arxiv.Result.Author('Parisa Haghani')]",2022-10-29 03:39:18+00:00,"We propose a novel method to accelerate training and inference process of
recurrent neural network transducer (RNN-T) based on the guidance from a
co-trained connectionist temporal classification (CTC) model. We made a key
assumption that if an encoder embedding frame is classified as a blank frame by
the CTC model, it is likely that this frame will be aligned to blank for all
the partial alignments or hypotheses in RNN-T and it can be discarded from the
decoder input. We also show that this frame reduction operation can be applied
in the middle of the encoder, which result in significant speed up for the
training and inference in RNN-T. We further show that the CTC alignment, a
by-product of the CTC decoder, can also be used to perform lattice reduction
for RNN-T during training. Our method is evaluated on the Librispeech and
SpeechStew tasks. We demonstrate that the proposed method is able to accelerate
the RNN-T inference by 2.2 times with similar or slightly better word error
rates (WER).",submitted to ICASSP 2023,
Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation,"[arxiv.Result.Author('Moitreya Chatterjee'), arxiv.Result.Author('Narendra Ahuja'), arxiv.Result.Author('Anoop Cherian')]",2022-10-29 02:55:39+00:00,"There exists an unequivocal distinction between the sound produced by a
static source and that produced by a moving one, especially when the source
moves towards or away from the microphone. In this paper, we propose to use
this connection between audio and visual dynamics for solving two challenging
tasks simultaneously, namely: (i) separating audio sources from a mixture using
visual cues, and (ii) predicting the 3D visual motion of a sounding source
using its separated audio. Towards this end, we present Audio Separator and
Motion Predictor (ASMP) -- a deep learning framework that leverages the 3D
structure of the scene and the motion of sound sources for better audio source
separation. At the heart of ASMP is a 2.5D scene graph capturing various
objects in the video and their pseudo-3D spatial proximities. This graph is
constructed by registering together 2.5D monocular depth predictions from the
2D video frames and associating the 2.5D scene regions with the outputs of an
object detector applied on those frames. The ASMP task is then mathematically
modeled as the joint problem of: (i) recursively segmenting the 2.5D scene
graph into several sub-graphs, each associated with a constituent sound in the
input audio mixture (which is then separated) and (ii) predicting the 3D
motions of the corresponding sound sources from the separated audio. To
empirically evaluate ASMP, we present experiments on two challenging
audio-visual datasets, viz. Audio Separation in the Wild (ASIW) and Audio
Visual Event (AVE). Our results demonstrate that ASMP achieves a clear
improvement in source separation quality, outperforming prior works on both
datasets, while also estimating the direction of motion of the sound sources
better than other methods.",Accepted at NeurIPS 2022,
Learning to Compute the Articulatory Representations of Speech with the MIRRORNET,"[arxiv.Result.Author('Yashish M. Siriwardena'), arxiv.Result.Author('Carol Espy-Wilson'), arxiv.Result.Author('Shihab Shamma')]",2022-10-29 00:46:48+00:00,"Most organisms including humans function by coordinating and integrating
sensory signals with motor actions to survive and accomplish desired tasks.
Learning these complex sensorimotor mappings proceeds simultaneously and often
in an unsupervised or semi-supervised fashion. An autoencoder architecture
(MirrorNet) inspired by this sensorimotor learning paradigm is explored in this
work to learn how to control an articulatory synthesizer. The synthesizer takes
as input control signals consisting of six vocal Tract Variables (TVs) and
source features (voicing indicators and pitch), and generates the corresponding
auditory spectrograms. Due to the non-linear structure of the synthesizer, the
control parameters that produce a target speech signal are not readily
computable nor are they always unique. Here we demonstrate how to initialize
the MirrorNet learning so as to produce a meaningful range of articulatory
values. Once trained, the MirrorNet successfully estimates the TVs and source
features needed to synthesize any arbitrary speech utterance. This approach
outperforms the best previously designed `speech inversion' systems on the
Wisconsin X-ray microbeam (XRMB) dataset.",,
The Secret Source : Incorporating Source Features to Improve Acoustic-to-Articulatory Speech Inversion,"[arxiv.Result.Author('Yashish M. Siriwardena'), arxiv.Result.Author('Carol Espy-Wilson')]",2022-10-29 00:28:29+00:00,"In this work, we incorporated acoustically derived source features,
aperiodicity, periodicity and pitch as additional targets to an
acoustic-to-articulatory speech inversion (SI) system. We also propose a
Temporal Convolution based SI system, which uses auditory spectrograms as the
input speech representation, to learn long-range dependencies and complex
interactions between the source and vocal tract, to improve the SI task. The
experiments are conducted with both the Wisconsin X-ray microbeam (XRMB) and
Haskins Production Rate Comparison (HPRC) datasets, with comparisons done with
respect to three baseline SI model architectures. The proposed SI system with
the HPRC dataset gains an improvement of close to 28% when the source features
are used as additional targets. The same SI system outperforms the current best
performing SI models by around 9% on the XRMB dataset.",,
Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention,"[arxiv.Result.Author('Xubo Liu'), arxiv.Result.Author('Qiushi Huang'), arxiv.Result.Author('Xinhao Mei'), arxiv.Result.Author('Haohe Liu'), arxiv.Result.Author('Qiuqiang Kong'), arxiv.Result.Author('Jianyuan Sun'), arxiv.Result.Author('Shengchen Li'), arxiv.Result.Author('Tom Ko'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Lilian H. Tang'), arxiv.Result.Author('Mark D. Plumbley'), arxiv.Result.Author('Volkan Kılıç'), arxiv.Result.Author('Wenwu Wang')]",2022-10-28 22:45:41+00:00,"Audio captioning is the task of generating captions that describe the content
of audio clips. In the real world, many objects produce similar sounds. It is
difficult to identify these auditory ambiguous sound events with access to
audio information only. How to accurately recognize ambiguous sounds is a major
challenge for audio captioning systems. In this work, inspired by the
audio-visual multi-modal perception of human beings, we propose visually-aware
audio captioning, which makes use of visual information to help the recognition
of ambiguous sounding objects. Specifically, we introduce an off-the-shelf
visual encoder to process the video inputs, and incorporate the extracted
visual features into an audio captioning system. Furthermore, to better exploit
complementary contexts from redundant audio-visual streams, we propose an
audio-visual attention mechanism that integrates audio and visual information
adaptively according to their confidence levels. Experimental results on
AudioCaps, the largest publicly available audio captioning dataset, show that
the proposed method achieves significant improvement over a strong baseline
audio captioning system and is on par with the state-of-the-art result.",Submitted to ICASSP 2023,
Evaluation of Categorical Generative Models -- Bridging the Gap Between Real and Synthetic Data,"[arxiv.Result.Author('Florence Regol'), arxiv.Result.Author('Anja Kroon'), arxiv.Result.Author('Mark Coates')]",2022-10-28 21:05:25+00:00,"The machine learning community has mainly relied on real data to benchmark
algorithms as it provides compelling evidence of model applicability.
Evaluation on synthetic datasets can be a powerful tool to provide a better
understanding of a model's strengths, weaknesses, and overall capabilities.
Gaining these insights can be particularly important for generative modeling as
the target quantity is completely unknown. Multiple issues related to the
evaluation of generative models have been reported in the literature. We argue
those problems can be avoided by an evaluation based on ground truth. General
criticisms of synthetic experiments are that they are too simplified and not
representative of practical scenarios. As such, our experimental setting is
tailored to a realistic generative task. We focus on categorical data and
introduce an appropriately scalable evaluation method. Our method involves
tasking a generative model to learn a distribution in a high-dimensional
setting. We then successively bin the large space to obtain smaller probability
spaces where meaningful statistical tests can be applied. We consider
increasingly large probability spaces, which correspond to increasingly
difficult modeling tasks and compare the generative models based on the highest
task difficulty they can reach before being detected as being too far from the
ground truth. We validate our evaluation procedure with synthetic experiments
on both synthetic generative models and current state-of-the-art categorical
generative models.","Submitted to the 2023 International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP). June 2023. 5 pages, 4 figures",
HeartSiam: A Domain Invariant Model for Heart Sound Classification,"[arxiv.Result.Author('Reza Yousefi Mashhoor'), arxiv.Result.Author('Ahmad Ayatollahi')]",2022-10-28 20:26:42+00:00,"Cardiovascular disease is one of the leading causes of death according to
WHO. Phonocardiography (PCG) is a costeffective, non-invasive method suitable
for heart monitoring. The main aim of this work is to classify heart sounds
into normal/abnormal categories. Heart sounds are recorded using different
stethoscopes, thus varying in the domain. Based on recent studies, this
variability can affect heart sound classification. This work presents a Siamese
network architecture for learning the similarity between normal vs. normal or
abnormal vs. abnormal signals and the difference between normal vs. abnormal
signals. By applying this similarity and difference learning across all
domains, the task of domain invariant heart sound classification can be well
achieved. We have used the multi-domain 2016 Physionet/CinC challenge dataset
for the evaluation method. Results: On the evaluation set provided by the
challenge, we have achieved a sensitivity of 82.8%, specificity of 75.3%, and
mean accuracy of 79.1%. While overcoming the multi-domain problem, the proposed
method has surpassed the first-place method of the Physionet challenge in terms
of specificity up to 10.9% and mean accuracy up to 5.6%. Also, compared with
similar state-of-the-art domain invariant methods, our model converges faster
and performs better in specificity (4.1%) and mean accuracy (1.5%) with an
equal number of epochs learned.",,
Efficient Speech Translation with Dynamic Latent Perceivers,"[arxiv.Result.Author('Ioannis Tsiamas'), arxiv.Result.Author('Gerard I. Gállego'), arxiv.Result.Author('José A. R. Fonollosa'), arxiv.Result.Author('Marta R. Costa-jussá')]",2022-10-28 16:52:48+00:00,"Transformers have been the dominant architecture for Speech Translation in
recent years, achieving significant improvements in translation quality. Since
speech signals are longer than their textual counterparts, and due to the
quadratic complexity of the Transformer, a down-sampling step is essential for
its adoption in Speech Translation. Instead, in this research, we propose to
ease the complexity by using a Perceiver encoder to map the speech inputs to a
fixed-length latent representation. Furthermore, we introduce a novel way of
training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent
spaces without any additional computational overhead. Speech-to-Text Perceivers
with DLA can match the performance of a Transformer baseline across three
language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to
DLA at inference, and can be flexibly deployed with various computational
budgets, without significant drops in translation quality.",,
Filter and evolve: progressive pseudo label refining for semi-supervised automatic speech recognition,"[arxiv.Result.Author('Zezhong Jin'), arxiv.Result.Author('Dading Zhong'), arxiv.Result.Author('Xiao Song'), arxiv.Result.Author('Zhaoyi Liu'), arxiv.Result.Author('Naipeng Ye'), arxiv.Result.Author('Qingcheng Zeng')]",2022-10-28 16:15:58+00:00,"Fine tuning self supervised pretrained models using pseudo labels can
effectively improve speech recognition performance. But, low quality pseudo
labels can misguide decision boundaries and degrade performance. We propose a
simple yet effective strategy to filter low quality pseudo labels to alleviate
this problem. Specifically, pseudo-labels are produced over the entire training
set and filtered via average probability scores calculated from the model
output. Subsequently, an optimal percentage of utterances with high probability
scores are considered reliable training data with trustworthy labels. The model
is iteratively updated to correct the unreliable pseudo labels to minimize the
effect of noisy labels. The process above is repeated until unreliable pseudo
abels have been adequately corrected. Extensive experiments on LibriSpeech show
that these filtered samples enable the refined model to yield more correct
predictions, leading to better ASR performances under various experimental
settings.",,
Universal speaker recognition encoders for different speech segments duration,"[arxiv.Result.Author('Sergey Novoselov'), arxiv.Result.Author('Vladimir Volokhov'), arxiv.Result.Author('Galina Lavrentyeva')]",2022-10-28 16:06:00+00:00,"Creating universal speaker encoders which are robust for different acoustic
and speech duration conditions is a big challenge today. According to our
observations systems trained on short speech segments are optimal for short
phrase speaker verification and systems trained on long segments are superior
for long segments verification. A system trained simultaneously on pooled short
and long speech segments does not give optimal verification results and usually
degrades both for short and long segments. This paper addresses the problem of
creating universal speaker encoders for different speech segments duration. We
describe our simple recipe for training universal speaker encoder for any type
of selected neural network architecture. According to our evaluation results of
wav2vec-TDNN based systems obtained for NIST SRE and VoxCeleb1 benchmarks the
proposed universal encoder provides speaker verification improvements in case
of different enrollment and test speech segment duration. The key feature of
the proposed encoder is that it has the same inference time as the selected
neural network architecture.",Submitted to ICASSP'23,
Target-Speaker Voice Activity Detection via Sequence-to-Sequence Prediction,"[arxiv.Result.Author('Ming Cheng'), arxiv.Result.Author('Weiqing Wang'), arxiv.Result.Author('Yucong Zhang'), arxiv.Result.Author('Xiaoyi Qin'), arxiv.Result.Author('Ming Li')]",2022-10-28 13:50:04+00:00,"Target-speaker voice activity detection is currently a promising approach for
speaker diarization in complex acoustic environments. This paper presents a
novel Sequence-to-Sequence Target-Speaker Voice Activity Detection
(Seq2Seq-TSVAD) method that can efficiently address the joint modeling of
large-scale speakers and predict high-resolution voice activities. Experimental
results show that larger speaker capacity and higher output resolution can
significantly reduce the diarization error rate (DER), which achieves the new
state-of-the-art performance of 4.55% on the VoxConverse test set and 10.77% on
Track 1 of the DIHARD-III evaluation set under the widely-used evaluation
metrics.",submitted to ICASSP2023,
Introducing topography in convolutional neural networks,"[arxiv.Result.Author('Maxime Poli'), arxiv.Result.Author('Emmanuel Dupoux'), arxiv.Result.Author('Rachid Riad')]",2022-10-28 13:20:31+00:00,"Parts of the brain that carry sensory tasks are organized topographically:
nearby neurons are responsive to the same properties of input signals. Thus, in
this work, inspired by the neuroscience literature, we proposed a new
topographic inductive bias in Convolutional Neural Networks (CNNs). To achieve
this, we introduced a new topographic loss and an efficient implementation to
topographically organize each convolutional layer of any CNN. We benchmarked
our new method on 4 datasets and 3 models in vision and audio tasks and showed
equivalent performance to all benchmarks. Besides, we also showcased the
generalizability of our topographic loss with how it can be used with different
topographic organizations in CNNs. Finally, we demonstrated that adding the
topographic inductive bias made CNNs more resistant to pruning. Our approach
provides a new avenue to obtain models that are more memory efficient while
maintaining better accuracy.",Submitted to ICASSP 2023,
Spatial Graph Signal Interpolation with an Application for Merging BCI Datasets with Various Dimensionalities,"[arxiv.Result.Author('Yassine El Ouahidi'), arxiv.Result.Author('Lucas Drumetz'), arxiv.Result.Author('Giulia Lioi'), arxiv.Result.Author('Nicolas Farrugia'), arxiv.Result.Author('Bastien Pasdeloup'), arxiv.Result.Author('Vincent Gripon')]",2022-10-28 11:24:53+00:00,"BCI Motor Imagery datasets usually are small and have different electrodes
setups. When training a Deep Neural Network, one may want to capitalize on all
these datasets to increase the amount of data available and hence obtain good
generalization results. To this end, we introduce a spatial graph signal
interpolation technique, that allows to interpolate efficiently multiple
electrodes. We conduct a set of experiments with five BCI Motor Imagery
datasets comparing the proposed interpolation with spherical splines
interpolation. We believe that this work provides novel ideas on how to
leverage graphs to interpolate electrodes and on how to homogenize multiple
datasets.","Submitted to the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2023)",
"Towards zero-shot Text-based voice editing using acoustic context conditioning, utterance embeddings, and reference encoders","[arxiv.Result.Author('Jason Fong'), arxiv.Result.Author('Yun Wang'), arxiv.Result.Author('Prabhav Agrawal'), arxiv.Result.Author('Vimal Manohar'), arxiv.Result.Author('Jilong Wu'), arxiv.Result.Author('Thilo Köhler'), arxiv.Result.Author('Qing He')]",2022-10-28 10:31:44+00:00,"Text-based voice editing (TBVE) uses synthetic output from text-to-speech
(TTS) systems to replace words in an original recording. Recent work has used
neural models to produce edited speech that is similar to the original speech
in terms of clarity, speaker identity, and prosody. However, one limitation of
prior work is the usage of finetuning to optimise performance: this requires
further model training on data from the target speaker, which is a costly
process that may incorporate potentially sensitive data into server-side
models. In contrast, this work focuses on the zero-shot approach which avoids
finetuning altogether, and instead uses pretrained speaker verification
embeddings together with a jointly trained reference encoder to encode
utterance-level information that helps capture aspects such as speaker identity
and prosody. Subjective listening tests find that both utterance embeddings and
a reference encoder improve the continuity of speaker identity and prosody
between the edited synthetic speech and unedited original recording in the
zero-shot setting.",Submitted to ICASSP 2023,
Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models,"[arxiv.Result.Author('Ramon Sanabria'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Sharon Goldwater')]",2022-10-28 10:26:46+00:00,"Given the strong results of self-supervised models on various tasks, there
have been surprisingly few studies exploring self-supervised representations
for acoustic word embeddings (AWE), fixed-dimensional vectors representing
variable-length spoken word segments. In this work, we study several
pre-trained models and pooling methods for constructing AWEs with
self-supervised representations. Owing to the contextualized nature of
self-supervised representations, we hypothesize that simple pooling methods,
such as averaging, might already be useful for constructing AWEs. When
evaluating on a standard word discrimination task, we find that HuBERT
representations with mean-pooling rival the state of the art on English AWEs.
More surprisingly, despite being trained only on English, HuBERT
representations evaluated on Xitsonga, Mandarin, and French consistently
outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on
English).",Submitted to IEEE ICASSP 2023,
Parameter-efficient transfer learning of pre-trained Transformer models for speaker verification using adapters,"[arxiv.Result.Author('Junyi Peng'), arxiv.Result.Author('Themos Stafylakis'), arxiv.Result.Author('Rongzhi Gu'), arxiv.Result.Author('Oldřich Plchot'), arxiv.Result.Author('Ladislav Mošner'), arxiv.Result.Author('Lukáš Burget'), arxiv.Result.Author('Jan Černocký')]",2022-10-28 10:07:42+00:00,"Recently, the pre-trained Transformer models have received a rising interest
in the field of speech processing thanks to their great success in various
downstream tasks. However, most fine-tuning approaches update all the
parameters of the pre-trained model, which becomes prohibitive as the model
size grows and sometimes results in overfitting on small datasets. In this
paper, we conduct a comprehensive analysis of applying parameter-efficient
transfer learning (PETL) methods to reduce the required learnable parameters
for adapting to speaker verification tasks. Specifically, during the
fine-tuning process, the pre-trained models are frozen, and only lightweight
modules inserted in each Transformer block are trainable (a method known as
adapters). Moreover, to boost the performance in a cross-language low-resource
scenario, the Transformer model is further tuned on a large intermediate
dataset before directly fine-tuning it on a small dataset. With updating fewer
than 4% of parameters, (our proposed) PETL-based methods achieve comparable
performances with full fine-tuning methods (Vox1-O: 0.55%, Vox1-E: 0.82%,
Vox1-H:1.73%).",submitted to ICASSP2023,
Assessing Phrase Break of ESL speech with Pre-trained Language Models,"[arxiv.Result.Author('Zhiyi Wang'), arxiv.Result.Author('Shaoguang Mao'), arxiv.Result.Author('Wenshan Wu'), arxiv.Result.Author('Yan Xia')]",2022-10-28 10:06:06+00:00,"This work introduces an approach to assessing phrase break in ESL learners'
speech with pre-trained language models (PLMs). Different with traditional
methods, this proposal converts speech to token sequences, and then leverages
the power of PLMs. There are two sub-tasks: overall assessment of phrase break
for a speech clip; fine-grained assessment of every possible phrase break
position. Speech input is first force-aligned with texts, then pre-processed to
a token sequence, including words and associated phrase break information. The
token sequence is then fed into the pre-training and fine-tuning pipeline. In
pre-training, a replaced break token detection module is trained with token
data where each token has a certain percentage chance to be randomly replaced.
In fine-tuning, overall and fine-grained scoring are optimized with text
classification and sequence labeling pipeline, respectively. With the
introduction of PLMs, the dependence on labeled training data has been greatly
reduced, and performance has improved.","Under Review, ICASSP 2023",
Laugh Betrays You? Learning Robust Speaker Representation From Speech Containing Non-Verbal Fragments,"[arxiv.Result.Author('Yuke Lin'), arxiv.Result.Author('Xiaoyi Qin'), arxiv.Result.Author('Huahua Cui'), arxiv.Result.Author('Zhenyi Zhu'), arxiv.Result.Author('Ming Li')]",2022-10-28 10:05:30+00:00,"The success of automatic speaker verification shows that discriminative
speaker representations can be extracted from neutral speech. However, as a
kind of non-verbal voice, laughter should also carry speaker information
intuitively. Thus, this paper focuses on exploring speaker verification about
utterances containing non-verbal laughter segments. We collect a set of clips
with laughter components by conducting a laughter detection script on VoxCeleb
and part of the CN-Celeb dataset. To further filter untrusted clips,
probability scores are calculated by our binary laughter detection classifier,
which is pre-trained by pure laughter and neutral speech. After that, based on
the clips whose scores are over the threshold, we construct trials under two
different evaluation scenarios: Laughter-Laughter (LL) and Speech-Laughter
(SL). Then a novel method called Laughter-Splicing based Network (LSN) is
proposed, which can significantly boost performance in both scenarios and
maintain the performance on the neutral speech, such as the VoxCeleb1 test set.
Specifically, our system achieves relative 20% and 22% improvement on
Laughter-Laughter and Speech-Laughter trials, respectively. The meta-data and
sample clips have been released at https://github.com/nevermoreLin/Laugh_LSN.",Submitted to ICASSP2023,
SG-VAD: Stochastic Gates Based Speech Activity Detection,"[arxiv.Result.Author('Jonathan Svirsky'), arxiv.Result.Author('Ofir Lindenbaum')]",2022-10-28 09:50:59+00:00,"We propose a novel voice activity detection (VAD) model in a low-resource
environment. Our key idea is to model VAD as a denoising task, and construct a
network that is designed to identify nuisance features for a speech
classification task. We train the model to simultaneously identify irrelevant
features while predicting the type of speech event. Our model contains only
7.8K parameters, outperforms the previously proposed methods on the AVA-Speech
evaluation set, and provides comparative results on the HAVIC dataset. We
present its architecture, experimental results, and ablation study on the
model's components. We publish the code and the models here
https://www.github.com/jsvir/vad.",,
Development of a rule-based lemmatization algorithm through Finite State Machine for Uzbek language,"[arxiv.Result.Author('Maksud Sharipov'), arxiv.Result.Author('Ogabek Sobirov')]",2022-10-28 09:21:06+00:00,"Lemmatization is one of the core concepts in natural language processing,
thus creating a lemmatization tool is an important task. This paper discusses
the construction of a lemmatization algorithm for the Uzbek language. The main
purpose of the work is to remove affixes of words in the Uzbek language by
means of the finite state machine and to identify a lemma (a word that can be
found in the dictionary) of the word. The process of removing affixes uses a
database of affixes and part of speech knowledge. This lemmatization consists
of the general rules and a part of speech data of the Uzbek language, affixes,
classification of affixes, removing affixes on the basis of the finite state
machine for each class, as well as a definition of this word lemma.","Preprint version of the paper to be published in The International
  Conference and Workshop on Agglutinative Language Technologies as a challenge
  of Natural Language Processing (ALTNLP), June 6, 2022, Koper, Slovenia",
Spectrograms Are Sequences of Patches,"[arxiv.Result.Author('Leyi Zhao'), arxiv.Result.Author('Yi Li')]",2022-10-28 08:39:36+00:00,"Self-supervised pre-training models have been used successfully in several
machine learning domains. However, only a tiny amount of work is related to
music. In our work, we treat a spectrogram of music as a series of patches and
design a self-supervised model that captures the features of these sequential
patches: Patchifier, which makes good use of self-supervised learning methods
from both NLP and CV domains. We do not use labeled data for the pre-training
process, only a subset of the MTAT dataset containing 16k music clips. After
pre-training, we apply the model to several downstream tasks. Our model
achieves a considerably acceptable result compared to other audio
representation models. Meanwhile, our work demonstrates that it makes sense to
consider audio as a series of patch segments.",,
NNSVS: A Neural Network-Based Singing Voice Synthesis Toolkit,"[arxiv.Result.Author('Ryuichi Yamamoto'), arxiv.Result.Author('Reo Yoneyama'), arxiv.Result.Author('Tomoki Toda')]",2022-10-28 08:37:13+00:00,"This paper describes the design of NNSVS, an open-source software for neural
network-based singing voice synthesis research. NNSVS is inspired by Sinsy, an
open-source pioneer in singing voice synthesis research, and provides many
additional features such as multi-stream models, autoregressive fundamental
frequency models, and neural vocoders. Furthermore, NNSVS provides extensive
documentation and numerous scripts to build complete singing voice synthesis
systems. Experimental results demonstrate that our best system significantly
outperforms our reproduction of Sinsy and other baseline systems. The toolkit
is available at https://github.com/nnsvs/nnsvs.",Submitted to ICASSP 2023,
Dysfluencies Seldom Come Alone -- Detection as a Multi-Label Problem,"[arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Dominik Wagner'), arxiv.Result.Author('Florian Hönig'), arxiv.Result.Author('Tobias Bocklet'), arxiv.Result.Author('Elmar Nöth'), arxiv.Result.Author('Korbinian Riedhammer')]",2022-10-28 08:22:23+00:00,"Specially adapted speech recognition models are necessary to handle stuttered
speech. For these to be used in a targeted manner, stuttered speech must be
reliably detected. Recent works have treated stuttering as a multi-class
classification problem or viewed detecting each dysfluency type as an isolated
task; that does not capture the nature of stuttering, where one dysfluency
seldom comes alone, i.e., co-occurs with others. This work explores an approach
based on a modified wav2vec 2.0 system for end-to-end stuttering detection and
classification as a multi-label problem. The method is evaluated on
combinations of three datasets containing English and German stuttered speech,
yielding state-of-the-art results for stuttering detection on the
SEP-28k-Extended dataset. Experimental results provide evidence for the
transferability of features and the generalizability of the method across
datasets and languages.",Submitted to ICASSP 2023,
End-to-end Ensemble-based Feature Selection for Paralinguistics Tasks,"[arxiv.Result.Author('Tamás Grósz'), arxiv.Result.Author('Mittul Singh'), arxiv.Result.Author('Sudarsana Reddy Kadiri'), arxiv.Result.Author('Hemant Kathania'), arxiv.Result.Author('Mikko Kurimo')]",2022-10-28 08:18:56+00:00,"The events of recent years have highlighted the importance of telemedicine
solutions which could potentially allow remote treatment and diagnosis.
Relatedly, Computational Paralinguistics, a unique subfield of Speech
Processing, aims to extract information about the speaker and form an important
part of telemedicine applications. In this work, we focus on two paralinguistic
problems: mask detection and breathing state prediction. Solutions developed
for these tasks could be invaluable and have the potential to help monitor and
limit the spread of a virus like COVID-19. The current state-of-the-art methods
proposed for these tasks are ensembles based on deep neural networks like
ResNets in conjunction with feature engineering. Although these ensembles can
achieve high accuracy, they also have a large footprint and require substantial
computational power reducing portability to devices with limited resources.
These drawbacks also mean that the previously proposed solutions are infeasible
to be used in a telemedicine system due to their size and speed. On the other
hand, employing lighter feature-engineered systems can be laborious and add
further complexity making them difficult to create a deployable system quickly.
This work proposes an ensemble-based automatic feature selection method to
enable the development of fast and memory-efficient systems. In particular, we
propose an output-gradient-based method to discover essential features using
large, well-performing ensembles before training a smaller one. In our
experiments, we observed considerable (25-32%) reductions in inference times
using neural network ensembles based on output-gradient-based features. Our
method offers a simple way to increase the speed of the system and enable
real-time usage while maintaining competitive results with larger-footprint
ensemble using all spectral features.",,
Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform,"[arxiv.Result.Author('Masaya Kawamura'), arxiv.Result.Author('Yuma Shirahata'), arxiv.Result.Author('Ryuichi Yamamoto'), arxiv.Result.Author('Kentaro Tachibana')]",2022-10-28 08:15:05+00:00,"We propose a lightweight end-to-end text-to-speech model using multi-band
generation and inverse short-time Fourier transform. Our model is based on
VITS, a high-quality end-to-end text-to-speech model, but adopts two changes
for more efficient inference: 1) the most computationally expensive component
is partially replaced with a simple inverse short-time Fourier transform, and
2) multi-band generation, with fixed or trainable synthesis filters, is used to
generate waveforms. Unlike conventional lightweight models, which employ
optimization or knowledge distillation separately to train two cascaded
components, our method enjoys the full benefits of end-to-end optimization.
Experimental results show that our model synthesized speech as natural as that
synthesized by VITS, while achieving a real-time factor of 0.066 on an Intel
Core i7 CPU, 4.1 times faster than VITS. Moreover, a smaller version of the
model significantly outperformed a lightweight baseline model with respect to
both naturalness and inference speed. Code and audio samples are available from
https://github.com/MasayaKawamura/MB-iSTFT-VITS.",Submitted to ICASSP 2023,
Period VITS: Variational Inference with Explicit Pitch Modeling for End-to-end Emotional Speech Synthesis,"[arxiv.Result.Author('Yuma Shirahata'), arxiv.Result.Author('Ryuichi Yamamoto'), arxiv.Result.Author('Eunwoo Song'), arxiv.Result.Author('Ryo Terashima'), arxiv.Result.Author('Jae-Min Kim'), arxiv.Result.Author('Kentaro Tachibana')]",2022-10-28 07:52:30+00:00,"Several fully end-to-end text-to-speech (TTS) models have been proposed that
have shown better performance compared to cascade models (i.e., training
acoustic and vocoder models separately). However, they often generate unstable
pitch contour with audible artifacts when the dataset contains emotional
attributes, i.e., large diversity of pronunciation and prosody. To address this
problem, we propose Period VITS, a novel end-to-end TTS model that incorporates
an explicit periodicity generator. In the proposed method, we introduce a frame
pitch predictor that predicts prosodic features, such as pitch and voicing
flags, from the input text. From these features, the proposed periodicity
generator produces a sample-level sinusoidal source that enables the waveform
decoder to accurately reproduce the pitch. Finally, the entire model is jointly
optimized in an end-to-end manner with variational inference and adversarial
objectives. As a result, the decoder becomes capable of generating more stable,
expressive, and natural output waveforms. The experimental results showed that
the proposed model significantly outperforms baseline models in terms of
naturalness, with improved pitch stability in the generated samples.",Submitted to ICASSP 2023,
Determining Ratio of Prunable Channels in MobileNet by Sparsity for Acoustic Scene Classification,"[arxiv.Result.Author('Yiqiang Cai'), arxiv.Result.Author('Shengchen Li')]",2022-10-28 07:41:16+00:00,"MobileNet is widely used for Acoustic Scene Classification (ASC) in embedded
systems. Existing works reduce the complexity of ASC algorithms by pruning some
components, e.g. pruning channels in the convolutional layer. In practice, the
maximum proportion of channels being pruned, which is defined as Ratio of
Prunable Channels ($R_\textit{PC}$), is often decided empirically. This paper
proposes a method that determines the $R_\textit{PC}$ by simple linear
regression models related to the Sparsity of Channels ($S_C$) in the
convolutional layers. In the experiment, $R_\textit{PC}$ is examined by
removing inactive channels until reaching a knee point of performance decrease.
Simple methods for calculating the $S_C$ of trained models and resulted
$R_\textit{PC}$ are proposed. The experiment results demonstrate that 1) the
decision of $R_\textit{PC}$ is linearly dependent on $S_C$ and the
hyper-parameters have a little impact on the relationship; 2) MobileNet shows a
high sensitivity and stability on proposed method.","5 pages, 2 figures, submitted to ICASSP 2023",
Signal inpainting from Fourier magnitudes,"[arxiv.Result.Author('Louis Bahrman'), arxiv.Result.Author('Marina Krémé'), arxiv.Result.Author('Paul Magron'), arxiv.Result.Author('Antoine Deleforge')]",2022-10-28 07:13:33+00:00,"Signal inpainting is the task of restoring degraded or missing samples in a
signal. In this paper we address signal inpainting when Fourier magnitudes are
observed. We propose a mathematical formulation of the problem that highlights
its connection with phase retrieval, and we introduce two methods for solving
it. First, we derive an alternating minimization scheme, which shares
similarities with the Gerchberg-Saxton algorithm, a classical phase retrieval
method. Second, we propose a convex relaxation of the problem, which is
inspired by recent approaches that reformulate phase retrieval into a
semidefinite program. We assess the potential of these methods for the task of
inpainting gaps in speech signals. Our methods exhibit both a high probability
of recovering the original signals and robustness to magnitude noise.",,
The Importance of Speech Stimuli for Pathologic Speech Classification,"[arxiv.Result.Author('Ilja Baumann'), arxiv.Result.Author('Dominik Wagner'), arxiv.Result.Author('Franziska Braun'), arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Elmar Nöth'), arxiv.Result.Author('Korbinian Riedhammer'), arxiv.Result.Author('Tobias Bocklet')]",2022-10-28 07:02:44+00:00,"Current findings show that pre-trained wav2vec 2.0 models can be successfully
used as feature extractors to discriminate on speaker-based tasks. We
demonstrate that latent representations extracted at different layers of a
pre-trained wav2vec 2.0 system can be effectively used for binary
classification of various types of pathologic speech. We examine the
pathologies laryngectomy, oral squamous cell carcinoma, parkinson's disease and
cleft lip and palate for this purpose. The results show that a distinction
between pathological and healthy voices, especially with latent representations
from the lower layers, performs well with the lowest accuracy from 77.2% for
parkinson's disease to 100% for laryngectomy classification. However,
cross-pathology and cross-healthy tests show that the trained classifiers seem
to be biased. The recognition rates vary considerably if there is a mismatch
between training and out-of-domain test data, e.g., in age, spoken content or
acoustic conditions.","Submitted to ICASSP 2023. arXiv admin note: text overlap with
  arXiv:2210.15336",
On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis,"[arxiv.Result.Author('Atsushi Ando'), arxiv.Result.Author('Ryo Masumura'), arxiv.Result.Author('Akihiko Takashima'), arxiv.Result.Author('Satoshi Suzuki'), arxiv.Result.Author('Naoki Makishima'), arxiv.Result.Author('Keita Suzuki'), arxiv.Result.Author('Takafumi Moriya'), arxiv.Result.Author('Takanori Ashihara'), arxiv.Result.Author('Hiroshi Sato')]",2022-10-28 06:48:35+00:00,"This paper investigates the effectiveness and implementation of
modality-specific large-scale pre-trained encoders for multimodal sentiment
analysis~(MSA). Although the effectiveness of pre-trained encoders in various
fields has been reported, conventional MSA methods employ them for only
linguistic modality, and their application has not been investigated. This
paper compares the features yielded by large-scale pre-trained encoders with
conventional heuristic features. One each of the largest pre-trained encoders
publicly available for each modality are used; CLIP-ViT, WavLM, and BERT for
visual, acoustic, and linguistic modalities, respectively. Experiments on two
datasets reveal that methods with domain-specific pre-trained encoders attain
better performance than those with conventional features in both unimodal and
multimodal scenarios. We also find it better to use the outputs of the
intermediate layers of the encoders than those of the output layer. The codes
are available at https://github.com/ando-hub/MSA_Pretrain.",Accepted to SLT 2022,
A comprehensive study on self-supervised distillation for speaker representation learning,"[arxiv.Result.Author('Zhengyang Chen'), arxiv.Result.Author('Yao Qian'), arxiv.Result.Author('Bing Han'), arxiv.Result.Author('Yanmin Qian'), arxiv.Result.Author('Michael Zeng')]",2022-10-28 06:48:28+00:00,"In real application scenarios, it is often challenging to obtain a large
amount of labeled data for speaker representation learning due to speaker
privacy concerns. Self-supervised learning with no labels has become a more and
more promising way to solve it. Compared with contrastive learning,
self-distilled approaches use only positive samples in the loss function and
thus are more attractive. In this paper, we present a comprehensive study on
self-distilled self-supervised speaker representation learning, especially on
critical data augmentation. Our proposed strategy of audio perturbation
augmentation has pushed the performance of the speaker representation to a new
limit. The experimental results show that our model can achieve a new SoTA on
Voxceleb1 speaker verification evaluation benchmark ( i.e., equal error rate
(EER) 2.505%, 2.473%, and 4.791% for trial Vox1-O, Vox1-E and Vox1-H ,
respectively), discarding any speaker labels in the training phase.",Accepted by SLT2022,
Forecasting local behavior of multi-agent system and its application to forest fire model,"[arxiv.Result.Author('Beomseok Kang'), arxiv.Result.Author('Minah Lee'), arxiv.Result.Author('Harshit Kumar'), arxiv.Result.Author('Saibal Mukhopadhyay')]",2022-10-28 05:39:44+00:00,"In this paper, we study a CNN-LSTM model to forecast the state of a specific
agent in a large multi-agent system. The proposed model consists of a CNN
encoder to represent the system into a low-dimensional vector, a LSTM module to
learn the agent dynamics in the vector space, and a MLP decoder to predict the
future state of an agent. A forest fire model is considered as an example where
we need to predict when a specific tree agent will be burning. We observe that
the proposed model achieves higher AUC with less computation than a frame-based
model and significantly saves computational costs such as the activation than
ConvLSTM.","submitted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)",
Speaker recognition with two-step multi-modal deep cleansing,"[arxiv.Result.Author('Ruijie Tao'), arxiv.Result.Author('Kong Aik Lee'), arxiv.Result.Author('Zhan Shi'), arxiv.Result.Author('Haizhou Li')]",2022-10-28 05:20:31+00:00,"Neural network-based speaker recognition has achieved significant improvement
in recent years. A robust speaker representation learns meaningful knowledge
from both hard and easy samples in the training set to achieve good
performance. However, noisy samples (i.e., with wrong labels) in the training
set induce confusion and cause the network to learn the incorrect
representation. In this paper, we propose a two-step audio-visual deep
cleansing framework to eliminate the effect of noisy labels in speaker
representation learning. This framework contains a coarse-grained cleansing
step to search for the peculiar samples, followed by a fine-grained cleansing
step to filter out the noisy labels. Our study starts from an efficient
audio-visual speaker recognition system, which achieves a close to perfect
equal-error-rate (EER) of 0.01\%, 0.07\% and 0.13\% on the Vox-O, E and H test
sets. With the proposed multi-modal cleansing mechanism, four different speaker
recognition networks achieve an average improvement of 5.9\%. Code has been
made available at:
\textcolor{magenta}{\url{https://github.com/TaoRuijie/AVCleanse}}.","5 pages, 3 figures",
Nonparallel High-Quality Audio Super Resolution with Domain Adaptation and Resampling CycleGANs,"[arxiv.Result.Author('Reo Yoneyama'), arxiv.Result.Author('Ryuichi Yamamoto'), arxiv.Result.Author('Kentaro Tachibana')]",2022-10-28 04:32:59+00:00,"Neural audio super-resolution models are typically trained on low- and
high-resolution audio signal pairs. Although these methods achieve highly
accurate super-resolution if the acoustic characteristics of the input data are
similar to those of the training data, challenges remain: the models suffer
from quality degradation for out-of-domain data, and paired data are required
for training. To address these problems, we propose Dual-CycleGAN, a
high-quality audio super-resolution method that can utilize unpaired data based
on two connected cycle consistent generative adversarial networks (CycleGAN).
Our method decomposes the super-resolution method into domain adaptation and
resampling processes to handle acoustic mismatch in the unpaired low- and
high-resolution signals. The two processes are then jointly optimized within
the CycleGAN framework. Experimental results verify that the proposed method
significantly outperforms conventional methods when paired data are not
available. Code and audio samples are available from
https://chomeyama.github.io/DualCycleGAN-Demo/.",Submitted to ICASSP 2023,
Improving short-video speech recognition using random utterance concatenation,"[arxiv.Result.Author('Haihua Xu'), arxiv.Result.Author('Van Tung Pham'), arxiv.Result.Author('Yerbolat Khassanov'), arxiv.Result.Author('Yist Lin'), arxiv.Result.Author('Tao Han'), arxiv.Result.Author('Tze Yuan Chong'), arxiv.Result.Author('Yi He'), arxiv.Result.Author('Zejun Ma')]",2022-10-28 03:54:57+00:00,"One of the limitations in end-to-end automatic speech recognition framework
is its performance would be compromised if train-test utterance lengths are
mismatched. In this paper, we propose a random utterance concatenation (RUC)
method to alleviate train-test utterance length mismatch issue for short-video
speech recognition task. Specifically, we are motivated by observations our
human-transcribed training utterances tend to be much shorter for short-video
spontaneous speech (~3 seconds on average), while our test utterance generated
from voice activity detection front-end is much longer (~10 seconds on
average). Such a mismatch can lead to sub-optimal performance. Experimentally,
by using the proposed RUC method, the best word error rate reduction (WERR) can
be achieved with around three fold training data size increase as well as two
utterance concatenation for each. In practice, the proposed method consistently
outperforms the strong baseline models, where 3.64% average WERR is achieved on
14 languages.","5 pages, 2 figures, 4 tables",
"""It's Not Just Hate'': A Multi-Dimensional Perspective on Detecting Harmful Speech Online","[arxiv.Result.Author('Federico Bianchi'), arxiv.Result.Author('Stefanie Anja Hills'), arxiv.Result.Author('Patricia Rossini'), arxiv.Result.Author('Dirk Hovy'), arxiv.Result.Author('Rebekah Tromble'), arxiv.Result.Author('Nava Tintarev')]",2022-10-28 03:34:50+00:00,"Well-annotated data is a prerequisite for good Natural Language Processing
models. Too often, though, annotation decisions are governed by optimizing time
or annotator agreement. We make a case for nuanced efforts in an
interdisciplinary setting for annotating offensive online speech. Detecting
offensive content is rapidly becoming one of the most important real-world NLP
tasks. However, most datasets use a single binary label, e.g., for hate or
incivility, even though each concept is multi-faceted. This modeling choice
severely limits nuanced insights, but also performance. We show that a more
fine-grained multi-label approach to predicting incivility and hateful or
intolerant content addresses both conceptual and performance issues. We release
a novel dataset of over 40,000 tweets about immigration from the US and UK,
annotated with six labels for different aspects of incivility and intolerance.
Our dataset not only allows for a more nuanced understanding of harmful speech
online, models trained on it also outperform or match performance on benchmark
datasets.",EMNLP 2022,
Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation,"[arxiv.Result.Author('Nobuyuki Morioka'), arxiv.Result.Author('Heiga Zen'), arxiv.Result.Author('Nanxin Chen'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Yifan Ding')]",2022-10-28 03:33:07+00:00,"Adapting a neural text-to-speech (TTS) model to a target speaker typically
involves fine-tuning most if not all of the parameters of a pretrained
multi-speaker backbone model. However, serving hundreds of fine-tuned neural
TTS models is expensive as each of them requires significant footprint and
separate computational resources (e.g., accelerators, memory). To scale speaker
adapted neural TTS voices to hundreds of speakers while preserving the
naturalness and speaker similarity, this paper proposes a parameter-efficient
few-shot speaker adaptation, where the backbone model is augmented with
trainable lightweight modules called residual adapters. This architecture
allows the backbone model to be shared across different target speakers.
Experimental results show that the proposed approach can achieve competitive
naturalness and speaker similarity compared to the full fine-tuning approaches,
while requiring only $\sim$0.1% of the backbone model parameters for each
speaker.",Submitted to ICASSP 2023,
Speech Enhancement with Intelligent Neural Homomorphic Synthesis,"[arxiv.Result.Author('Shulin He'), arxiv.Result.Author('Wei Rao'), arxiv.Result.Author('Jinjiang Liu'), arxiv.Result.Author('Jun Chen'), arxiv.Result.Author('Yukai Ju'), arxiv.Result.Author('Xueliang Zhang'), arxiv.Result.Author('Yannan Wang'), arxiv.Result.Author('Shidong Shang')]",2022-10-28 02:49:48+00:00,"Most neural network speech enhancement models ignore speech production
mathematical models by directly mapping Fourier transform spectrums or
waveforms. In this work, we propose a neural source filter network for speech
enhancement. Specifically, we use homomorphic signal processing and cepstral
analysis to obtain noisy speech's excitation and vocal tract. Unlike
traditional signal processing, we use an attentive recurrent network (ARN)
model predicted ratio mask to replace the liftering separation function. Then
two convolutional attentive recurrent network (CARN) networks are used to
predict the excitation and vocal tract of clean speech, respectively. The
system's output is synthesized from the estimated excitation and vocal.
Experiments prove that our proposed method performs better, with SI-SNR
improving by 1.363dB compared to FullSubNet.",Submitted to ICASSP 2023,
Local-global speaker representation for target speaker extraction,"[arxiv.Result.Author('Shulin He'), arxiv.Result.Author('Wei Rao'), arxiv.Result.Author('Kanghao Zhang'), arxiv.Result.Author('Yukai Ju'), arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Xueliang Zhang'), arxiv.Result.Author('Yannan Wang'), arxiv.Result.Author('Shidong Shang')]",2022-10-28 02:46:47+00:00,"Target speaker extraction is to extract the target speaker's voice from a
mixture of signals according to the given enrollment utterance. The target
speaker's enrollment utterance is also called as anchor speech. The effective
utilization of anchor speech is crucial for speaker extraction. In this study,
we propose a new system to exploit speaker information from anchor speech
fully. Unlike models that use only local or global features of the anchor, the
proposed method extracts speaker information on global and local levels and
feeds the features into a speech separation network. Our approach benefits from
the complementary advantages of both global and local features, and the
performance of speaker extraction is improved. We verified the feasibility of
this local-global representation (LGR) method using multiple speaker extraction
models. Systematic experiments were conducted on the open-source dataset
Libri-2talker, and the results showed that the proposed method significantly
outperformed the baseline models.",Submitted to ICASSP 2023,
GM-TCNet: Gated Multi-scale Temporal Convolutional Network using Emotion Causality for Speech Emotion Recognition,"[arxiv.Result.Author('Jia-Xin Ye'), arxiv.Result.Author('Xin-Cheng Wen'), arxiv.Result.Author('Xuan-Ze Wang'), arxiv.Result.Author('Yong Xu'), arxiv.Result.Author('Yan Luo'), arxiv.Result.Author('Chang-Li Wu'), arxiv.Result.Author('Li-Yan Chen'), arxiv.Result.Author('Kun-Hong Liu')]",2022-10-28 02:00:40+00:00,"In human-computer interaction, Speech Emotion Recognition (SER) plays an
essential role in understanding the user's intent and improving the interactive
experience. While similar sentimental speeches own diverse speaker
characteristics but share common antecedents and consequences, an essential
challenge for SER is how to produce robust and discriminative representations
through causality between speech emotions. In this paper, we propose a Gated
Multi-scale Temporal Convolutional Network (GM-TCNet) to construct a novel
emotional causality representation learning component with a multi-scale
receptive field. GM-TCNet deploys a novel emotional causality representation
learning component to capture the dynamics of emotion across the time domain,
constructed with dilated causal convolution layer and gating mechanism.
Besides, it utilizes skip connection fusing high-level features from different
gated convolution blocks to capture abundant and subtle emotion changes in
human speech. GM-TCNet first uses a single type of feature, mel-frequency
cepstral coefficients, as inputs and then passes them through the gated
temporal convolutional module to generate the high-level features. Finally, the
features are fed to the emotion classifier to accomplish the SER task. The
experimental results show that our model maintains the highest performance in
most cases compared to state-of-the-art techniques.","The source code is available at:
  https://github.com/Jiaxin-Ye/GM-TCNet","speech communication, 145, November 2022, 21-35"
On the Role of Visual Context in Enriching Music Representations,"[arxiv.Result.Author('Kleanthis Avramidis'), arxiv.Result.Author('Shanti Stewart'), arxiv.Result.Author('Shrikanth Narayanan')]",2022-10-28 01:45:07+00:00,"Human perception and experience of music is highly context-dependent.
Contextual variability contributes to differences in how we interpret and
interact with music, challenging the design of robust models for information
retrieval. Incorporating multimodal context from diverse sources provides a
promising approach toward modeling this variability. Music presented in media
such as movies and music videos provide rich multimodal context that modulates
underlying human experiences. However, such context modeling is underexplored,
as it requires large amounts of multimodal data along with relevant
annotations. Self-supervised learning can help address these challenges by
automatically extracting rich, high-level correspondences between different
modalities, hence alleviating the need for fine-grained annotations at scale.
In this study, we propose VCMR -- Video-Conditioned Music Representations, a
contrastive learning framework that learns music representations from audio and
the accompanying music videos. The contextual visual information enhances
representations of music audio, as evaluated on the downstream task of music
tagging. Experimental results show that the proposed framework can contribute
additive robustness to audio representations and indicates to what extent
musical elements are affected or determined by visual context.","5 pages, 4 figures, 1 table",
UX-NET: Filter-and-Process-based Improved U-Net for Real-time Time-domain Audio Separation,"[arxiv.Result.Author('Kashyap Patel'), arxiv.Result.Author('Anton Kovalyov'), arxiv.Result.Author('Issa Panahi')]",2022-10-28 01:24:21+00:00,"This study presents UX-Net, a time-domain audio separation network (TasNet)
based on a modified U-Net architecture. The proposed UX-Net works in real-time
and handles either single or multi-microphone input. Inspired by the
filter-and-process-based human auditory behavior, the proposed system
introduces novel mixer and separation modules, which result in cost and memory
efficient modeling of speech sources. The mixer module combines encoded input
in a latent feature space and outputs a desired number of output streams. Then,
in the separation module, a modified U-Net (UX) block is applied. The UX block
first filters the encoded input at various resolutions followed by aggregating
the filtered information and applying recurrent processing to estimate masks of
separated sources. The letter 'X' in UX-Net is a name placeholder for the type
of recurrent layer employed in the UX block. Empirical findings on the
WSJ0-2mix benchmark dataset show that one of the UX-Net configurations
outperforms the state-of-the-art Conv-TasNet system by 0.85 dB SI-SNR while
using only 16% of the model parameters, 58% fewer computations, and maintaining
low latency.",Submitted to ICASSP 2023,
Conditioning and Sampling in Variational Diffusion Models for Speech Super-Resolution,"[arxiv.Result.Author('Chin-Yun Yu'), arxiv.Result.Author('Sung-Lin Yeh'), arxiv.Result.Author('György Fazekas'), arxiv.Result.Author('Hao Tang')]",2022-10-27 22:31:20+00:00,"Recently, diffusion models (DMs) have been increasingly used in audio
processing tasks, including speech super-resolution (SR), which aims to restore
high-frequency content given low-resolution speech utterances. This is commonly
achieved by conditioning the network of noise predictor with low-resolution
audio. In this paper, we propose a novel sampling algorithm that communicates
the information of the low-resolution audio via the reverse sampling process of
DMs. The proposed method can be a drop-in replacement for the vanilla sampling
process and can significantly improve the performance of the existing works.
Moreover, by coupling the proposed sampling method with an unconditional DM,
i.e., a DM with no auxiliary inputs to its noise predictor, we can generalize
it to a wide range of SR setups. We also attain state-of-the-art results on the
VCTK Multi-Speaker benchmark with this novel formulation.",Submitted to ICASSP 2023,
AmberNet: A Compact End-to-End Model for Spoken Language Identification,"[arxiv.Result.Author('Fei Jia'), arxiv.Result.Author('Nithin Rao Koluguri'), arxiv.Result.Author('Jagadeesh Balam'), arxiv.Result.Author('Boris Ginsburg')]",2022-10-27 21:47:30+00:00,"We present AmberNet, a compact end-to-end neural network for Spoken Language
Identification. AmberNet consists of 1D depth-wise separable convolutions and
Squeeze-and-Excitation layers with global context, followed by statistics
pooling and linear layers. AmberNet achieves performance similar to
state-of-the-art(SOTA) models on VoxLingua107 dataset, while being 10x smaller.
AmberNet can be adapted to unseen languages and new acoustic conditions with
simple finetuning. It attains SOTA accuracy of 75.8% on FLEURS benchmark. We
show the model is easily scalable to achieve a better trade-off between
accuracy and speed. We further inspect the model's sensitivity to input length
and show that AmberNet performs well even on short utterances.",Submitted to ICASSP 2023,
Evaluating context-invariance in unsupervised speech representations,"[arxiv.Result.Author('Mark Hallap'), arxiv.Result.Author('Emmanuel Dupoux'), arxiv.Result.Author('Ewan Dunbar')]",2022-10-27 21:15:49+00:00,"Unsupervised speech representations have taken off, with benchmarks (SUPERB,
ZeroSpeech) demonstrating major progress on semi-supervised speech recognition,
speech synthesis, and speech-only language modelling. Inspiration comes from
the promise of ``discovering the phonemes'' of a language or a similar
low-bitrate encoding. However, one of the critical properties of phoneme
transcriptions is context-invariance: the phonetic context of a speech sound
can have massive influence on the way it is pronounced, while the text remains
stable. This is what allows tokens of the same word to have the same
transcriptions -- key to language understanding. Current benchmarks do not
measure context-invariance. We develop a new version of the ZeroSpeech ABX
benchmark that measures context-invariance, and apply it to recent
self-supervised representations. We demonstrate that the context-independence
of representations is predictive of the stability of word-level
representations. We suggest research concentrate on improving
context-independence of self-supervised and unsupervised representations.",Submitted to ICASSP 2023,
Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge,"[arxiv.Result.Author('Ewan Dunbar'), arxiv.Result.Author('Nicolas Hamilakis'), arxiv.Result.Author('Emmanuel Dupoux')]",2022-10-27 20:32:41+00:00,"Recent progress in self-supervised or unsupervised machine learning has
opened the possibility of building a full speech processing system from raw
audio without using any textual representations or expert labels such as
phonemes, dictionaries or parse trees. The contribution of the Zero Resource
Speech Challenge series since 2015 has been to break down this long-term
objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term
Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce
associated metrics and benchmarks enabling model comparison and cumulative
progress. We present an overview of the six editions of this challenge series
since 2015, discuss the lessons learned, and outline the areas which need more
work or give puzzling results.",,"Journal: IEEE Journal of Selected Topics in Signal Processing
  Publication Date: OCTOBER 2022 Volume: 16, Issue: 6 On Page(s): 1211-1226
  Print ISSN: 1932-4553 Online ISSN: 1941-0484 Digital Object Identifier:
  10.1109/JSTSP.2022.3206084"
