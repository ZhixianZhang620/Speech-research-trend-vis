{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get abstract\n",
    "df = pd.read_csv('result4.csv')\n",
    "df2 = df[['Abstract']]\n",
    "df2 = df2.dropna()\n",
    "abstract_string = df2['Abstract'].str.cat(sep=' ')\n",
    "abstract_string = abstract_string.replace(',','')\n",
    "abstract_string = abstract_string.replace('.','')\n",
    "abstract_string = abstract_string.replace('\"','')\n",
    "abstract_string = abstract_string.replace('(','')\n",
    "abstract_string = abstract_string.replace(')','')\n",
    "abstract_string = abstract_string.replace('!','')\n",
    "abstract_string = abstract_string.replace('?','')\n",
    "abstract_string = abstract_string.replace(':','')\n",
    "abstract_string = abstract_string.replace(';','')\n",
    "abstract_string = abstract_string.replace('=','')\n",
    "abstract_string = abstract_string.replace('%','')\n",
    "abstract_string = abstract_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read stopword.txt where stopwords are separated by a new line\n",
    "stopwords = pd.read_csv('stop_words.txt', header=None)\n",
    "sw = stopwords[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "abstract_string = ' '.join([word for word in abstract_string.split() if word not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count\n",
    "word_count = Counter(abstract_string.split())\n",
    "word_count = pd.DataFrame.from_dict(word_count, orient='index').reset_index()\n",
    "word_count.columns = ['word', 'count']\n",
    "word_count = word_count.sort_values(by='count', ascending=False)\n",
    "word_count = word_count.reset_index(drop=True)\n",
    "# word_count.to_csv('word_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech language data study model recognition learning performance method models paper based proposed features children analysis training neural methods noise approach speaker network task voice communication systems hearing social processing acoustic accuracy patients asr time compared studies propose quality english deep audio development word tasks hate participants speakers dataset detection signal automatic human languages rate improve perception people emotion auditory article feature classification attention clinical loss experiments including process target age content corpus findings input linguistic evaluation signals level provide intelligibility purpose acts students framework media conditions context trained approaches set control techniques sound differences measures experimental networks current types machine function frequency knowledge objective cognitive error support online design enhancement understanding production datasets algorithm natural effects role therapy assessment previous multiple terms state-of-the-art applications visual treatment spoken existing translation scores factors potential disorders source temporal demonstrate background users ability listening main developed characteristics representations technology individuals baseline effective form interaction identify addition result end-to-end conducted outcomes conclusions improvement abstract improved health investigate speeches emotional domain skills adults listeners impact specific representation applied complex performed public evidence aims strategies parameters evaluate memory future tts intervention discourse focus limited brain synthesis achieved range architecture individual common practice technique sentences speech-language review political achieve levels activity speaking challenge samples motor response included score evaluated education responses observed reported algorithms disease utterances structure aim relative application literature type field increased noisy covid-19 reading life patterns vocal single analyzed semantic convolutional average identification finally cochlear collected functions environment experience standard sentence disorder challenges effectiveness\n"
     ]
    }
   ],
   "source": [
    "#get first 100 words\n",
    "word_count = word_count.head(250)\n",
    "word_list = word_count['word'].tolist()\n",
    "wl = ' '.join(word_list)\n",
    "print(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_string = ' '.join([word for word in abstract_string.split() if word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to txt\n",
    "with open('word_string.txt', 'w') as f:\n",
    "    f.write(word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the first 100 words\n",
    "word_count = word_count.head(250)\n",
    "word_count.to_csv('word_count_250.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a json file for d3\n",
    "word_count = word_count.set_index('word')\n",
    "word_count.to_json('word_count.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_file = \"word_count_250.csv\"\n",
    "json_file = \"word_count_250.json\"\n",
    "\n",
    "# Read CSV file into a list of dictionaries\n",
    "with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = [row for row in reader]\n",
    "\n",
    "# Convert list of dictionaries to list of objects in desired format size in the format of string\n",
    "myWords = [{\"name\": row[\"word\"], \"value\": row[\"count\"]} for row in rows]\n",
    "# Write list of objects to JSON file\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(myWords, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '123.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ts/yc6c7tl96hjdf33yyy_9y96m0000gn/T/ipykernel_25358/929901514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'123.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#convert it to base64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '123.png'"
     ]
    }
   ],
   "source": [
    "#read 123.png\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open('123.png')\n",
    "\n",
    "#convert it to base64\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399d0e3137bfbbe732ffce16f404d0134f25670dcd6d6cdc60d5d201443092cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
